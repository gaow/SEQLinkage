{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yh3455/miniconda3/envs/seqpy3v0/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core module\n",
    "\n",
    "> API details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SEQLinkage.Main import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "args = Args().parser.parse_args(['--fam','../sample_i/rare_positions/sample_i_coding.hg38_multianno.fam', \n",
    "                                 '--vcf', '../sample_i/rare_positions/sample_i_coding.hg38_multianno.vcf.gz',\n",
    "                                 '--blueprint','./data/vipgenemap.hg38.txt','-f','MERLIN',\n",
    "'--tempdir','./Tempdir_s1',\n",
    "'--build', 'hg38',  '--freq', 'AF', '-K', '0.001', '--moi', 'AD', '-W', '0', '-M', '1', \n",
    "'--theta-max', '0.5', '--theta-inc', '0.05','--run-linkage', '--output', './testseqlink'])\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "args = Args().parser.parse_args(['--fam','../sample_i/rare_positions/sample_i_coding.hg38_multianno.fam', \n",
    "                                 '--vcf', '../sample_i/vcf/small_sample_i.vcf.gz',\n",
    "                                 '--blueprint','./data/vipgenemap.hg38.txt','-f','MERLIN',\n",
    "'--tempdir','./Tempdir_s1',\n",
    "'--build', 'hg38',  '--freq', 'AF', '-K', '0.001', '--moi', 'AD', '-W', '0', '-M', '1', \n",
    "'--theta-max', '0.5', '--theta-inc', '0.05','--run-linkage', '--output', './testseqlink'])\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "args = Args().parser.parse_args(['--fam','./seqlinkage-example/seqlinkage-example.fam','--vcf','./seqlinkage-example/seqlinkage-example.vcf.gz','-f','MERLIN',\n",
    "                                 '--tempdir','./seqlinkage-example/tmprst',\n",
    "                                 '--build','hg19','--blueprint','./seqlinkage-example/twogenomap.txt','--freq','EVSEAAF','-K','0.001','--moi','AR','-W','0','-M','1',\n",
    "                                 '--theta-max','0.5','--theta-inc','0.05','--run-linkage','--output','./seqlinkage-example/tsq20211130'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?shoud we set mle parameter as true? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "args = Args().parser.parse_args(['--fam','../MWE/sample2_uniq.fam', \n",
    "                                 '--vcf', '../MWE/sample_ii_coding.hg38_multianno.vcf.gz',\n",
    "'--blueprint','../MWE/genemap.hg38.txt', '--chrom-prefix','1','-f','MERLIN',\n",
    "'--tempdir','./Tempdir',\n",
    "'--build', 'hg38',  '--freq', 'AF', '-K', '0.001', '--moi', 'AD', '-W', '0', '-M', '1', \n",
    "'--theta-max', '0.5', '--theta-inc', '0.05','--run-linkage', '--output', './testseqlink1105'])\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "args = Args().parser.parse_args('--fam seqlinkage-example/seqlinkage-example.fam --vcf seqlinkage-example/seqlinkage-example.vcf.gz -f MERLIN --blueprint data/genemap.txt --freq EVSEAAF'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args().parser.parse_args('--fam data/mwe_normal_fam.csv --vcf data/first1000snp_full_samples.vcf.gz -f MERLIN --blueprint data/genemap.hg38.txt --freq AF'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(bin=0.8, blueprint='data/genemap.hg38.txt', single_markers=False, tfam='data/mwe_normal_fam.csv', vcf='data/first1000snp_full_samples.vcf.gz', build='hg19', prephased=False, freq='AF', freq_by_fam=None, mle=False, rvhaplo=False, recomb_max=1, recomb_cross_fam=False, rsq=0.0, include_vars=None, maf_cutoff=1.0, chr_prefix=None, output=None, format=['MERLIN'], prevalence=None, inherit_mode=None, wild_pen=None, muta_pen=None, theta_max=0.5, theta_inc=0.05, run_linkage=False, output_limit=10, jobs=16, tempdir=None, vanilla=True, quiet=False, debug=False, no_save=False, func=<function main at 0x2b8c25c03c10>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.from Core import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from __future__ import print_function\n",
    "from SEQLinkage.Utils import *\n",
    "from SEQLinkage.Runner import *\n",
    "from multiprocessing import Process, Queue\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import sys, faulthandler, platform\n",
    "import numpy as np\n",
    "import os\n",
    "if sys.version_info.major == 2:\n",
    "    from cstatgen import cstatgen_py2 as cstatgen\n",
    "    from cstatgen.egglib import Align\n",
    "else:\n",
    "    from cstatgen import cstatgen_py3 as cstatgen\n",
    "    import egglib\n",
    "    from egglib import Align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. RData class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RData(dict):\n",
    "    def __init__(self, vcf, tfam):\n",
    "        # tfam.samples: a dict of {sid:[fid, pid, mid, sex, trait], ...}\n",
    "        # tfam.families: a dict of {fid:[s1, s2 ...], ...}\n",
    "        self.tfam = TFAMParser(tfam)\n",
    "        self.vs = self.load_vcf(vcf)\n",
    "        self.samples_vcf = self.vs.GetSampleNames()\n",
    "        self.samples_not_vcf = checkSamples(self.samples_vcf, self.tfam.samples.keys())[1]\n",
    "        # samples have to be in both vcf and tfam data\n",
    "        self.samples = OrderedDict([(k, self.tfam.samples[k]) for k in self.samples_vcf if k in self.tfam.samples])\n",
    "        # a dict of {fid:[member names], ...}\n",
    "        self.families = {k : [x for x in self.samples if x in self.tfam.families[k]] for k in self.tfam.families}\n",
    "        # a dict of {fid:[idx ...], ...}\n",
    "        self.famsampidx = {}\n",
    "        # a dict of {fid:[maf1, maf2 ...]}\n",
    "        self.maf = OrderedDict()\n",
    "        # finalized sub_regions that are compliant to all families\n",
    "        self.complied_markers = []\n",
    "        # finalized sub regions (variants)\n",
    "        self.combined_regions = []\n",
    "        self.coordinates_by_region = []\n",
    "        # RV varnames by family\n",
    "        self.varnames_by_fam = {}\n",
    "        self.patterns={}\n",
    "        self.gnomAD_estimate={'AFR':(1-0.4589)/(2*7652),'AMR':(1-0.4455)/(2*16791),'ASJ':(1-0.2357)/(2*4925),'EAS':(1-0.4735)/(2*8624),'FIN':(1-0.3048)/(2*11150),'NFE':(1-0.5729)/(2*55860),'OTH':(1-0.4386)/(2*2743),'SAS':(1-0.5624)/(2*15391)}\n",
    "        # reorder family samples based on order of VCF file\n",
    "        for k in list(self.families.keys()):\n",
    "            if len(self.families[k]) == 0:\n",
    "                # skip families having no samples in VCF file\n",
    "                del self.families[k]\n",
    "            else:\n",
    "                self.famsampidx[k] = [i for i, x in enumerate(self.samples_vcf) if x in self.families[k]]\n",
    "        # a dict of {fid:[idx ...], ...}\n",
    "        self.famvaridx = {}\n",
    "        self.wtvar = {}\n",
    "        self.freq_by_fam = {}\n",
    "        self.include_vars = []\n",
    "        self.total_varnames={}\n",
    "        self.total_mafs={}\n",
    "        self.wt_maf={}\n",
    "        self.freq = []\n",
    "        self.genotype_all={}\n",
    "        self.mle_mafs={}\n",
    "        self.missing_persons=[]\n",
    "        self.gss = {} #test line\n",
    "        self.reset()\n",
    "    \n",
    "    def load_vcf(self,vcf):\n",
    "        # load VCF file header\n",
    "        return cstatgen.VCFstream(vcf)\n",
    "\n",
    "    def reset(self):\n",
    "        for item in self.samples:\n",
    "            self[item] = []\n",
    "            self.genotype_all[item] = []\n",
    "        self.variants = []\n",
    "        self.include_vars = []\n",
    "        self.total_varnames={}\n",
    "        self.total_mafs={}\n",
    "        self.wt_maf={}\n",
    "        self.chrom = None\n",
    "        for k in self.families.keys():\n",
    "            self.famvaridx[k] = []\n",
    "            self.wtvar[k] = []\n",
    "        self.maf = OrderedDict()\n",
    "        # superMarkerCount is the max num. of recombinant fragments among all fams\n",
    "        self.superMarkerCount = 0\n",
    "        self.complied_markers = []\n",
    "        self.combined_regions = []\n",
    "        self.coordinates_by_region = []\n",
    "        self.patterns={}\n",
    "        self.missing_persons=[]\n",
    "        self.gss = {} #test line\n",
    "\n",
    "\n",
    "    def getMidPosition(self):\n",
    "        if len(self.variants) == 0:\n",
    "            return None\n",
    "        return sum([x[1] for x in self.variants]) / len(self.variants)\n",
    "\n",
    "    def getFamVariants(self, fam, style = None):\n",
    "        if style is None:\n",
    "            return [item for idx, item in enumerate(self.variants) if idx in self.famvaridx[fam]]\n",
    "        elif style == \"map\":\n",
    "            names = []\n",
    "            pos = []\n",
    "            mafs = []\n",
    "            tmp_vars = self.famvaridx[fam]\n",
    "            if len(self.freq_by_fam.keys()) != 0:\n",
    "                pop_idx=self.freq.index(self.freq_by_fam[fam])\n",
    "            for idx in tmp_vars:\n",
    "                names.append(\"V{}-{}\".format(idx, self.variants[idx][1]))\n",
    "                pos.append(self.variants[idx][1])\n",
    "                tmp_mafs=self.variants[idx][-1]\n",
    "                if type(tmp_mafs) is list:\n",
    "                    mafs.append(tmp_mafs[pop_idx])\n",
    "                else:\n",
    "                    mafs.append(tmp_mafs)\n",
    "            return names, pos, mafs\n",
    "        else:\n",
    "            raise ValueError(\"Unknown style '{}'\".format(style))\n",
    "\n",
    "    def getFamSamples(self, fam):\n",
    "        nvar = len([item for idx, item in enumerate(self.variants) if idx in self.famvaridx[fam]])\n",
    "        output = [[]] * len(self.tfam.families[fam])\n",
    "        for idx, item in enumerate(self.tfam.sort_family(fam)):\n",
    "            # sample info, first 5 columns of ped\n",
    "            output[idx] = self.tfam.samples[item][:-1]\n",
    "            # sample genotypes\n",
    "            if item in self.samples:\n",
    "                output[idx].extend(self[item])\n",
    "            else:\n",
    "                output[idx].extend([\"00\"] * nvar)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.RegionExtractor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionExtractor:\n",
    "    '''Extract given genomic region from VCF\n",
    "    converting genotypes into dictionary of\n",
    "    genotype list'''\n",
    "    def __init__(self, filename, build = env.build, chr_prefix = None, allele_freq_info = None, include_vars_file=None):\n",
    "        self.vcf = cstatgen.VCFstream(filename)\n",
    "        self.chrom = self.startpos = self.endpos = self.name = None\n",
    "        self.chr_prefix = chr_prefix\n",
    "        # name of allele frequency meta info\n",
    "        self.af_info = allele_freq_info\n",
    "        self.xchecker = PseudoAutoRegion('X', build)\n",
    "        self.ychecker = PseudoAutoRegion('Y', build)\n",
    "        self.include_vars_file = include_vars_file\n",
    "\n",
    "    def apply(self, data):\n",
    "        # Clean up\n",
    "        data.reset()\n",
    "        data.chrom = self.chrom\n",
    "        self.vcf.Extract(self.chrom, self.startpos, self.endpos)\n",
    "        varIdx = 0\n",
    "        # for each variant site\n",
    "        while (self.vcf.Next()):\n",
    "            # skip tri-allelic sites\n",
    "            if not self.vcf.IsBiAllelic():\n",
    "                with env.triallelic_counter.get_lock():\n",
    "                    env.triallelic_counter.value += 1\n",
    "                continue\n",
    "            if len(data.variants) > 0:\n",
    "                if self.vcf.GetPosition()==data.variants[-1][1]:\n",
    "                    continue\n",
    "            # check if the line's sample number matches the entire VCF sample number\n",
    "            if not self.vcf.CountSampleGenotypes() == self.vcf.sampleCount:\n",
    "                raise ValueError('Genotype and sample mismatch for region {}: {:,d} vs {:,d}'.\\\n",
    "                             format(self.name, self.vcf.CountSampleGenotypes(), self.vcf.sampleCount))\n",
    "            # valid line found, get variant info\n",
    "            try:\n",
    "                if type(self.af_info) is list:\n",
    "                    maf = []\n",
    "                    large_maf = []\n",
    "                    for pop_info in self.af_info:\n",
    "                        large_maf.append(False)\n",
    "                        try:\n",
    "                            maf.append(float(self.vcf.GetInfo(pop_info)))\n",
    "                        except ValueError:\n",
    "                            maf.append(0.0)\n",
    "                    for idx in range(len(maf)):\n",
    "                        if maf[idx] > 0.5:\n",
    "                            large_maf[idx]=True\n",
    "                            maf[idx] = 1-maf[idx]\n",
    "                else:\n",
    "                    large_maf=False\n",
    "                    try:\n",
    "                        maf = float(self.vcf.GetInfo(self.af_info)) if self.af_info else None\n",
    "                    except ValueError:\n",
    "                        maf = 0.0\n",
    "                    if maf > 0.5:\n",
    "                        large_maf=True\n",
    "                        maf = 1 - maf\n",
    "            except Exception:\n",
    "                raise ValueError(\"VCF line {}:{} does not have valid allele frequency field {}!\".\\\n",
    "                                 format(self.vcf.GetChrom(), self.vcf.GetPosition(), self.af_info))\n",
    "            data.variants.append([self.vcf.GetChrom(), self.vcf.GetPosition(), self.name, maf])\n",
    "            # for each family assign member genotype if the site is non-trivial to the family\n",
    "            for k in data.families:\n",
    "                gs = self.vcf.GetGenotypes(data.famsampidx[k])\n",
    "                if len(data.freq_by_fam) > 0:\n",
    "                    popidx=self.af_info.index(data.freq_by_fam[k])\n",
    "                    if large_maf[popidx]:\n",
    "                        tmpgs=[]\n",
    "                        for tmpg in gs:\n",
    "                            if tmpg=='00':\n",
    "                                tmpgs.append(tmpg)\n",
    "                            else:\n",
    "                                tmpgs.append(''.join([str(3-int(tg)) for tg in tmpg]))\n",
    "                        gs=tuple(tmpgs)\n",
    "                else:\n",
    "                    if large_maf:\n",
    "                        tmpgs=[]\n",
    "                        for tmpg in gs:\n",
    "                            if tmpg=='00':\n",
    "                                tmpgs.append(tmpg)\n",
    "                            else:\n",
    "                                tmpgs.append(''.join([str(3-int(tg)) for tg in tmpg]))\n",
    "                        gs=tuple(tmpgs)\n",
    "                for person, g in zip(data.families[k], gs):\n",
    "                    data.genotype_all[person].append(g)\n",
    "                if len(set(''.join(gs))) <= 1:\n",
    "                    # skip monomorphic gs\n",
    "                    continue\n",
    "                else:\n",
    "                    if len(set(''.join([x for x in gs if x != \"00\"]))) <= 1:\n",
    "                        data.wtvar[k].append(varIdx)\n",
    "                    # this variant is found in the family\n",
    "                    data.famvaridx[k].append(varIdx)\n",
    "                    for person, g in zip(data.families[k], gs):\n",
    "                        data[person].append(g)\n",
    "            varIdx += 1\n",
    "        #\n",
    "        if varIdx == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            if not self.include_vars_file is None:\n",
    "                with open(self.include_vars_file) as invar_fh:\n",
    "                    for invar_line in invar_fh:\n",
    "                        chrom, pos = invar_line.split()\n",
    "                        for vidx,v in enumerate(data.variants):\n",
    "                            if v[0] == chrom and v[1] == int(pos):\n",
    "                                data.include_vars.append(\"{}\".format(pos))\n",
    "                                break\n",
    "            else:\n",
    "                data.include_vars = [\"{}\".format(item[1]) for item in data.variants]\n",
    "            with env.variants_counter.get_lock():\n",
    "                env.variants_counter.value += varIdx\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def getRegion(self, region):\n",
    "        self.chrom, self.startpos, self.endpos, self.name = region[:4]\n",
    "        self.startpos = int(self.startpos)\n",
    "        self.endpos = int(self.endpos) + 1\n",
    "        if self.chrom in ['X','23']:\n",
    "            if self.xchecker.check(self.startpos) or self.xchecker.check(self.endpos):\n",
    "                self.chrom = 'XY'\n",
    "        if self.chrom in ['Y','24']:\n",
    "            if self.ychecker.check(self.startpos) or self.ychecker.check(self.endpos):\n",
    "                self.chrom = 'XY'\n",
    "        if self.chr_prefix and not self.chrom.startswith(self.chr_prefix):\n",
    "            self.chrom = self.chr_prefix + self.chrom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.MarkerMaker class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkerMaker:\n",
    "    def __init__(self, wsize, maf_cutoff = None,single_markers=False,recomb_max = 1,af_info=None,freq_by_fam=False,rsq=0.0,mle=False,rvhaplo=False,recomb_perfam=True):\n",
    "        self.missings = (\"0\", \"0\")\n",
    "        self.gtconv = {'1':0, '2':1}\n",
    "        self.recomb_max = recomb_max\n",
    "        self.haplotyper = cstatgen.HaplotypingEngine(verbose = env.debug)\n",
    "        self.af_info = af_info\n",
    "        self.freq_by_fam = freq_by_fam\n",
    "        self.rsq=rsq\n",
    "        self.mle=mle          #use MLE estimate from families for MAF\n",
    "        self.count= not mle   #count founder alleles to estimate MAF\n",
    "        self.rvhaplo=rvhaplo\n",
    "        self.recomb_perfam=recomb_perfam\n",
    "        if wsize == 0 or wsize >= 1:\n",
    "            self.r2 = None\n",
    "        else:\n",
    "            self.r2 = wsize\n",
    "        self.coder = cstatgen.HaplotypeCoder(wsize)\n",
    "        self.maf_cutoff = maf_cutoff\n",
    "        self.single_markers = single_markers\n",
    "        self.name = None\n",
    "\n",
    "    def apply(self, data):\n",
    "        # temp raw haplotype, maf and variant names data\n",
    "        haplotypes = OrderedDict()\n",
    "        mafs = {}   ##Per fam per variant\n",
    "        uniq_vars = []\n",
    "        exclude_vars = []\n",
    "        varnames = {}\n",
    "        recombPos = {}\n",
    "        #try:\n",
    "            # haplotyping plus collect found allele counts\n",
    "            # and computer founder MAFS\n",
    "        self.__Haplotype(data, haplotypes, mafs, varnames,recombPos,uniq_vars,exclude_vars)\n",
    "        print('__Haplotype',haplotypes, mafs, varnames,recombPos,uniq_vars,exclude_vars)\n",
    "        self.haplotypes, self.mafs, self.varnames = haplotypes, mafs, varnames  ###anno\n",
    "        if len(varnames):\n",
    "            if not any ([len(varnames[x]) - 1 for x in varnames]):\n",
    "                # all families have only one variant\n",
    "                self.__AssignSNVHaplotypes(data, haplotypes, mafs, varnames)\n",
    "            else:\n",
    "                # calculate LD clusters using founder haplotypes\n",
    "                clusters = self.__ClusterByLD(data, haplotypes, varnames)\n",
    "                # recoding the genotype of the region\n",
    "                self.__CodeHaplotypes(data, haplotypes, mafs, varnames, clusters)\n",
    "    #except Exception as e:\n",
    "        #    if env.debug:\n",
    "        #        raise\n",
    "        #    return -1\n",
    "        self.__FormatHaplotypes(data,recombPos,varnames,uniq_vars)\n",
    "        return 0\n",
    "\n",
    "    def __getMLEfreq(self,data, markers_to_analyze, pos_all, families, rsq, output_log):\n",
    "        output_sample=[]\n",
    "        mle_mafs={}\n",
    "        if len(markers_to_analyze)==0:\n",
    "            return mle_mafs\n",
    "        for fam in families:\n",
    "            for person in data.tfam.sort_family(fam):\n",
    "                output_sample.append([])\n",
    "                last_ele=len(output_sample)-1\n",
    "                output_sample[last_ele] = data.tfam.samples[person][:-1]\n",
    "                if person in data.samples:\n",
    "                    for marker in markers_to_analyze:\n",
    "                        idx=int(marker.split('-')[0][1:])\n",
    "                        output_sample[last_ele].append(data.genotype_all[person][idx])\n",
    "                else:\n",
    "                    output_sample[last_ele].extend([\"00\"] * len(markers_to_analyze))\n",
    "        with stdoutRedirect(to = output_log):\n",
    "            af = self.haplotyper.Execute(data.chrom, markers_to_analyze, pos_all, output_sample, rsq, output_log,False)\n",
    "        with open(output_log) as mle_fh:\n",
    "            for line in mle_fh:\n",
    "                if line.startswith('V'):\n",
    "                    tmp_eles = line.split(':')\n",
    "                    if tmp_eles[0] not in mle_mafs:\n",
    "                        freqs=tmp_eles[1].split()\n",
    "                        mle_maf = float(freqs[1])\n",
    "                        if mle_maf>0.5:\n",
    "                            mle_mafs[tmp_eles[0]]=float(\"%.9f\"%(1-mle_maf))\n",
    "                        else:\n",
    "                            #alt allele is more frequent\n",
    "                            mle_mafs[tmp_eles[0]]=float(\"%.9f\"%mle_maf)\n",
    "                            marker_idx=int(tmp_eles[0].split('-')[0][1:])\n",
    "                            for fam in families:\n",
    "                                if marker_idx not in data.famvaridx[fam]:\n",
    "                                    continue\n",
    "                                tmp_famvaridx=data.famvaridx[fam].index(marker_idx)\n",
    "                                for person in data.families[fam]:\n",
    "                                    tmpg=data.genotype_all[person][marker_idx]\n",
    "                                    tmpg_switch=''.join([str(3-int(tg)) for tg in tmpg]) if tmpg!='00' else tmpg\n",
    "                                    data.genotype_all[person][marker_idx]=tmpg_switch\n",
    "                                    tmpg2=data[person][tmp_famvaridx]\n",
    "                                    tmpg_switch2=''.join([str(3-int(tg)) for tg in tmpg2]) if tmpg2!='00' else tmpg2\n",
    "                                    data[person][tmp_famvaridx]=tmpg_switch2\n",
    "        return mle_mafs\n",
    "\n",
    "    def __computefounderfreq(self,data, families):\n",
    "        #count founder alleles to estimate MAF\n",
    "        total_founder_alleles=0\n",
    "        tmp_haplotypes=OrderedDict()\n",
    "        tmp_mafs={}\n",
    "        for item in families:\n",
    "            tmp_haplotypes[item] = self.__PedToHaplotype(data.getFamSamples(item))\n",
    "            # count founder alleles\n",
    "            for hap in tmp_haplotypes[item]:\n",
    "                if not data.tfam.is_founder(hap[1]):\n",
    "                    continue\n",
    "                total_founder_alleles+=1.0\n",
    "                for idxv, v in enumerate(data.getFamVariants(item,style=\"map\")[0]):\n",
    "                    if v not in tmp_mafs:\n",
    "                        # [#alt, #haplotypes]\n",
    "                        tmp_mafs[v] = [0, 0]\n",
    "                    gt = hap[2 + idxv][1] if hap[2 + idxv][0].isupper() else hap[2 + idxv][0]\n",
    "                    if not gt == \"?\":\n",
    "                    #genotyped\n",
    "                        tmp_mafs[v][0] += self.gtconv[gt]\n",
    "                    else:\n",
    "                    #genotype is missing\n",
    "                        tmp_mafs[v][1] -= 1.0\n",
    "        #compute MAFs based on counts\n",
    "        for v in tmp_mafs:\n",
    "            if type(tmp_mafs[v]) is not list:\n",
    "                continue\n",
    "            tmp_mafs[v] = tmp_mafs[v][0] / (tmp_mafs[v][1]+total_founder_alleles) if tmp_mafs[v][1]+total_founder_alleles > 0 else 0.0\n",
    "        return tmp_mafs\n",
    "\n",
    "    def __Haplotype(self, data, haplotypes, mafs, varnames,recombPos,uniq_vars,exclude_vars):\n",
    "        '''genetic haplotyping. haplotypes stores per family data'''\n",
    "        # FIXME: it is SWIG's (2.0.12) fault not to properly destroy the object \"Pedigree\" in \"Execute()\"\n",
    "        # So there is a memory leak here which I tried to partially handle on C++\n",
    "        #\n",
    "        # Per family haplotyping\n",
    "        #\n",
    "        self.markers = [\"V{}-{}\".format(idx, item[1]) for idx, item in enumerate(data.variants)]\n",
    "        for item in data.families:\n",
    "            varnames[item], positions, vcf_mafs = data.getFamVariants(item, style = \"map\")\n",
    "            if len(varnames[item]) == 0:\n",
    "                for person in data.families[item]:\n",
    "                    data[person] = self.missings\n",
    "                continue\n",
    "            if env.debug:\n",
    "                with env.lock:\n",
    "                    sys.stderr.write('\\n'.join(['\\t'.join(x) for x in data.getFamSamples(item)]) + '\\n\\n')\n",
    "            # haplotyping\n",
    "            self.hap = {}\n",
    "            with env.lock:\n",
    "                if not env.prephased:\n",
    "                    tmp_log_output=env.tmp_log + str(os.getpid()) \n",
    "                    #with stdoutRedirect(to = tmp_log_output + '.log'):\n",
    "                    haplotypes[item] = self.haplotyper.Execute(data.chrom, varnames[item], sorted(positions), \n",
    "                                                                   data.getFamSamples(item), self.rsq, tmp_log_output)[0]\n",
    "                    print('haplotyper execute',item,haplotypes[item])\n",
    "                    self.hap[item] = haplotypes[item]\n",
    "                else:\n",
    "                    haplotypes[item] = self.__PedToHaplotype(data.getFamSamples(item))\n",
    "           \n",
    "            if len(haplotypes[item]) == 0:\n",
    "                # C++ haplotyping implementation failed\n",
    "                with env.chperror_counter.get_lock():\n",
    "                    env.chperror_counter.value += 1\n",
    "            # either use privided MAF or computer MAF\n",
    "            if all(vcf_mafs):\n",
    "                for idx, v in enumerate(varnames[item]):\n",
    "                    if v not in mafs:\n",
    "                        mafs[v] = vcf_mafs[idx]\n",
    "            else:\n",
    "                # count founder alleles\n",
    "                for hap in haplotypes[item]:\n",
    "                    if not data.tfam.is_founder(hap[1]):\n",
    "                        continue\n",
    "                    for idxv, v in enumerate(varnames[item]):\n",
    "                        if v not in mafs:\n",
    "                            # [#alt, #haplotypes]\n",
    "                            mafs[v] = [0, 0]\n",
    "                        gt = hap[2 + idxv][1] if hap[2 + idxv][0].isupper() else hap[2 + idxv][0]\n",
    "                        if not gt == \"?\":\n",
    "                            mafs[v][0] += self.gtconv[gt]\n",
    "                            mafs[v][1] += 1.0\n",
    "        #\n",
    "        # Compute founder MAFs\n",
    "        #\n",
    "        for v in mafs:\n",
    "            if type(mafs[v]) is not list:\n",
    "                continue\n",
    "            mafs[v] = mafs[v][0] / mafs[v][1] if mafs[v][1] > 0 else 0.0\n",
    "        if env.debug:\n",
    "            with env.lock:\n",
    "                print(\"variant mafs = \", mafs, \"\\n\", file = sys.stderr)\n",
    "        #\n",
    "        # Drop some variants if maf is greater than given threshold\n",
    "        #\n",
    "        if self.maf_cutoff is not None:\n",
    "            exclude_vars = []\n",
    "            for v in mafs.keys():\n",
    "                if mafs[v] > self.maf_cutoff:\n",
    "                    exclude_vars.append(v)\n",
    "            for i in haplotypes.keys():\n",
    "                haplotypes[i] = listit(haplotypes[i])\n",
    "                for j in range(len(haplotypes[i])):\n",
    "                    haplotypes[i][j] = haplotypes[i][j][:2] + \\\n",
    "                      [x for idx, x in enumerate(haplotypes[i][j][2:]) if varnames[i][idx] not in exclude_vars]\n",
    "                varnames[i] = [x for x in varnames[i] if x not in exclude_vars]\n",
    "                # handle trivial data\n",
    "                if len(varnames[i]) == 0:\n",
    "                    for person in data.families[i]:\n",
    "                        data[person] = self.missings\n",
    "                    del varnames[i]\n",
    "                    del haplotypes[i]\n",
    "            # count how many variants are removed\n",
    "            with env.commonvar_counter.get_lock():\n",
    "                env.commonvar_counter.value += len(exclude_vars)\n",
    "\n",
    "    def __ClusterByLD(self, data, haplotypes, varnames):\n",
    "        if self.r2 is None:\n",
    "            return None\n",
    "        # get founder haplotypes\n",
    "        founder_haplotypes = []\n",
    "        markers = sorted(set(itertools.chain(*varnames.values())), key = lambda x: int(x.split(\"-\")[0][1:]))\n",
    "        for item in haplotypes:\n",
    "            for ihap, hap in enumerate(haplotypes[item]):\n",
    "                if not data.tfam.is_founder(hap[1]):\n",
    "                    continue\n",
    "                gt = [hap[2 + varnames[item].index(v)] if v in varnames[item] else '?' for v in markers]\n",
    "                founder_haplotypes.append((\"{}-{}\".format(hap[1], ihap % 2), \"\".join([x[1] if x[0].isupper() else x[0] for x in gt])))\n",
    "        # calculate LD blocks, use r2 measure\n",
    "        ld = Align.create(founder_haplotypes).matrixLD(validCharacters=\"12\")[\"r2\"]\n",
    "        blocks = []\n",
    "        for j in ld:\n",
    "            block = [j]\n",
    "            for k in ld[j]:\n",
    "                if ld[j][k] > self.r2:\n",
    "                    block.append(k)\n",
    "            if len(block) > 1:\n",
    "                blocks.append(block)\n",
    "        self.ld, self.blocks = ld, blocks\n",
    "        # get LD clusters\n",
    "        clusters = [[markers[idx] for idx in item] for item in list(connected_components(blocks))]\n",
    "        if env.debug:\n",
    "            with env.lock:\n",
    "                print(\"LD blocks: \", blocks, file = sys.stderr)\n",
    "                print(\"LD clusters: \", clusters, file = sys.stderr)\n",
    "        return clusters\n",
    "\n",
    "\n",
    "    def __CodeHaplotypes(self, data, haplotypes, mafs, varnames, clusters):\n",
    "        # apply CHP coding\n",
    "        for item in data.famvaridx:\n",
    "            if item not in haplotypes and data[data.families[item][0]] != ('0','0'):\n",
    "                # when only wild-type haplotypes are present in a family, still code them instead of ignoring the family\n",
    "                if self.freq_by_fam:\n",
    "                    pop=data.freq_by_fam[item]\n",
    "                    try:\n",
    "                        varnames[item]=data.total_varnames[pop]\n",
    "                        mafs[item]=data.total_mafs[pop]\n",
    "                    except:\n",
    "                        continue\n",
    "                else:\n",
    "                    varnames[item]=data.total_varnames['pop']\n",
    "                    mafs[item]=data.total_mafs\n",
    "                haplotypes[item]=[]\n",
    "                for person in data.families[item]:\n",
    "                    tmp_person=[item, person]\n",
    "                    if '00' in data[person]:\n",
    "                        tmp_person+=['?:']*len(varnames[item])\n",
    "                    else:\n",
    "                        tmp_person+=['1:']*len(varnames[item])\n",
    "                    haplotypes[item].append(tmp_person)\n",
    "                    haplotypes[item].append(tmp_person)\n",
    "            elif item in haplotypes:\n",
    "                nonvar_hap_flag=False\n",
    "                #determine if wild-type haplotype is present in a family\n",
    "                for hap in haplotypes[item]:\n",
    "                    tmp_genes=[]\n",
    "                    for tmpa in hap[2:]:\n",
    "                        if 'A' in tmpa or 'B' in tmpa:\n",
    "                            tmp_genes.append(tmpa[1])\n",
    "                        else:\n",
    "                            tmp_genes.append(tmpa[0])\n",
    "                    if set(tmp_genes)==set(['1']):\n",
    "                        #non variant haplotype\n",
    "                        nonvar_hap_flag=True\n",
    "                        break\n",
    "                if not nonvar_hap_flag:\n",
    "                    #if family don't have wild-type haplotype, add a fake one to ensure correct coding\n",
    "                    var_num=len(varnames[item])\n",
    "                    fake_person=[item, 'FAKEPERSON']+['1:']*var_num\n",
    "                    haplotypes[item].append(fake_person)\n",
    "                for hidx,hap in enumerate(haplotypes[item]):\n",
    "                    if hap[1] in data.missing_persons:\n",
    "                        missing_person=[item,hap[1]]+['?:']*len(varnames[item])\n",
    "                        haplotypes[item][hidx]=missing_person\n",
    "\n",
    "        if not clusters is None:\n",
    "            clusters_idx = [[[varnames[item].index(x) for x in y] for y in clusters] for item in haplotypes]\n",
    "        else:\n",
    "            clusters_idx = [[[]] for item in haplotypes]\n",
    "        if env.debug:\n",
    "            for item in haplotypes:\n",
    "                with env.lock:\n",
    "                    print(varnames[item],file=sys.stderr)\n",
    "                    print(\"hap{0}\\t{1}\\n\".format(item,haplotypes[item]),file=sys.stderr)\n",
    "        self.coder.Execute(haplotypes.values(), [[mafs[item][v] for v in varnames[item]] for item in haplotypes], clusters_idx)\n",
    "        if env.debug:\n",
    "            with env.lock:\n",
    "                if clusters:\n",
    "                    print(\"Family LD clusters: \", clusters_idx, \"\\n\", file = sys.stderr)\n",
    "                self.coder.Print()\n",
    "        # line: [fid, sid, hap1, hap2]\n",
    "        for line in self.coder.GetHaplotypes():\n",
    "            if not line[1] in data:\n",
    "                # this sample is not in VCF file. Every variant site should be missing\n",
    "                # they have to be skipped for now\n",
    "                continue\n",
    "            data[line[1]] = (line[2].split(','), line[4].split(','))\n",
    "            #sub-region count for each sample individual\n",
    "            superMarkerCount=len(data[line[1]][0])\n",
    "            if line[0] not in data.patterns:\n",
    "                data.patterns[line[0]]=[[] for x in range(superMarkerCount)]\n",
    "            for t_Marker in range(superMarkerCount):\n",
    "                t_pat1=line[3].split(',')[t_Marker]\n",
    "                t_pat2=line[5].split(',')[t_Marker]\n",
    "                if t_pat1 not in data.patterns[line[0]][t_Marker]:\n",
    "                    data.patterns[line[0]][t_Marker].append(t_pat1)\n",
    "                if t_pat2 not in data.patterns[line[0]][t_Marker]:\n",
    "                    data.patterns[line[0]][t_Marker].append(t_pat2)\n",
    "            if len(data[line[1]][0]) > data.superMarkerCount:\n",
    "                data.superMarkerCount = len(data[line[1]][0])\n",
    "        # get MAF\n",
    "        for item in data.famvaridx:\n",
    "            if item not in haplotypes:\n",
    "                for person in data.families[item]:\n",
    "                    data[person]=(['0']*data.superMarkerCount,['0']*data.superMarkerCount)\n",
    "        for item in haplotypes:\n",
    "            data.maf[item] = self.coder.GetAlleleFrequencies(item)\n",
    "            if not len(data.maf[item][0]):\n",
    "                continue\n",
    "            data.varnames_by_fam[item]=varnames[item]\n",
    "            wt_maf=0\n",
    "            if self.freq_by_fam:\n",
    "                try:\n",
    "                    wt_maf=data.wt_maf[data.freq_by_fam[item]]\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                wt_maf=data.wt_maf['pop']\n",
    "            tmp_data_maf=[]\n",
    "            for v in data.maf[item]:\n",
    "                if len(v)==1:\n",
    "                    tmp_data_maf.append((v[0],1-v[0]))\n",
    "                else:\n",
    "                    if np.sum(v)<1:\n",
    "                        tmp_ratio=sum(v[1:])/(1-wt_maf)\n",
    "                        tmp_list=[wt_maf]\n",
    "                        if tmp_ratio==0:\n",
    "                            tmp_list.append(1-wt_maf)\n",
    "                        else:\n",
    "                            for tmpv in v[1:]:\n",
    "                                tmp_list.append(tmpv/tmp_ratio)\n",
    "                        tmp_data_maf.append(tuple(tmp_list))\n",
    "                    else:\n",
    "                        tmp_data_maf.append(v)\n",
    "            data.maf[item]=tuple(tmp_data_maf)\n",
    "        if env.debug:\n",
    "            with env.lock:\n",
    "                print(\"marker freqs = \", data.maf, \"\\n\", file = sys.stderr)\n",
    "\n",
    "\n",
    "    def __AssignSNVHaplotypes(self, data, haplotypes, mafs, varnames):\n",
    "        for item in haplotypes:\n",
    "            # each person's haplotype\n",
    "            data.varnames_by_fam[item]=varnames[item]\n",
    "            token = ''\n",
    "            for idx,line in enumerate(haplotypes[item]):\n",
    "                if line[1] in data.missing_persons:\n",
    "                    data[line[1]]=('0','0')\n",
    "                else:\n",
    "                    if not idx % 2:\n",
    "                        token = line[2][1] if line[2][0].isupper() else line[2][0]\n",
    "                        if token=='?':\n",
    "                            token='0'\n",
    "                    else:\n",
    "                        tmp_token = line[2][1] if line[2][0].isupper() else line[2][0]\n",
    "                        if tmp_token=='?':\n",
    "                            tmp_token='0'\n",
    "                        data[line[1]] = (token, tmp_token)\n",
    "\n",
    "            # get MAF\n",
    "            data.maf[item] = [(1 - mafs[item][varnames[item][0]], mafs[item][varnames[item][0]])]\n",
    "            data.maf[item] = tuple(tuple(np.array(v) / np.sum(v)) if np.sum(v) else v\n",
    "                              for v in data.maf[item])\n",
    "        for item in data.famvaridx:\n",
    "            if item not in haplotypes and data[data.families[item][0]] != ('0','0'):\n",
    "                for person in data.families[item]:\n",
    "                    if '00' in data[person]:\n",
    "                        data[person]=('0','0')\n",
    "                    else:\n",
    "                        data[person]=('1','1')\n",
    "                t_maf=0\n",
    "                if self.freq_by_fam:\n",
    "                    try:\n",
    "                        t_maf=data.wt_maf[data.freq_by_fam[item]]\n",
    "                    except:\n",
    "                        for person in data.families[item]:\n",
    "                            data[person]=('0','0')\n",
    "                else:\n",
    "                    t_maf=data.wt_maf['pop']\n",
    "                data.maf[item]=((t_maf,1-t_maf),)\n",
    "        if env.debug:\n",
    "            with env.lock:\n",
    "                print(\"marker freqs = \", data.maf, \"\\n\", file = sys.stderr)\n",
    "\n",
    "\n",
    "    def __FormatHaplotypes(self, data,recombPos,varnames,uniq_vars):\n",
    "        # Reformat sample genotypes\n",
    "        ## Linhai Edit: Reformat to deal with recombination events in families\n",
    "        if self.recomb_perfam:\n",
    "            #code recombination per family basis, no need to consider overlap across families\n",
    "            for person in data:\n",
    "                if type(data[person]) is not tuple:\n",
    "                    data[person] = self.missings\n",
    "                    continue\n",
    "                diff = data.superMarkerCount - len(data[person][0])\n",
    "                data[person] = zip(*data[person])\n",
    "                if diff > 0:\n",
    "                    data[person].extend([self.missings] * diff)\n",
    "        else:\n",
    "            #code recombination across families to generate sub-regions that extend across families\n",
    "            tmp_combined_recombPos={}\n",
    "            sorted_var = sorted(uniq_vars, key=lambda x: int(x.split('-')[0][1:]))\n",
    "            for fam in data.maf.keys():\n",
    "                if len(data.maf[fam])>1:\n",
    "                    for pair in sorted(recombPos[fam].keys(), key=lambda x:(sorted_var.index(x[0]),sorted_var.index(x[1]))):\n",
    "                        if pair[1] == varnames[fam][0]:\n",
    "                            ##remove recombination event if occurred at 1st RV\n",
    "                            del recombPos[fam][pair]\n",
    "                            continue\n",
    "                        if fam not in tmp_combined_recombPos:\n",
    "                            tmp_combined_recombPos[fam]=[pair]\n",
    "                        else:\n",
    "                            tmp_combined_recombPos[fam].append(pair)\n",
    "            tmp_all_recombs=[pair for pairs in tmp_combined_recombPos.values() for pair in pairs]\n",
    "            sorted_combined_recombPos=sorted(list(set(tmp_all_recombs)),key=lambda x:(sorted_var.index(x[0]),sorted_var.index(x[1])))\n",
    "            recomb_fams=tmp_combined_recombPos.keys()\n",
    "            ##get sub-regions that applies to all families\n",
    "            for varidx,variant in enumerate(sorted_var):\n",
    "                included_fams=len(recomb_fams)\n",
    "                for recomb_region in sorted_combined_recombPos:\n",
    "                    if varidx > sorted_var.index(recomb_region[0]) and varidx < sorted_var.index(recomb_region[1]):\n",
    "                        ##if the variant is in a recombination region\n",
    "                        included_fams-=1\n",
    "                if included_fams==len(recomb_fams):\n",
    "                    if data.combined_regions==[]:\n",
    "                        data.combined_regions.append([variant])\n",
    "                    else:\n",
    "                        if sorted_var.index(data.combined_regions[-1][-1])==varidx-1:\n",
    "                            neighbour_recomb_flag=False\n",
    "                            for recomb_region in sorted_combined_recombPos:\n",
    "                                recomb_idx=sorted_var.index(recomb_region[1])\n",
    "                                if recomb_idx==varidx:\n",
    "                                    neighbour_recomb_flag=True\n",
    "                                    break\n",
    "                                elif recomb_idx>varidx:\n",
    "                                    break\n",
    "                            if neighbour_recomb_flag:\n",
    "                                data.combined_regions.append([variant])\n",
    "                            else:\n",
    "                                data.combined_regions[-1].append(variant)\n",
    "                        else:\n",
    "                            data.combined_regions.append([variant])\n",
    "            ##Get the markers in families compliant with the sub_regions\n",
    "            for sub_region in data.combined_regions:\n",
    "                markers={}\n",
    "                for fam in recomb_fams:\n",
    "                    pidx=0\n",
    "                    for pair in sorted(recombPos[fam].keys(), key=lambda x:(sorted_var.index(x[0]),sorted_var.index(x[1]))):\n",
    "                        sub_region_start=sorted_var.index(sub_region[0])\n",
    "                        sub_region_end=sorted_var.index(sub_region[-1])\n",
    "                        recomb_start=sorted_var.index(pair[0])\n",
    "                        recomb_end=sorted_var.index(pair[1])\n",
    "                        if sub_region_end <= recomb_start:\n",
    "                            markers[fam]=pidx\n",
    "                            break\n",
    "                        elif sub_region_end > recomb_start and sub_region_start>recomb_start and sub_region_end<recomb_end:\n",
    "                            ##within the recombination region\n",
    "                            markers[fam]=None\n",
    "                            break\n",
    "                        pidx+=1\n",
    "                    if fam not in markers:\n",
    "                        markers[fam]=pidx\n",
    "                data.complied_markers.append(markers)\n",
    "            data.superMarkerCount=len(data.combined_regions)\n",
    "            #coordinates for sub_regions\n",
    "            data.coordinates_by_region=[(int(sub_region[0].split('-')[1])+int(sub_region[-1].split('-')[1]))/2 for sub_region in data.combined_regions]\n",
    "            for person in data:\n",
    "                if type(data[person]) is not tuple:\n",
    "                    data[person] = self.missings\n",
    "                    continue\n",
    "                diff = data.superMarkerCount - len(data[person][0])\n",
    "                data[person] = zip(*data[person])\n",
    "                if diff > 0:\n",
    "                    if len(data[person]) == 1:\n",
    "                        ##only one whole region with no recombination\n",
    "                        data[person].extend(data[person] * diff)\n",
    "                    else:\n",
    "                        famid=''\n",
    "                        for fam in data.complied_markers[0].keys():\n",
    "                            if person in data.families[fam]:\n",
    "                                famid=fam\n",
    "                        complied_data=[]\n",
    "                        for marker in data.complied_markers:\n",
    "                            complied_data.append(data[person][marker[famid]])\n",
    "                        data[person]=complied_data\n",
    "\n",
    "    def __PedToHaplotype(self, ped):\n",
    "        '''convert prephased ped format to haplotype format.\n",
    "        Input: e.g. [['13346', '5888', '0', '0', '1', '11', '11', '11'], ['13346', '5856', '0', '0', '2', '12', '12', '12'], ['13346', '5920', '5888', '5856', '1', '12', '12', '12'], ['13346', '6589', '5888', '5856', '1', '11', '11', '11']]\n",
    "        Output: e.g. (('13346', '5856', '1:', '1:', '1:'), ('13346', '5856', '2:', '2:', '2:'), ('13346', '5888', '1:', '1:', '1:'), ('13346', '5888', '1:', '1:', '1:'), ('13346', '6589', '1:', '1|', '1|'), ('13346', '6589', '1:', '1|', '1|'), ('13346', '5920', '2:', '2|', '2|'), ('13346', '5920', '1:', '1|', '1|'))\n",
    "        '''\n",
    "        haps = []\n",
    "        for item in ped:\n",
    "            entry = [item[0], item[1]] + [x[0] + ':' if x[0] != '0' else '?:' for x in item[5:]]\n",
    "            haps.append(tuple(entry))\n",
    "            entry = [item[0], item[1]] + [x[1] + ':' if x[1] != '0' else '?:' for x in item[5:]]\n",
    "            haps.append(tuple(entry))\n",
    "        return tuple(haps)\n",
    "\n",
    "    def getRegion(self, region):\n",
    "        self.name = region[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.LinkageWriter class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkageWriter:\n",
    "    def __init__(self, num_missing_append = 0):\n",
    "        self.chrom = self.prev_chrom = self.name = self.distance = self.distance_avg = self.distance_m = self.distance_f = None\n",
    "        self.distance_by_region=[]\n",
    "        self.mid_position=None\n",
    "        self.reset()\n",
    "        self.missings = [\"0\", \"0\"]\n",
    "        self.num_missing = num_missing_append\n",
    "\n",
    "    def apply(self, data):\n",
    "        if self.chrom != self.prev_chrom:\n",
    "            if self.prev_chrom is None:\n",
    "                self.prev_chrom = self.chrom\n",
    "            else:\n",
    "                # new chrom entered,\n",
    "                # commit whatever is in buffer before accepting new data\n",
    "                self.commit()\n",
    "        # write tped output\n",
    "        position = str(data.getMidPosition())\n",
    "        if data.superMarkerCount <= 1:\n",
    "            # genotypes\n",
    "            gs = [data[s][0] for s in data.samples]\n",
    "            if len(set(gs)) == 1:\n",
    "                # everyone's genotype is the same (most likely missing or monomorphic)\n",
    "                return 2\n",
    "            self.tped += env.delimiter.join([self.chrom, self.name, self.distance, position] + \\\n",
    "                list(itertools.chain(*gs)) + self.missings*self.num_missing) + \"\\n\"\n",
    "            # freqs\n",
    "            for k in data.maf:\n",
    "                self.freq += env.delimiter.join([k, self.name] + map(str, data.maf[k][0])) + \"\\n\"\n",
    "        else:\n",
    "            # have to expand each region into mutiple chunks to account for different recomb points\n",
    "            gs = zip(*[data[s] for s in data.samples])\n",
    "            # sub-chunk id\n",
    "            cid = 0\n",
    "            skipped_chunk = []\n",
    "            self.distance_by_region=[self.distance_converter(x,int(position)) for x in data.coordinates_by_region]\n",
    "            for idx, g in enumerate(gs):\n",
    "                if len(set(g)) == 1:\n",
    "                    skipped_chunk.append(idx)\n",
    "                    continue\n",
    "                cid += 1\n",
    "                self.tped += \\\n",
    "                  env.delimiter.join([self.chrom, '{}[{}]'.format(self.name, cid), self.distance_by_region[cid-1], position] + \\\n",
    "                  list(itertools.chain(*g)) + self.missings*self.num_missing) + \"\\n\"\n",
    "            if cid == 0:\n",
    "                # everyone's genotype is the same (most likely missing or monomorphic)\n",
    "                return 2\n",
    "            # freqs\n",
    "            for k in data.maf:\n",
    "                cid = 0\n",
    "                for idx in range(data.superMarkerCount):\n",
    "                    if idx in skipped_chunk:\n",
    "                        continue\n",
    "                    if not data.complied_markers:\n",
    "                        #if recombination coded per family instead of across families\n",
    "                        if idx >= len(data.maf[k]):\n",
    "                            break\n",
    "                        cid += 1\n",
    "                        self.freq += env.delimiter.join([k, '{}[{}]'.format(self.name, cid)] + \\\n",
    "                                                    map(str, data.maf[k][idx])) + \"\\n\"\n",
    "                    else:\n",
    "                        if len(data.maf[k])>1:\n",
    "                            matched_idx=data.complied_markers[idx][k]\n",
    "                            cid += 1\n",
    "                            self.freq += env.delimiter.join([k, '{}[{}]'.format(self.name, cid)] + \\\n",
    "                                                map(str, data.maf[k][matched_idx])) + \"\\n\"\n",
    "                        elif len(data.maf[k])==1:\n",
    "                            cid += 1\n",
    "                            self.freq += env.delimiter.join([k, '{}[{}]'.format(self.name, cid)] + \\\n",
    "                                                map(str, data.maf[k][0])) + \"\\n\"\n",
    "        if data.combined_regions:\n",
    "            self.chp += \"CHP Super Marker positions: \"+repr(data.combined_regions)+\"\\n\"\n",
    "        for item in data.varnames_by_fam:\n",
    "            try:\n",
    "                pattern_txt=[tuple(sorted(data.patterns[item][tmarker],key=lambda x:x.count('2') )) for tmarker in range(len(data.patterns[item]))]\n",
    "            except:\n",
    "                pattern_txt=''\n",
    "            self.varfam += \"{}\\t{}\\t{}\\n\".format(item,data.varnames_by_fam[item],pattern_txt)\n",
    "        if self.counter < env.batch:\n",
    "            self.counter += data.superMarkerCount\n",
    "        else:\n",
    "            self.commit()\n",
    "        return 0\n",
    "\n",
    "    def commit(self):\n",
    "        if self.tped:\n",
    "            with env.lock:\n",
    "                with open(os.path.join(env.tmp_cache, '{}.chr{}.tped'.format(env.output, self.prev_chrom)),\n",
    "                          'a') as f:\n",
    "                    f.write(self.tped)\n",
    "        if self.freq:\n",
    "            with env.lock:\n",
    "                with open(os.path.join(env.tmp_cache, '{}.chr{}.freq'.format(env.output, self.prev_chrom)),\n",
    "                          'a') as f:\n",
    "                    f.write(self.freq)\n",
    "        if self.chp:\n",
    "            with env.lock:\n",
    "                with open(os.path.join(env.tmp_cache, '{}.chr{}.chp'.format(env.output, self.prev_chrom)),\n",
    "                          'a') as f:\n",
    "                    f.write(self.chp)\n",
    "        if self.varfam:\n",
    "            with env.lock:\n",
    "                with open(os.path.join(env.tmp_cache, '{}.chr{}.var'.format(env.output, self.prev_chrom)),\n",
    "                          'a') as f:\n",
    "                    f.write(self.varfam)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.tped = ''\n",
    "        self.freq = ''\n",
    "        self.chp = ''\n",
    "        self.varfam = ''\n",
    "        self.counter = 0\n",
    "        self.prev_chrom = self.chrom\n",
    "\n",
    "    def distance_converter(self, x, mid_position):\n",
    "        delta=(x-mid_position)/1000000.0\n",
    "        distance='%.5f'%(float(self.distance_avg)+delta)\n",
    "        distance_m='%.5f'%(float(self.distance_m)+delta)\n",
    "        distance_f='%.5f'%(float(self.distance_f)+delta)\n",
    "        return \";\".join([distance,distance_m,distance_f])\n",
    "\n",
    "    def getRegion(self, region):\n",
    "        self.chrom = region[0]\n",
    "        self.name, self.distance_avg, self.distance_m, self.distance_f = region[3:]\n",
    "        self.distance = \";\".join([self.distance_avg, self.distance_m, self.distance_f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.EncoderWorker class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderWorker(Process):\n",
    "    def __init__(self, queue, length, data, extractor, coder, writer):\n",
    "        Process.__init__(self)\n",
    "        self.queue = queue\n",
    "        self.numGrps = float(length)\n",
    "        self.data = data\n",
    "        self.extractor = extractor\n",
    "        self.maker = coder\n",
    "        self.writer = writer\n",
    "\n",
    "    def report(self):\n",
    "        env.log('{:,d} units processed {{{:.2%}}} ...'.\\\n",
    "                format(env.success_counter.value, env.total_counter.value / self.numGrps), flush = True)\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            try:\n",
    "                region = self.queue.pop(0) if isinstance(self.queue, list) else self.queue.get()\n",
    "                if region is None:\n",
    "                    self.writer.commit()\n",
    "                    self.report()\n",
    "                    # total mendelian errors found\n",
    "                    with env.mendelerror_counter.get_lock():\n",
    "                        env.mendelerror_counter.value += self.maker.haplotyper.CountMendelianErrors()\n",
    "                    # total recombination events found\n",
    "                    with env.recomb_counter.get_lock():\n",
    "                        env.recomb_counter.value += self.maker.coder.CountRecombinations()\n",
    "                    break\n",
    "                else:\n",
    "                    with env.total_counter.get_lock():\n",
    "                        env.total_counter.value += 1\n",
    "                    self.extractor.getRegion(region)\n",
    "                    self.writer.getRegion(region)\n",
    "                    self.maker.getRegion(region)\n",
    "                    isSuccess = True\n",
    "                    for m in [self.extractor, self.maker, self.writer]:\n",
    "                        status = m.apply(self.data)\n",
    "                        if status == -1:\n",
    "                            with env.chperror_counter.get_lock():\n",
    "                                # previous module failed\n",
    "                                env.chperror_counter.value += 1\n",
    "                        if status == 1:\n",
    "                            with env.null_counter.get_lock():\n",
    "                                env.null_counter.value += 1\n",
    "                        if status == 2:\n",
    "                            with env.trivial_counter.get_lock():\n",
    "                                env.trivial_counter.value += 1\n",
    "                        if status != 0:\n",
    "                            isSuccess = False\n",
    "                            break\n",
    "                    if isSuccess:\n",
    "                        with env.success_counter.get_lock():\n",
    "                            env.success_counter.value += 1\n",
    "                    if env.total_counter.value % (env.batch * env.jobs) == 0:\n",
    "                        self.report()\n",
    "            except KeyboardInterrupt:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RData(dict):\n",
    "    def __init__(self, samples_vcf, tfam):\n",
    "        # tfam.samples: a dict of {sid:[fid, pid, mid, sex, trait], ...}\n",
    "        # tfam.families: a dict of {fid:[s1, s2 ...], ...}\n",
    "        self.tfam = tfam\n",
    "        # samples have to be in both vcf and tfam data\n",
    "        self.samples = OrderedDict([(k, tfam.samples[k]) for k in samples_vcf if k in tfam.samples])\n",
    "        # a dict of {fid:[member names], ...}\n",
    "        self.families = {k : [x for x in self.samples if x in tfam.families[k]] for k in tfam.families}\n",
    "        # a dict of {fid:[idx ...], ...}\n",
    "        self.famsampidx = {}\n",
    "        # a dict of {fid:[maf1, maf2 ...]}\n",
    "        self.maf = OrderedDict()\n",
    "        # reorder family samples based on order of VCF file\n",
    "        for k in self.families.keys():\n",
    "            if len(self.families[k]) == 0:\n",
    "                # skip families having no samples in VCF file\n",
    "                del self.families[k]\n",
    "            else:\n",
    "                self.famsampidx[k] = [i for i, x in enumerate(samples_vcf) if x in self.families[k]]\n",
    "        # a dict of {fid:[idx ...], ...}\n",
    "        self.famvaridx = {}\n",
    "        self.gss = {} #test line\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        for item in self.samples:\n",
    "            self[item] = []\n",
    "        self.variants = []\n",
    "        self.chrom = None\n",
    "        for k in self.families:\n",
    "            self.famvaridx[k] = []\n",
    "        self.maf = OrderedDict()\n",
    "        # superMarkerCount is the max num. of recombinant fragments among all fams\n",
    "        self.superMarkerCount = 0\n",
    "        self.gss = {} #test line\n",
    "\n",
    "    def getMidPosition(self):\n",
    "        if len(self.variants) == 0:\n",
    "            return None\n",
    "        return sum([x[1] for x in self.variants]) / len(self.variants)\n",
    "\n",
    "    def getFamVariants(self, fam, style = None):\n",
    "        if style is None:\n",
    "            return [item for idx, item in enumerate(self.variants) if idx in self.famvaridx[fam]]\n",
    "        elif style == \"map\":\n",
    "            names = []\n",
    "            pos = []\n",
    "            mafs = []\n",
    "            for idx in self.famvaridx[fam]:\n",
    "                names.append(\"V{}-{}\".format(idx, self.variants[idx][1]))\n",
    "                pos.append(self.variants[idx][1])\n",
    "                mafs.append(self.variants[idx][-1])\n",
    "            return names, pos, mafs\n",
    "        else:\n",
    "            raise ValueError(\"Unknown style '{}'\".format(style))\n",
    "\n",
    "    def getFamSamples(self, fam):\n",
    "        nvar = len([item for idx, item in enumerate(self.variants) if idx in self.famvaridx[fam]])\n",
    "        output = [[]] * len(self.tfam.families[fam])\n",
    "        for idx, item in enumerate(self.tfam.sort_family(fam)):\n",
    "            # sample info, first 5 columns of ped\n",
    "            output[idx] = self.tfam.samples[item][:-1]\n",
    "            # sample genotypes\n",
    "            if item in self.samples:\n",
    "                output[idx].extend(self[item])\n",
    "            else:\n",
    "                output[idx].extend([\"00\"] * nvar)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RegionExtractor:\n",
    "    '''Extract given genomic region from VCF\n",
    "    converting genotypes into dictionary of\n",
    "    genotype list'''\n",
    "    def __init__(self, filename, build = env.build, chr_prefix = None, allele_freq_info = None):\n",
    "        self.vcf = cstatgen.VCFstream(filename)\n",
    "        self.chrom = self.startpos = self.endpos = self.name = None\n",
    "        self.chr_prefix = chr_prefix\n",
    "        # name of allele frequency meta info\n",
    "        self.af_info = allele_freq_info\n",
    "        self.xchecker = PseudoAutoRegion('X', build)\n",
    "        self.ychecker = PseudoAutoRegion('Y', build)\n",
    "\n",
    "    def apply(self, data):\n",
    "        # Clean up\n",
    "        data.reset()\n",
    "        data.chrom = self.chrom\n",
    "        self.vcf.Extract(self.chrom, self.startpos, self.endpos)\n",
    "        varIdx = 0\n",
    "        # for each variant site\n",
    "        gss = {} #test line\n",
    "        while (self.vcf.Next()):\n",
    "            # skip tri-allelic sites\n",
    "            if not self.vcf.IsBiAllelic():\n",
    "                with env.triallelic_counter.get_lock():\n",
    "                    env.triallelic_counter.value += 1\n",
    "                continue\n",
    "            # check if the line's sample number matches the entire VCF sample number\n",
    "            if not self.vcf.CountSampleGenotypes() == self.vcf.sampleCount:\n",
    "                raise ValueError('Genotype and sample mismatch for region {}: {:,d} vs {:,d}'.\\\n",
    "                             format(self.name, self.vcf.CountSampleGenotypes(), self.vcf.sampleCount))\n",
    "            # valid line found, get variant info\n",
    "            try:\n",
    "                maf = float(self.vcf.GetInfo(self.af_info)) if self.af_info else None\n",
    "                if maf > 0.5:\n",
    "                    maf = 1 - maf\n",
    "                elif maf<=0.0:\n",
    "                    maf = 0.0001    #fixme\n",
    "            except Exception as e:\n",
    "                maf = 0.005\n",
    "                #raise ValueError(\"VCF line {}:{} does not have valid allele frequency field {}!\".\\\n",
    "                #                 format(self.vcf.GetChrom(), self.vcf.GetPosition(), self.af_info))\n",
    "            data.variants.append([self.vcf.GetChrom(), self.vcf.GetPosition(), self.name, maf])\n",
    "            # for each family assign member genotype if the site is non-trivial to the family\n",
    "            for k in data.families:\n",
    "                gs = self.vcf.GetGenotypes(data.famsampidx[k])\n",
    "                gss[k] = gs\n",
    "                if len(set(''.join([x for x in gs if x != \"00\"]))) <= 1:\n",
    "                    # skip monomorphic gs\n",
    "                    continue\n",
    "                else:\n",
    "                    # this variant is found in the family\n",
    "                    data.famvaridx[k].append(varIdx)\n",
    "                    for person, g in zip(data.families[k], gs):\n",
    "                        data[person].append(g)\n",
    "            data.gss[varIdx] = gss #test line\n",
    "            varIdx += 1\n",
    "        #print(self.name,'varIdx',varIdx)\n",
    "        #\n",
    "        if varIdx == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            with env.variants_counter.get_lock():\n",
    "                env.variants_counter.value += varIdx\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def getRegion(self, region):\n",
    "        self.chrom, self.startpos, self.endpos, self.name = region[:4]\n",
    "        self.startpos = int(self.startpos)\n",
    "        self.endpos = int(self.endpos) + 1\n",
    "        if self.chrom in ['X','23']:\n",
    "            if self.xchecker.check(self.startpos) or self.xchecker.check(self.endpos):\n",
    "                self.chrom = 'XY'\n",
    "        if self.chrom in ['Y','24']:\n",
    "            if self.ychecker.check(self.startpos) or self.ychecker.check(self.endpos):\n",
    "                self.chrom = 'XY'\n",
    "        if self.chr_prefix and not self.chrom.startswith(self.chr_prefix):\n",
    "            self.chrom = self.chr_prefix + self.chrom\n",
    "\n",
    "\n",
    "class MarkerMaker:\n",
    "    def __init__(self, wsize, maf_cutoff = None):\n",
    "        self.missings = (\"0\", \"0\")\n",
    "        self.gtconv = {'1':0, '2':1}\n",
    "        self.haplotyper = cstatgen.HaplotypingEngine(verbose = env.debug)\n",
    "        if wsize == 0 or wsize >= 1:\n",
    "            self.r2 = None\n",
    "        else:\n",
    "            self.r2 = wsize\n",
    "        self.coder = cstatgen.HaplotypeCoder(wsize)\n",
    "        self.maf_cutoff = maf_cutoff\n",
    "        self.rsq = 0.0\n",
    "    def getRegion(self, region):\n",
    "        self.name = region[3]\n",
    "        env.dtest[self.name] = {'dregions':[],'dvariants':[],'dfamvaridx':[],'dgeno':[],'gss':[],'hapimp':{},'ld':[],'coder':{},'format':[]}\n",
    "        env.dtest[self.name]['dregions'].append(region) #test line\n",
    "        env.dtest[self.name] = OrderedDict(env.dtest[self.name])\n",
    "\n",
    "    def apply(self, data):\n",
    "        # temp raw haplotype, maf and variant names data\n",
    "        haplotypes = OrderedDict()\n",
    "        mafs = {}\n",
    "        varnames = {}\n",
    "        #try:\n",
    "            # haplotyping plus collect found allele counts\n",
    "            # and computer founder MAFS\n",
    "        env.dtest[self.name]['gss'].append(deepcopy(data.gss))\n",
    "        env.dtest[self.name]['dvariants'].append(data.variants)\n",
    "        env.dtest[self.name]['dfamvaridx'].append(deepcopy(data.famvaridx))\n",
    "        #print('data',data)\n",
    "        env.dtest[self.name]['dgeno'].append(data.copy())\n",
    "        self.__Haplotype(data, haplotypes, mafs, varnames)\n",
    "        if len(varnames):\n",
    "            if not any ([len(varnames[x]) - 1 for x in varnames]):\n",
    "                # all families have only one variant\n",
    "                self.__AssignSNVHaplotypes(data, haplotypes, mafs, varnames)\n",
    "            else:\n",
    "                # calculate LD clusters using founder haplotypes\n",
    "                #clusters = self.__ClusterByLD(data, haplotypes, varnames)\n",
    "                clusters=[]\n",
    "                #print('clusters:',clusters)\n",
    "                # recoding the genotype of the region\n",
    "                env.dtest[self.name]['coder']['input'] = [data.copy(), haplotypes, mafs, varnames, clusters]\n",
    "                self.__CodeHaplotypes(data, haplotypes, mafs, varnames, clusters)\n",
    "                env.dtest[self.name]['coder']['output'] = [self.coder.GetHaplotypes(),data.copy(),data.superMarkerCount,deepcopy(data.maf)]\n",
    "        #except Exception as e:\n",
    "        #    return -1\n",
    "        self.__FormatHaplotypes(data)\n",
    "        env.dtest[self.name]['format'] = data.copy()\n",
    "        return 0\n",
    "\n",
    "    def __Haplotype(self, data, haplotypes, mafs, varnames):\n",
    "        '''genetic haplotyping. haplotypes stores per family data'''\n",
    "        # FIXME: it is SWIG's (2.0.12) fault not to properly destroy the object \"Pedigree\" in \"Execute()\"\n",
    "        # So there is a memory leak here which I tried to partially handle on C++\n",
    "        #\n",
    "        # Per family haplotyping\n",
    "        #\n",
    "        self.markers = [\"V{}-{}\".format(idx, item[1]) for idx, item in enumerate(data.variants)]\n",
    "        print('in Haplotype')\n",
    "        for item in data.families:\n",
    "            print('running family',item)\n",
    "            varnames[item], positions, vcf_mafs = data.getFamVariants(item, style = \"map\")\n",
    "            env.dtest[self.name]['hapimp'][item] = [item,varnames[item], positions, vcf_mafs] #test line\n",
    "            if len(varnames[item]) == 0:\n",
    "                for person in data.families[item]:\n",
    "                    data[person] = self.missings\n",
    "                continue\n",
    "            if env.debug:\n",
    "                with env.lock:\n",
    "                    sys.stderr.write('\\n'.join(['\\t'.join(x) for x in data.getFamSamples(item)]) + '\\n\\n')\n",
    "            # haplotyping\n",
    "            with env.lock:\n",
    "                if not env.prephased:\n",
    "                    #with stdoutRedirect(to = env.tmp_log + str(os.getpid()) + '.log'):\n",
    "                    #    haplotypes[item] = self.haplotyper.Execute(data.chrom, varnames[item],\n",
    "                    #                                           sorted(positions), data.getFamSamples(item))[0]\n",
    "                    tmp_log_output=env.tmp_log + str(os.getpid()) + '.log'\n",
    "                    if isnotebook():\n",
    "                        haplotypes[item] = self.haplotyper.Execute(data.chrom, varnames[item], sorted(positions), \n",
    "                                                                       data.getFamSamples(item), self.rsq, tmp_log_output)[0]\n",
    "                    else:\n",
    "                        with stdoutRedirect(to = tmp_log_output):\n",
    "                            haplotypes[item] = self.haplotyper.Execute(data.chrom, varnames[item], sorted(positions), \n",
    "                                                                       data.getFamSamples(item), self.rsq, tmp_log_output)[0]\n",
    "\n",
    "                else:\n",
    "                    haplotypes[item] = self.__PedToHaplotype(data.getFamSamples(item))\n",
    "            env.dtest[self.name]['hapimp'][item].append(haplotypes[item]) #test line\n",
    "            if len(haplotypes[item]) == 0:\n",
    "                # C++ haplotyping implementation failed\n",
    "                with env.chperror_counter.get_lock():\n",
    "                    env.chperror_counter.value += 1\n",
    "            # either use privided MAF or computer MAF\n",
    "            if all(vcf_mafs):\n",
    "                for idx, v in enumerate(varnames[item]):\n",
    "                    if v not in mafs:\n",
    "                        mafs[v] = vcf_mafs[idx]\n",
    "            else:\n",
    "                # count founder alleles\n",
    "                print('count founder alleles')\n",
    "                for hap in haplotypes[item]:\n",
    "                    if not data.tfam.is_founder(hap[1]):\n",
    "                        continue\n",
    "                    for idxv, v in enumerate(varnames[item]):\n",
    "                        if v not in mafs:\n",
    "                            # [#alt, #haplotypes]\n",
    "                            mafs[v] = [0, 0]\n",
    "                        gt = hap[2 + idxv][1] if hap[2 + idxv][0].isupper() else hap[2 + idxv][0]\n",
    "                        if not gt == \"?\":\n",
    "                            mafs[v][0] += self.gtconv[gt]\n",
    "                            mafs[v][1] += 1.0\n",
    "        env.dtest[self.name]['hapimp'][item].append(mafs) #test line\n",
    "        #\n",
    "        # Compute founder MAFs\n",
    "        #\n",
    "        for v in mafs:\n",
    "            if type(mafs[v]) is not list:\n",
    "                continue\n",
    "            mafs[v] = mafs[v][0] / mafs[v][1] if mafs[v][1] > 0 else 0.0\n",
    "        if env.debug:\n",
    "            with env.lock:\n",
    "                print(\"variant mafs = \", mafs, \"\\n\", file = sys.stderr)\n",
    "        #\n",
    "        # Drop some variants if maf is greater than given threshold\n",
    "        #\n",
    "        if self.maf_cutoff is not None:\n",
    "            exclude_vars = []\n",
    "            for v in mafs.keys():\n",
    "                if mafs[v] > self.maf_cutoff:\n",
    "                    exclude_vars.append(v)\n",
    "            for i in haplotypes.keys():\n",
    "                haplotypes[i] = listit(haplotypes[i])\n",
    "                for j in range(len(haplotypes[i])):\n",
    "                    haplotypes[i][j] = haplotypes[i][j][:2] + \\\n",
    "                      [x for idx, x in enumerate(haplotypes[i][j][2:]) if varnames[i][idx] not in exclude_vars]\n",
    "                varnames[i] = [x for x in varnames[i] if x not in exclude_vars]\n",
    "                # handle trivial data\n",
    "                if len(varnames[i]) == 0:\n",
    "                    for person in data.families[i]:\n",
    "                        data[person] = self.missings\n",
    "                    del varnames[i]\n",
    "                    del haplotypes[i]\n",
    "            # count how many variants are removed\n",
    "            with env.commonvar_counter.get_lock():\n",
    "                env.commonvar_counter.value += len(exclude_vars)\n",
    "\n",
    "\n",
    "    def __ClusterByLD(self, data, haplotypes, varnames):\n",
    "        if self.r2 is None:\n",
    "            return None\n",
    "        # get founder haplotypes\n",
    "        founder_haplotypes = []\n",
    "        markers = sorted(set(itertools.chain(*varnames.values())), key = lambda x: int(x.split(\"-\")[0][1:]))\n",
    "        for item in haplotypes:\n",
    "            for ihap, hap in enumerate(haplotypes[item]):\n",
    "                if not data.tfam.is_founder(hap[1]):\n",
    "                    continue\n",
    "                gt = [hap[2 + varnames[item].index(v)] if v in varnames[item] else '?' for v in markers]\n",
    "                founder_haplotypes.append((\"{}-{}\".format(hap[1], ihap % 2), \"\".join([x[1] if x[0].isupper() else x[0] for x in gt])))\n",
    "        # calculate LD blocks, use r2 measure\n",
    "        blocks = []\n",
    "        if sys.version_info.major == 2:\n",
    "            ld = Align.create(founder_haplotypes).matrixLD(validCharacters=\"12\")[\"r2\"]\n",
    "            for j in ld:  #upper triangle\n",
    "                block = [j]\n",
    "                for k in ld[j]:\n",
    "                    try:\n",
    "                        if ld[j][k] > self.r2:\n",
    "                            block.append(k)\n",
    "                    except:\n",
    "                        print('ld value',ld[j][k])\n",
    "                if len(block) > 1:\n",
    "                    blocks.append(block)\n",
    "        else:\n",
    "            ldi,ld = egglib.stats.matrix_LD(Align.create(founder_haplotypes,egglib.Alphabet(cat='string',expl=['1','2'],miss='?')),('rsq'))\n",
    "            for j in range(len(ldi)): #lower triangle\n",
    "                block = [j]\n",
    "                for k in range(j+1,len(ldi)):\n",
    "                    try:\n",
    "                        if ld[k][j] > self.r2:\n",
    "                            block.append(k)\n",
    "                    except:\n",
    "                        print('ld value',ld[k][j])\n",
    "                if len(block) > 1:\n",
    "                    blocks.append(block)\n",
    "        # get LD clusters\n",
    "        clusters = [[markers[idx] for idx in item] for item in list(connected_components(blocks))]\n",
    "        env.dtest[self.name]['ld'] = [ld,blocks,clusters]\n",
    "        if env.debug:\n",
    "            with env.lock:\n",
    "                print(\"LD blocks: \", blocks, file = sys.stderr)\n",
    "                print(\"LD clusters: \", clusters, file = sys.stderr)\n",
    "        return clusters\n",
    "\n",
    "\n",
    "    def __CodeHaplotypes(self, data, haplotypes, mafs, varnames, clusters):\n",
    "        # apply CHP coding\n",
    "        if clusters is not None:\n",
    "            clusters_idx = [[[varnames[item].index(x) for x in y] for y in clusters] for item in haplotypes]\n",
    "        else:\n",
    "            clusters_idx = [[[]] for item in haplotypes]\n",
    "        self.coder.Execute(list(haplotypes.values()), [[mafs[v] for v in varnames[item]] for item in haplotypes], clusters_idx)\n",
    "        if env.debug:\n",
    "            with env.lock:\n",
    "                if clusters:\n",
    "                    print(\"Family LD clusters: \", clusters_idx, \"\\n\", file = sys.stderr)\n",
    "                self.coder.Print()\n",
    "        # line: [fid, sid, hap1, hap2]\n",
    "        for line in self.coder.GetHaplotypes():\n",
    "            #if not line[1] in data:\n",
    "                # this sample is not in VCF file. Every variant site should be missing\n",
    "                # they have to be skipped for now\n",
    "            #    continue\n",
    "            data[line[1]] = (line[2].split(','), line[4].split(','))\n",
    "            if len(data[line[1]][0]) > data.superMarkerCount:\n",
    "                data.superMarkerCount = len(data[line[1]][0])\n",
    "        # get MAF\n",
    "        for item in haplotypes:\n",
    "            data.maf[item] = self.coder.GetAlleleFrequencies(item)\n",
    "            data.maf[item] = tuple(tuple(np.array(v) / np.sum(v)) if np.sum(v) else v\n",
    "                              for v in data.maf[item])\n",
    "        if env.debug:\n",
    "            with env.lock:\n",
    "                print(\"marker freqs = \", data.maf, \"\\n\", file = sys.stderr)\n",
    "\n",
    "\n",
    "    def __AssignSNVHaplotypes(self, data, haplotypes, mafs, varnames):\n",
    "        for item in haplotypes:\n",
    "            # each person's haplotype\n",
    "            token = ''\n",
    "            for idx, line in enumerate(haplotypes[item]):\n",
    "                if not idx % 2:\n",
    "                    token = line[2][1] if line[2][0].isupper() else line[2][0]\n",
    "                else:\n",
    "                    data[line[1]] = (token, line[2][1] if line[2][0].isupper() else line[2][0])\n",
    "            # get maf\n",
    "            data.maf[item] = [(1 - mafs[varnames[item][0]], mafs[varnames[item][0]])]\n",
    "            data.maf[item] = tuple(tuple(np.array(v) / np.sum(v)) if np.sum(v) else v\n",
    "                              for v in data.maf[item])\n",
    "        if env.debug:\n",
    "            with env.lock:\n",
    "                print(\"marker freqs = \", data.maf, \"\\n\", file = sys.stderr)\n",
    "\n",
    "\n",
    "    def __FormatHaplotypes(self, data):\n",
    "        # Reformat sample genotypes\n",
    "        for person in data:\n",
    "            if type(data[person]) is not tuple:\n",
    "                data[person] = self.missings\n",
    "                continue\n",
    "            diff = data.superMarkerCount - len(data[person][0])\n",
    "            data[person] = list(zip(*data[person]))\n",
    "            if diff > 0:\n",
    "                data[person].extend([self.missings] * diff)\n",
    "\n",
    "    def __PedToHaplotype(self, ped):\n",
    "        '''convert prephased ped format to haplotype format.\n",
    "        Input: e.g. [['13346', '5888', '0', '0', '1', '11', '11', '11'], ['13346', '5856', '0', '0', '2', '12', '12', '12'], ['13346', '5920', '5888', '5856', '1', '12', '12', '12'], ['13346', '6589', '5888', '5856', '1', '11', '11', '11']]\n",
    "        Output: e.g. (('13346', '5856', '1:', '1:', '1:'), ('13346', '5856', '2:', '2:', '2:'), ('13346', '5888', '1:', '1:', '1:'), ('13346', '5888', '1:', '1:', '1:'), ('13346', '6589', '1:', '1|', '1|'), ('13346', '6589', '1:', '1|', '1|'), ('13346', '5920', '2:', '2|', '2|'), ('13346', '5920', '1:', '1|', '1|'))\n",
    "        '''\n",
    "        haps = []\n",
    "        for item in ped:\n",
    "            entry = [item[0], item[1]] + [x[0] + ':' if x[0] != '0' else '?:' for x in item[5:]]\n",
    "            haps.append(tuple(entry))\n",
    "            entry = [item[0], item[1]] + [x[1] + ':' if x[1] != '0' else '?:' for x in item[5:]]\n",
    "            haps.append(tuple(entry))\n",
    "        return tuple(haps)\n",
    "\n",
    "\n",
    "class LinkageWriter:\n",
    "    def __init__(self, num_missing_append = 0):\n",
    "        self.chrom = self.prev_chrom = self.name = self.distance = self.distance_avg = self.distance_m = self.distance_f = None\n",
    "        self.reset()\n",
    "        self.missings = [\"0\", \"0\"]\n",
    "        self.num_missing = num_missing_append\n",
    "\n",
    "    def apply(self, data):\n",
    "        if self.chrom != self.prev_chrom:\n",
    "            if self.prev_chrom is None:\n",
    "                self.prev_chrom = self.chrom\n",
    "            else:\n",
    "                # new chrom entered,\n",
    "                # commit whatever is in buffer before accepting new data\n",
    "                self.commit()\n",
    "        # write tped output\n",
    "        position = str(data.getMidPosition())\n",
    "        if data.superMarkerCount <= 1:\n",
    "            # genotypes\n",
    "            gs = [data[s][0] for s in data.samples]\n",
    "            if len(set(gs)) == 1:\n",
    "                # everyone's genotype is the same (most likely missing or monomorphic)\n",
    "                return 2\n",
    "            self.tped += env.delimiter.join([self.chrom, self.name, self.distance, position] + \\\n",
    "                list(itertools.chain(*gs)) + self.missings*self.num_missing) + \"\\n\"\n",
    "            # freqs\n",
    "            for k in data.maf:\n",
    "                self.freq += env.delimiter.join([k, self.name] + list(map(str, data.maf[k][0]))) + \"\\n\"\n",
    "        else:\n",
    "            # have to expand each region into mutiple chunks to account for different recomb points\n",
    "            gs = list(zip(*[data[s] for s in data.samples]))\n",
    "            # sub-chunk id\n",
    "            cid = 0\n",
    "            skipped_chunk = []\n",
    "            for idx, g in enumerate(gs):\n",
    "                if len(set(g)) == 1:\n",
    "                    skipped_chunk.append(idx)\n",
    "                    continue\n",
    "                cid += 1\n",
    "                self.tped += \\\n",
    "                  env.delimiter.join([self.chrom, '{}[{}]'.format(self.name, cid), self.distance, position] + \\\n",
    "                  list(itertools.chain(*g)) + self.missings*self.num_missing) + \"\\n\"\n",
    "            if cid == 0:\n",
    "                # everyone's genotype is the same (most likely missing or monomorphic)\n",
    "                return 2\n",
    "            # freqs\n",
    "            for k in data.maf:\n",
    "                cid = 0\n",
    "                for idx in range(data.superMarkerCount):\n",
    "                    if idx in skipped_chunk:\n",
    "                        continue\n",
    "                    if idx >= len(data.maf[k]):\n",
    "                        break\n",
    "                    cid += 1\n",
    "                    self.freq += env.delimiter.join([k, '{}[{}]'.format(self.name, cid)] + \\\n",
    "                                                    list(map(str, data.maf[k][idx]))) + \"\\n\"\n",
    "        if self.counter < env.batch:\n",
    "            self.counter += data.superMarkerCount\n",
    "        else:\n",
    "            self.commit()\n",
    "        return 0\n",
    "\n",
    "    def commit(self):\n",
    "        if self.tped:\n",
    "            with env.lock:\n",
    "                with open(os.path.join(env.tmp_cache, '{}.chr{}.tped'.format(env.output, self.prev_chrom)),\n",
    "                          'a') as f:\n",
    "                    f.write(self.tped)\n",
    "        if self.freq:\n",
    "            with env.lock:\n",
    "                with open(os.path.join(env.tmp_cache, '{}.chr{}.freq'.format(env.output, self.prev_chrom)),\n",
    "                          'a') as f:\n",
    "                    f.write(self.freq)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.tped = ''\n",
    "        self.freq = ''\n",
    "        self.counter = 0\n",
    "        self.prev_chrom = self.chrom\n",
    "\n",
    "    def getRegion(self, region):\n",
    "        self.chrom = region[0]\n",
    "        self.name, self.distance_avg, self.distance_m, self.distance_f = region[3:]\n",
    "        self.distance = \";\".join([self.distance_avg, self.distance_m, self.distance_f])\n",
    "\n",
    "\n",
    "class EncoderWorker(Process):\n",
    "    def __init__(self, queue, length, data, extractor, coder, writer):\n",
    "        Process.__init__(self)\n",
    "        self.queue = queue\n",
    "        self.numGrps = float(length)\n",
    "        self.data = data\n",
    "        self.extractor = extractor\n",
    "        self.maker = coder\n",
    "        self.writer = writer\n",
    "\n",
    "    def report(self):\n",
    "        env.log('{:,d} units processed {{{:.2%}}} ...'.\\\n",
    "                format(env.success_counter.value, env.total_counter.value / self.numGrps), flush = True)\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            try:\n",
    "                region = self.queue.get()\n",
    "                if region is None:\n",
    "                    self.writer.commit()\n",
    "                    self.report()\n",
    "                    # total mendelian errors found\n",
    "                    with env.mendelerror_counter.get_lock():\n",
    "                        env.mendelerror_counter.value += self.maker.haplotyper.CountMendelianErrors()\n",
    "                    # total recombination events found\n",
    "                    with env.recomb_counter.get_lock():\n",
    "                        env.recomb_counter.value += self.maker.coder.CountRecombinations()\n",
    "                    break\n",
    "                else:\n",
    "                    with env.total_counter.get_lock():\n",
    "                        env.total_counter.value += 1\n",
    "                    self.extractor.getRegion(region)\n",
    "                    self.maker.getRegion(region)\n",
    "                    self.writer.getRegion(region)\n",
    "                    isSuccess = True\n",
    "                    for m in [self.extractor, self.maker, self.writer]:\n",
    "                        status = m.apply(self.data)\n",
    "                        if status == -1:\n",
    "                            with env.chperror_counter.get_lock():\n",
    "                                # previous module failed\n",
    "                                env.chperror_counter.value += 1\n",
    "                        if status == 1:\n",
    "                            with env.null_counter.get_lock():\n",
    "                                env.null_counter.value += 1\n",
    "                        if status == 2:\n",
    "                            with env.trivial_counter.get_lock():\n",
    "                                env.trivial_counter.value += 1\n",
    "                        if status != 0:\n",
    "                            isSuccess = False\n",
    "                            break\n",
    "                    if isSuccess:\n",
    "                        with env.success_counter.get_lock():\n",
    "                            env.success_counter.value += 1\n",
    "                    if env.total_counter.value % (env.batch * env.jobs) == 0:\n",
    "                        self.report()\n",
    "            except KeyboardInterrupt:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: Binary trait detected in [/mnt/mfs/statgen/yin/Github/linkage/SEQpy3/data/mwe_normal_fam.csv]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkParams(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AF'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.no_save:\n",
    "    cache = NoCache()\n",
    "else:\n",
    "    cache = Cache(env.cache_dir, env.output, vars(args))\n",
    "cache.setID('vcf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 7 samples found in FAM file but not in VCF file:\n",
      "1036_2, 22_1_10, 22_1_20, 28_9_186, 1036_1, 28_9_100, 28_9_101\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: 3,461 samples in VCF file will be ignored due to absence in FAM file\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: 3,479 samples found in [/mnt/mfs/statgen/yin/Github/linkage/SEQpy3/data/first1000snp_full_samples.vcf.gz]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: write encoded data to TPED format\n",
    "if not args.vanilla and cache.check():\n",
    "    env.log('Loading regional marker data from archive ...')\n",
    "    cache.load(target_dir = env.tmp_dir, names = ['CACHE'])\n",
    "    env.success_counter.value = sum(map(fileLinesCount, glob.glob('{}/*.tped'.format(env.tmp_cache))))\n",
    "    env.batch = 10\n",
    "else:\n",
    "    # load VCF file header\n",
    "    data = RData(args.vcf, args.tfam)\n",
    "    vs = data.vs\n",
    "    samples_vcf = data.samples_vcf\n",
    "\n",
    "if len(samples_vcf) == 0:\n",
    "    env.error(\"Fail to extract samples from [{}]\".format(args.vcf), exit = True)\n",
    "env.log('{:,d} samples found in [{}]'.format(len(samples_vcf), args.vcf))\n",
    "samples_not_vcf = data.samples_not_vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 3479)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples_not_vcf),len(samples_vcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data.families) == 0:\n",
    "    env.error('No valid family to process. ' \\\n",
    "              'Families have to be at least trio with at least one member in VCF file.', exit = True)\n",
    "if len(data.samples) == 0:\n",
    "    env.error('No valid sample to process. ' \\\n",
    "              'Samples have to be in families, and present in both TFAM and VCF files.', exit = True)\n",
    "rewriteFamfile(os.path.join(env.tmp_cache, '{}.tfam'.format(env.output)),\n",
    "               data.tfam.samples, list(data.samples.keys()) + samples_not_vcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 3 families with a total of 18 samples will be scanned for 28,325 pre-defined units\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if args.single_markers:\n",
    "    regions=[]\n",
    "    for x in vs.GetGenomeCoordinates():\n",
    "        region_info = (x[0], x[1], x[1], \"{}:{}\".format(x[0], x[1]), '.', '.', '.')\n",
    "        if region_info not in regions:\n",
    "            regions.append(region_info)\n",
    "    args.blueprint = None\n",
    "else:\n",
    "    # load blueprint\n",
    "    try:\n",
    "        with open(args.blueprint, 'r') as f:\n",
    "            regions = [x.strip().split() for x in f.readlines()]\n",
    "    except IOError:\n",
    "        env.error(\"Cannot load regional marker blueprint [{}]. \".format(args.blueprint), exit = True)\n",
    "env.log('{:,d} families with a total of {:,d} samples will be scanned for {:,d} pre-defined units'.\\\n",
    "        format(len(data.families), len(data.samples), len(regions)))\n",
    "env.jobs = max(min(args.jobs, len(regions)), 1)\n",
    "regions.extend([None] * env.jobs)\n",
    "queue = [] if env.jobs == 1 else Queue()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor =RegionExtractor(args.vcf, chr_prefix = args.chr_prefix, allele_freq_info = args.freq)\n",
    "maker =            MarkerMaker(args.bin, maf_cutoff = args.maf_cutoff)\n",
    "writer =             LinkageWriter(len(samples_not_vcf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in Haplotype\n",
      "running family 1036\n",
      "running family Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V8-13273 V12-13302 V17-13417 V22-13687 \n",
      "\n",
      "V8-13273: 0 0.707106 0.292894 \n",
      "total familyCount:1\n",
      "V12-13302: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V17-13417: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V22-13687: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "22_1\n",
      "running family 28_9\n",
      "Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V8-13273 V15-13380 V17-13417 \n",
      "\n",
      "V8-13273: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V15-13380: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V17-13417: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "0 3 25 ['1', '11868', '14362', 'LOC102725121@1', '9.177127474362311e-07', '1.1657192989882668e-06', '6.814189157634088e-07']\n",
      "in Haplotype\n",
      "running family 1036\n",
      "running familyEstimating allele frequencies... [using maximum likelihood]0\n",
      "   V8-13273 V12-13302 V17-13417 V22-13687 \n",
      "\n",
      "V8-13273: 0 0.707106 0.292894 \n",
      "total familyCount:1\n",
      " 22_1\n",
      "running familyV12-13302: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V17-13417: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V22-13687: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V8-13273 V15-13380 V17-13417 \n",
      "\n",
      "V8-13273: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V15-13380: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V17-13417: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      " 28_9\n",
      "in Haplotype\n",
      "running family 1036\n",
      "running family 22_1\n",
      "running family 28_9\n",
      "in Haplotype\n",
      "running family 1036\n",
      "Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V3-14464 V8-14653 V18-14907 V19-14930 V20-14933 V40-16103 V50-16378 \n",
      "   V55-16487 V69-17147 V79-17358 V85-17385 V89-17407 V124-17697 V151-17928 \n",
      "   V152-17929 V159-20184 V160-20191 V162-20212 V165-20227 V169-20235 \n",
      "   V171-20250 V178-20316 V180-20485 V182-20522 V184-20547 V194-29368 \n",
      "\n",
      "V3-14464: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V8-14653: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V18-14907: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V19-14930: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V20-14933: 0 1 \n",
      "total familyCount:1\n",
      "V40-16103: 0 0.707106 0.292894 \n",
      "total familyCount:1\n",
      "V50-16378: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V55-16487: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "running family V69-17147: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V79-17358: 0 1 \n",
      "total familyCount:1\n",
      "V85-17385: 0 1 \n",
      "total familyCount:1\n",
      "V89-17407: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V124-17697: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V151-17928: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V152-17929: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V159-20184: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V160-20191: 0 1 \n",
      "total familyCount:1\n",
      "V162-20212: 0 1 \n",
      "total familyCount:1\n",
      "V165-20227: 0 1 \n",
      "total familyCount:1\n",
      "V169-20235: 0 1 \n",
      "total familyCount:1\n",
      "V171-20250: 0 1 \n",
      "total familyCount:1\n",
      "V178-20316: 0 1 \n",
      "total familyCount:1\n",
      "V180-20485: 0 1 \n",
      "total familyCount:1\n",
      "V182-20522: 0 1 \n",
      "total familyCount:1\n",
      "V184-20547: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V194-29368: 0 1 \n",
      "total familyCount:1\n",
      "Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V3-14464 V8-14653 V10-14677 V18-14907 V19-14930 V40-16103 V50-16378 \n",
      "   V55-16487 V69-17147 V80-17365 V85-17385 V86-17398 V89-17407 V95-17479 \n",
      "   V108-17559 V112-17589 V124-17697 V129-17722 V131-17746 V142-17829 \n",
      "   V155-19190 V1522_1\n",
      "running family9-20184 V160-20191 V162-20212 V165-20227 V169-20235 \n",
      "   V171-20250 V178-20316 V184-20547 \n",
      "\n",
      "V3-14464: 0 0.739454 0.260546 \n",
      "total familyCount:1\n",
      "V8-14653: 0 0.707103 0.292897 \n",
      "total familyCount:1\n",
      "V10-14677: 0 0.739454 0.260546 \n",
      "total familyCount:1\n",
      "V18-14907: 0 0.707103 0.292897 \n",
      "total familyCount:1\n",
      "V19-14930: 0 0.728713 0.271287 \n",
      "total familyCount:1\n",
      "V40-16103: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V50-16378: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V55-16487: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V69-17147: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V80-17365: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V85-17385: 0 0.739454 0.260546 \n",
      "total familyCount:1\n",
      "V86-17398: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V89-17407: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V95-17479: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V108-17559: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V112-17589: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V124-17697: 0 0.739454 0.260546 \n",
      "total familyCount:1\n",
      "V129-17722: 0 0.744757 0.255243 \n",
      "total famil 28_9\n",
      "yCount:1\n",
      "V131-17746: 0 0.739454 0.260546 \n",
      "total familyCount:1\n",
      "V142-17829: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V155-19190: 0 0.739454 0.260546 \n",
      "total familyCount:1\n",
      "V159-20184: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V160-20191: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V162-20212: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V165-20227: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V169-20235: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V171-20250: 0 0.739454 0.260546 \n",
      "total familyCount:1\n",
      "V178-20316: 0 0.739454 0.260546 \n",
      "total familyCount:1\n",
      "V184-20547: 0 0.739454 0.260546 \n",
      "total familyCount:1\n",
      "Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V3-14464 V5-14470 V8-14653 V10-14677 V18-14907 V19-14930 V34-16068 \n",
      "   V40-16103 V50-16378 V84-17379 V85-17385 V90-17408 V105-17519 V107-17556 \n",
      "   V116-17614 V155-19190 V158-20166 V160-20191 V162-20212 V165-20227 \n",
      "   V166-20227 V168-20231 V169-20235 V171-20250 V174-20254 V178-20316 \n",
      "   V183-20545 V184-20547 V194-29368 \n",
      "\n",
      "V3-14464: 0 0.646634 0.353366 \n",
      "total familyCount:1\n",
      "V5-14470: 0 0.824706 0.175294 \n",
      "total familyCount:1\n",
      "V8-14653: 0 0.452863 0.547137 \n",
      "total familyCount:1\n",
      "V10-14677: 0 0.656432 0.343568 \n",
      "total familyCount:1\n",
      "V18-14907: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V19-14930: 0 0.628665 0.371335 \n",
      "total familyCount:1\n",
      "V34-16068: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V40-16103: 0 0.628667 0.371333 \n",
      "total familyCount:1\n",
      "V50-16378: 0 0.617751 0.382249 \n",
      "total familyCount:1\n",
      "V84-17379: 0 0.824706 0.175294 \n",
      "total familyCount:1\n",
      "V85-17385: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V90-17408: 0 0.824706 0.175294 \n",
      "total familyCount:1\n",
      "V105-17519: 0 0.824706 0.175294 \n",
      "total familyCount:1\n",
      "V107-17556: 0 0.820231 0.179769 \n",
      "total familyCount:1\n",
      "V116-17614: 0 0.824706 0.175294 \n",
      "total familyCount:1\n",
      "V155-19190: 0 0.6 0.4 \n",
      "total familyCount:1\n",
      "V158-20166: 0 0.628665 0.371335 \n",
      "total familyCount:1\n",
      "V160-20191: 0 0.628665 0.371335 \n",
      "total familyCount:1\n",
      "V162-20212: 0 0.628665 0.371335 \n",
      "total familyCount:1\n",
      "V165-20227: 0 0.656432 0.343568 \n",
      "total familyCount:1\n",
      "V166-20227: 0 0.646634 0.353366 \n",
      "total familyCount:1\n",
      "V168-20231: 0 0.811255 0.188745 \n",
      "total familyCount:1\n",
      "V169-20235: 0 0.824706 0.175294 \n",
      "total familyCount:1\n",
      "V171-20250: 0 0.628665 0.371335 \n",
      "total familyCount:1\n",
      "V174-20254: 0 0.824706 0.175294 \n",
      "total familyCount:1\n",
      "V178-20316: 0 0.628665 0.371335 \n",
      "total familyCount:1\n",
      "V183-20545: 0 0.820231 0.179769 \n",
      "total familyCount:1\n",
      "V184-20547: 0 0.617751 0.382249 \n",
      "total familyCount:1\n",
      "V194-29368: 0 0.656432 0.343568 \n",
      "total familyCount:1\n",
      "Estimating allele frequencies... [using maximum likelihood]0          \n",
      "   V4-17385 V8-17407 \n",
      "\n",
      "V4-17385: 0 1 \n",
      "total familyCount:1\n",
      "V8-17407: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V4-17385 V5-17398 V8-17407 \n",
      "\n",
      "V4-17385: 0 0.739454 0.260546 \n",
      "total familyCount:1\n",
      "V5-17398: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V8-17407: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V3-17379 V4-17385 V9-17408 \n",
      "\n",
      "V3-17379: 0 0.824706 0.175294 \n",
      "total familyCount:1\n",
      "V4-17385: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V9-17408: 0 0.824706 0.175294 \n",
      "total familyCount:1\n"
     ]
    }
   ],
   "source": [
    "for j, region in enumerate(regions[:20]):\n",
    "    i = 0\n",
    "    #for region in rg:\n",
    "    extractor.getRegion(region)\n",
    "    maker.getRegion(region)\n",
    "    writer.getRegion(region)\n",
    "    isSuccess = True\n",
    "    for m in [extractor, maker, writer]:\n",
    "        status = m.apply(data)\n",
    "        #print(data,data.genotype_all)\n",
    "        i+=1\n",
    "        if status == -1:\n",
    "            with env.chperror_counter.get_lock():\n",
    "                # previous module failed\n",
    "                env.chperror_counter.value += 1\n",
    "        if status == 1:\n",
    "            with env.null_counter.get_lock():\n",
    "                env.null_counter.value += 1\n",
    "        if status == 2:\n",
    "            with env.trivial_counter.get_lock():\n",
    "                env.trivial_counter.value += 1\n",
    "        if status != 0:\n",
    "            isSuccess = False\n",
    "            break\n",
    "    if isSuccess:\n",
    "        with env.success_counter.get_lock():\n",
    "            env.success_counter.value += 1\n",
    "    if j%1000==0:\n",
    "        print(j,i,len(data.variants),region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['LOC102725121@1', 'DDX11L1', 'WASH7P', 'MIR6859-1@1,MIR6859-2@1,MIR6859-3@1,MIR6859-4@1', 'MIR1302-10@1,MIR1302-11@1,MIR1302-2@1,MIR1302-9@1', 'FAM138A@1,FAM138C@1,FAM138F@1', 'OR4F5', 'LOC729737', 'LOC100132062@1,LOC100132287@1', 'OR4F16@1,OR4F29@1,OR4F3@1', 'LOC101928626', 'MIR12136', 'OR4F16@2,OR4F29@2,OR4F3@2', 'LOC100133331', 'LOC100288069', 'FAM87B', 'LINC00115', 'LINC01128', 'FAM41C', 'LINC02593'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.dtest.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('dregions',\n",
       "              [['1',\n",
       "                '11868',\n",
       "                '14362',\n",
       "                'LOC102725121@1',\n",
       "                '9.177127474362311e-07',\n",
       "                '1.1657192989882668e-06',\n",
       "                '6.814189157634088e-07']]),\n",
       "             ('dvariants',\n",
       "              [[['1', 13035, 'LOC102725121@1', 0.0002],\n",
       "                ['1', 13053, 'LOC102725121@1', 0.0004],\n",
       "                ['1', 13080, 'LOC102725121@1', 1e-05],\n",
       "                ['1', 13115, 'LOC102725121@1', 1e-05],\n",
       "                ['1', 13151, 'LOC102725121@1', 0.0001],\n",
       "                ['1', 13169, 'LOC102725121@1', 0.0001],\n",
       "                ['1', 13244, 'LOC102725121@1', 0.0002],\n",
       "                ['1', 13248, 'LOC102725121@1', 0.0006],\n",
       "                ['1', 13273, 'LOC102725121@1', 0.1548],\n",
       "                ['1', 13289, 'LOC102725121@1', 0.0029],\n",
       "                ['1', 13298, 'LOC102725121@1', 2.531e-05],\n",
       "                ['1', 13302, 'LOC102725121@1', 0.0103],\n",
       "                ['1', 13302, 'LOC102725121@1', 9.451e-05],\n",
       "                ['1', 13303, 'LOC102725121@1', 0.0002],\n",
       "                ['1', 13379, 'LOC102725121@1', 7.851e-05],\n",
       "                ['1', 13380, 'LOC102725121@1', 0.0088],\n",
       "                ['1', 13393, 'LOC102725121@1', 0.0002],\n",
       "                ['1', 13417, 'LOC102725121@1', 0.1197],\n",
       "                ['1', 13453, 'LOC102725121@1', 0.0003],\n",
       "                ['1', 13494, 'LOC102725121@1', 0.0015],\n",
       "                ['1', 13504, 'LOC102725121@1', 0.0026],\n",
       "                ['1', 13687, 'LOC102725121@1', 0.0049],\n",
       "                ['1', 13687, 'LOC102725121@1', 0.0001],\n",
       "                ['1', 14159, 'LOC102725121@1', 0.0021],\n",
       "                ['1', 14345, 'LOC102725121@1', 0.0001]]]),\n",
       "             ('dfamvaridx',\n",
       "              [{'1036': [8, 12, 17, 22], '22_1': [8, 15, 17], '28_9': []}]),\n",
       "             ('dgeno',\n",
       "              [{'28_9_103': [],\n",
       "                '28_9_106': [],\n",
       "                '28_9_108': [],\n",
       "                '28_9_109': [],\n",
       "                '28_9_111': [],\n",
       "                '28_9_105': [],\n",
       "                '28_9_114': [],\n",
       "                '28_9_110': [],\n",
       "                '1036_5': ['12', '12', '12', '11'],\n",
       "                '1036_99': ['12', '11', '12', '11'],\n",
       "                '1036_6': ['00', '00', '11', '12'],\n",
       "                '1036_4': ['11', '12', '11', '11'],\n",
       "                '1036_3': ['11', '11', '11', '11'],\n",
       "                '22_1_2': ['11', '12', '11'],\n",
       "                '22_1_3': ['11', '11', '11'],\n",
       "                '22_1_4': ['11', '11', '11'],\n",
       "                '22_1_5': ['11', '11', '11'],\n",
       "                '22_1_99': ['12', '11', '12']}]),\n",
       "             ('gss',\n",
       "              [{0: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                1: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                2: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                3: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                4: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                5: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                6: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                7: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                8: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                9: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                10: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                11: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                12: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                13: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                14: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                15: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                16: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                17: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                18: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                19: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                20: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                21: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                22: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                23: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')},\n",
       "                24: {'1036': ('11', '11', '11', '11', '11'),\n",
       "                 '22_1': ('11', '11', '11', '11', '11'),\n",
       "                 '28_9': ('11', '11', '11', '11', '11', '11', '11', '11')}}]),\n",
       "             ('hapimp',\n",
       "              {'1036': ['1036',\n",
       "                ['V8-13273', 'V12-13302', 'V17-13417', 'V22-13687'],\n",
       "                [13273, 13302, 13417, 13687],\n",
       "                [0.1548, 9.451e-05, 0.1197, 0.0001],\n",
       "                (('1036', '1036_1', '1:', '1:', '1:', '1:'),\n",
       "                 ('1036', '1036_1', '?:', '?:', '?:', '?:'),\n",
       "                 ('1036', '1036_2', '2:', '1:', '2:', '1:'),\n",
       "                 ('1036', '1036_2', '?:', '?:', '?:', '?:'),\n",
       "                 ('1036', '1036_6', '1:', '2:', '1:', '1:'),\n",
       "                 ('1036', '1036_6', '1:', '1:', '1:', '2:'),\n",
       "                 ('1036', '1036_99', '2:', '1|', '2|', '1|'),\n",
       "                 ('1036', '1036_99', '1:', '1|', '1|', '1|'),\n",
       "                 ('1036', '1036_5', '2:', '1|', '2|', '1|'),\n",
       "                 ('1036', '1036_5', '1:', '2|', '1|', '1|'),\n",
       "                 ('1036', '1036_4', '1:', '1|', '1|', '1|'),\n",
       "                 ('1036', '1036_4', '1:', '2|', '1|', '1|'),\n",
       "                 ('1036', '1036_3', '1:', '1|', '1|', '1\\\\'),\n",
       "                 ('1036', '1036_3', '1:', '1|', '1|', '1\\\\'))],\n",
       "               '22_1': ['22_1',\n",
       "                ['V8-13273', 'V15-13380', 'V17-13417'],\n",
       "                [13273, 13380, 13417],\n",
       "                [0.1548, 0.0088, 0.1197],\n",
       "                (('22_1', '22_1_10', '2:', '1:', '2:'),\n",
       "                 ('22_1', '22_1_10', '1:', '1:', '1:'),\n",
       "                 ('22_1', '22_1_20', '1:', '1:', '1:'),\n",
       "                 ('22_1', '22_1_20', '1:', '2:', '1:'),\n",
       "                 ('22_1', '22_1_99', '1:', '1|', '1|'),\n",
       "                 ('22_1', '22_1_99', '2:', '1|', '2|'),\n",
       "                 ('22_1', '22_1_5', '1:', '1|', '1|'),\n",
       "                 ('22_1', '22_1_5', '1:', '1|', '1|'),\n",
       "                 ('22_1', '22_1_4', '1:', '1|', '1|'),\n",
       "                 ('22_1', '22_1_4', '1:', '1|', '1|'),\n",
       "                 ('22_1', '22_1_3', '1:', '1|', '1|'),\n",
       "                 ('22_1', '22_1_3', '1:', '1|', '1|'),\n",
       "                 ('22_1', '22_1_2', '1:', '2|', '1|'),\n",
       "                 ('22_1', '22_1_2', '1:', '1|', '1|'))],\n",
       "               '28_9': ['28_9',\n",
       "                [],\n",
       "                [],\n",
       "                [],\n",
       "                {'V8-13273': 0.1548,\n",
       "                 'V12-13302': 9.451e-05,\n",
       "                 'V17-13417': 0.1197,\n",
       "                 'V22-13687': 0.0001,\n",
       "                 'V15-13380': 0.0088}]}),\n",
       "             ('ld', []),\n",
       "             ('coder',\n",
       "              {'input': [{'28_9_103': ('0', '0'),\n",
       "                 '28_9_106': ('0', '0'),\n",
       "                 '28_9_108': ('0', '0'),\n",
       "                 '28_9_109': ('0', '0'),\n",
       "                 '28_9_111': ('0', '0'),\n",
       "                 '28_9_105': ('0', '0'),\n",
       "                 '28_9_114': ('0', '0'),\n",
       "                 '28_9_110': ('0', '0'),\n",
       "                 '1036_5': ['12', '12', '12', '11'],\n",
       "                 '1036_99': ['12', '11', '12', '11'],\n",
       "                 '1036_6': ['00', '00', '11', '12'],\n",
       "                 '1036_4': ['11', '12', '11', '11'],\n",
       "                 '1036_3': ['11', '11', '11', '11'],\n",
       "                 '22_1_2': ['11', '12', '11'],\n",
       "                 '22_1_3': ['11', '11', '11'],\n",
       "                 '22_1_4': ['11', '11', '11'],\n",
       "                 '22_1_5': ['11', '11', '11'],\n",
       "                 '22_1_99': ['12', '11', '12']},\n",
       "                OrderedDict([('1036',\n",
       "                              [['1036', '1036_1', '1:', '1:', '1:', '1:'],\n",
       "                               ['1036', '1036_1', '?:', '?:', '?:', '?:'],\n",
       "                               ['1036', '1036_2', '2:', '1:', '2:', '1:'],\n",
       "                               ['1036', '1036_2', '?:', '?:', '?:', '?:'],\n",
       "                               ['1036', '1036_6', '1:', '2:', '1:', '1:'],\n",
       "                               ['1036', '1036_6', '1:', '1:', '1:', '2:'],\n",
       "                               ['1036', '1036_99', '2:', '1|', '2|', '1|'],\n",
       "                               ['1036', '1036_99', '1:', '1|', '1|', '1|'],\n",
       "                               ['1036', '1036_5', '2:', '1|', '2|', '1|'],\n",
       "                               ['1036', '1036_5', '1:', '2|', '1|', '1|'],\n",
       "                               ['1036', '1036_4', '1:', '1|', '1|', '1|'],\n",
       "                               ['1036', '1036_4', '1:', '2|', '1|', '1|'],\n",
       "                               ['1036', '1036_3', '1:', '1|', '1|', '1\\\\'],\n",
       "                               ['1036', '1036_3', '1:', '1|', '1|', '1\\\\']]),\n",
       "                             ('22_1',\n",
       "                              [['22_1', '22_1_10', '2:', '1:', '2:'],\n",
       "                               ['22_1', '22_1_10', '1:', '1:', '1:'],\n",
       "                               ['22_1', '22_1_20', '1:', '1:', '1:'],\n",
       "                               ['22_1', '22_1_20', '1:', '2:', '1:'],\n",
       "                               ['22_1', '22_1_99', '1:', '1|', '1|'],\n",
       "                               ['22_1', '22_1_99', '2:', '1|', '2|'],\n",
       "                               ['22_1', '22_1_5', '1:', '1|', '1|'],\n",
       "                               ['22_1', '22_1_5', '1:', '1|', '1|'],\n",
       "                               ['22_1', '22_1_4', '1:', '1|', '1|'],\n",
       "                               ['22_1', '22_1_4', '1:', '1|', '1|'],\n",
       "                               ['22_1', '22_1_3', '1:', '1|', '1|'],\n",
       "                               ['22_1', '22_1_3', '1:', '1|', '1|'],\n",
       "                               ['22_1', '22_1_2', '1:', '2|', '1|'],\n",
       "                               ['22_1', '22_1_2', '1:', '1|', '1|']])]),\n",
       "                {'V8-13273': 0.1548,\n",
       "                 'V12-13302': 9.451e-05,\n",
       "                 'V17-13417': 0.1197,\n",
       "                 'V22-13687': 0.0001,\n",
       "                 'V15-13380': 0.0088},\n",
       "                {'1036': ['V8-13273', 'V12-13302', 'V17-13417', 'V22-13687'],\n",
       "                 '22_1': ['V8-13273', 'V15-13380', 'V17-13417'],\n",
       "                 '28_9': []},\n",
       "                []],\n",
       "               'output': [(('1036',\n",
       "                  '1036_1',\n",
       "                  '1,1',\n",
       "                  '111,1',\n",
       "                  '0,0',\n",
       "                  'NULL,NULL'),\n",
       "                 ('1036', '1036_2', '3,1', '212,1', '0,0', 'NULL,NULL'),\n",
       "                 ('1036', '1036_6', '2,1', '121,1', '1,2', '111,2'),\n",
       "                 ('1036', '1036_99', '3,1', '212,1', '1,1', '111,1'),\n",
       "                 ('1036', '1036_5', '3,1', '212,1', '2,1', '121,1'),\n",
       "                 ('1036', '1036_4', '1,1', '111,1', '2,1', '121,1'),\n",
       "                 ('1036', '1036_3', '1,1', '111,1', '1,1', '111,1'),\n",
       "                 ('22_1', '22_1_10', '3', '212', '1', '111'),\n",
       "                 ('22_1', '22_1_20', '1', '111', '2', '121'),\n",
       "                 ('22_1', '22_1_99', '1', '111', '3', '212'),\n",
       "                 ('22_1', '22_1_5', '1', '111', '1', '111'),\n",
       "                 ('22_1', '22_1_4', '1', '111', '1', '111'),\n",
       "                 ('22_1', '22_1_3', '1', '111', '1', '111'),\n",
       "                 ('22_1', '22_1_2', '2', '121', '1', '111')),\n",
       "                {'28_9_103': ('0', '0'),\n",
       "                 '28_9_106': ('0', '0'),\n",
       "                 '28_9_108': ('0', '0'),\n",
       "                 '28_9_109': ('0', '0'),\n",
       "                 '28_9_111': ('0', '0'),\n",
       "                 '28_9_105': ('0', '0'),\n",
       "                 '28_9_114': ('0', '0'),\n",
       "                 '28_9_110': ('0', '0'),\n",
       "                 '1036_5': (['3', '1'], ['2', '1']),\n",
       "                 '1036_99': (['3', '1'], ['1', '1']),\n",
       "                 '1036_6': (['2', '1'], ['1', '2']),\n",
       "                 '1036_4': (['1', '1'], ['2', '1']),\n",
       "                 '1036_3': (['1', '1'], ['1', '1']),\n",
       "                 '22_1_2': (['2'], ['1']),\n",
       "                 '22_1_3': (['1'], ['1']),\n",
       "                 '22_1_4': (['1'], ['1']),\n",
       "                 '22_1_5': (['1'], ['1']),\n",
       "                 '22_1_99': (['1'], ['3']),\n",
       "                 '1036_1': (['1', '1'], ['0', '0']),\n",
       "                 '1036_2': (['3', '1'], ['0', '0']),\n",
       "                 '22_1_10': (['3'], ['1']),\n",
       "                 '22_1_20': (['1'], ['2'])},\n",
       "                2,\n",
       "                OrderedDict([('1036',\n",
       "                              ((0.9756108487483803,\n",
       "                                9.22136964316592e-05,\n",
       "                                0.02429693755518805),\n",
       "                               (0.9999, 0.0001))),\n",
       "                             ('22_1',\n",
       "                              ((0.9673214995311192,\n",
       "                                0.008588003627798477,\n",
       "                                0.02409049684108229),))])]}),\n",
       "             ('format',\n",
       "              {'28_9_103': [('0', '0'), ('0', '0')],\n",
       "               '28_9_106': [('0', '0'), ('0', '0')],\n",
       "               '28_9_108': [('0', '0'), ('0', '0')],\n",
       "               '28_9_109': [('0', '0'), ('0', '0')],\n",
       "               '28_9_111': [('0', '0'), ('0', '0')],\n",
       "               '28_9_105': [('0', '0'), ('0', '0')],\n",
       "               '28_9_114': [('0', '0'), ('0', '0')],\n",
       "               '28_9_110': [('0', '0'), ('0', '0')],\n",
       "               '1036_5': [('3', '2'), ('1', '1')],\n",
       "               '1036_99': [('3', '1'), ('1', '1')],\n",
       "               '1036_6': [('2', '1'), ('1', '2')],\n",
       "               '1036_4': [('1', '2'), ('1', '1')],\n",
       "               '1036_3': [('1', '1'), ('1', '1')],\n",
       "               '22_1_2': [('2', '1'), ('0', '0')],\n",
       "               '22_1_3': [('1', '1'), ('0', '0')],\n",
       "               '22_1_4': [('1', '1'), ('0', '0')],\n",
       "               '22_1_5': [('1', '1'), ('0', '0')],\n",
       "               '22_1_99': [('1', '3'), ('0', '0')],\n",
       "               '1036_1': [('1', '0'), ('1', '0')],\n",
       "               '1036_2': [('3', '0'), ('1', '0')],\n",
       "               '22_1_10': [('3', '1'), ('0', '0')],\n",
       "               '22_1_20': [('1', '2'), ('0', '0')]})])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.dtest['LOC102725121@1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = {k:env.dtest[k] for k in ['MC1R','PAPPA2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dtestpy3_fixedcoder.pickle', 'wb') as handle:\n",
    "    pickle.dump(a1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#with open('dtestpy3_fixedcoder.pickle', 'rb') as handle:\n",
    "#    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {k:env.dtest[k] for k in ['MC1R','PAPPA2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dtestpy3.pickle', 'wb') as handle:\n",
    "    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#with open('dtestpy3.pickle', 'rb') as handle:\n",
    "#    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('dregions',\n",
       "              [['16',\n",
       "                '89984286',\n",
       "                '89987385',\n",
       "                'MC1R',\n",
       "                '133.689089888',\n",
       "                '159.050776809',\n",
       "                '111.195245154']]),\n",
       "             ('dvariants',\n",
       "              [[['16', 89984370, 'MC1R', 0.00015],\n",
       "                ['16', 89984604, 'MC1R', 0.00015],\n",
       "                ['16', 89984739, 'MC1R', 0.00015],\n",
       "                ['16', 89985940, 'MC1R', 0.085784],\n",
       "                ['16', 89986608, 'MC1R', 0.107936],\n",
       "                ['16', 89986760, 'MC1R', 0.00015],\n",
       "                ['16', 89987201, 'MC1R', 0.00015]]]),\n",
       "             ('dfamvaridx',\n",
       "              [{'1': [0, 1, 2, 3, 4, 5, 6], '2': [0, 1, 2, 3, 4, 5, 6]}]),\n",
       "             ('dgeno',\n",
       "              [{'II:2': ['12', '12', '11', '11', '11', '12', '12'],\n",
       "                'I:2': ['12', '12', '11', '11', '11', '12', '12'],\n",
       "                'I:1': ['12', '12', '11', '11', '11', '11', '12'],\n",
       "                'II:3': ['12', '12', '11', '11', '11', '12', '12'],\n",
       "                'II:4': ['20', '12', '12', '12', '12', '12', '22'],\n",
       "                'II:1': ['22', '22', '11', '11', '11', '12', '22'],\n",
       "                'II:B': ['11', '11', '11', '11', '11', '11', '12'],\n",
       "                'I:B': ['12', '12', '12', '12', '12', '12', '11'],\n",
       "                'I:A': ['11', '11', '11', '11', '11', '11', '22'],\n",
       "                'II:C': ['11', '11', '11', '11', '11', '11', '12'],\n",
       "                'II:D': ['12', '12', '12', '12', '12', '12', '12'],\n",
       "                'II:A': ['12', '12', '12', '12', '12', '12', '12']}]),\n",
       "             ('gss',\n",
       "              [{0: {'1': ('12', '12', '12', '12', '22', '22'),\n",
       "                 '2': ('12', '11', '22', '12', '12', '12')},\n",
       "                1: {'1': ('12', '12', '12', '12', '22', '22'),\n",
       "                 '2': ('12', '11', '22', '12', '12', '12')},\n",
       "                2: {'1': ('12', '12', '12', '12', '22', '22'),\n",
       "                 '2': ('12', '11', '22', '12', '12', '12')},\n",
       "                3: {'1': ('12', '12', '12', '12', '22', '22'),\n",
       "                 '2': ('12', '11', '22', '12', '12', '12')},\n",
       "                4: {'1': ('12', '12', '12', '12', '22', '22'),\n",
       "                 '2': ('12', '11', '22', '12', '12', '12')},\n",
       "                5: {'1': ('12', '12', '12', '12', '22', '22'),\n",
       "                 '2': ('12', '11', '22', '12', '12', '12')},\n",
       "                6: {'1': ('12', '12', '12', '12', '22', '22'),\n",
       "                 '2': ('12', '11', '22', '12', '12', '12')}}]),\n",
       "             ('hapimp',\n",
       "              {'1': ['1',\n",
       "                ['V0-89984370',\n",
       "                 'V1-89984604',\n",
       "                 'V2-89984739',\n",
       "                 'V3-89985940',\n",
       "                 'V4-89986608',\n",
       "                 'V5-89986760',\n",
       "                 'V6-89987201'],\n",
       "                [89984370,\n",
       "                 89984604,\n",
       "                 89984739,\n",
       "                 89985940,\n",
       "                 89986608,\n",
       "                 89986760,\n",
       "                 89987201],\n",
       "                [0.00015,\n",
       "                 0.00015,\n",
       "                 0.00015,\n",
       "                 0.085784,\n",
       "                 0.107936,\n",
       "                 0.00015,\n",
       "                 0.00015],\n",
       "                (('1', 'I:1', '1:', '1:', '1:', '1:', '1:', '1:', '1:'),\n",
       "                 ('1', 'I:1', '2:', '2:', '1:', '1:', '1:', '1:', '2:'),\n",
       "                 ('1', 'I:2', '2:', '2:', '1:', '1:', '1:', '2:', '2:'),\n",
       "                 ('1', 'I:2', '1:', '1:', '1:', '1:', '1:', '1:', '1:'),\n",
       "                 ('1', 'II:3', '2:', '2|', '1:', '1:', '1:', '2|', '2|'),\n",
       "                 ('1', 'II:3', '1:', '1|', '1:', '1:', '1:', '1|', '1|'),\n",
       "                 ('1', 'II:2', '2:', '2|', '1:', '1:', '1:', '2|', '2|'),\n",
       "                 ('1', 'II:2', '1:', '1|', '1:', '1:', '1:', '1|', '1|'),\n",
       "                 ('1', 'II:1', '2:', '2|', '1:', '1:', '1:', '2|', '2|'),\n",
       "                 ('1', 'II:1', '2:', '2|', '1:', '1:', '1:', '1|', '2|'),\n",
       "                 ('1', 'II:4', '2:', '2|', '1:', '1:', '1:', '2\\\\', '2|'),\n",
       "                 ('1', 'II:4', '1:', '1|', '1:', '1:', '1:', '1\\\\', '2|'))],\n",
       "               '2': ['2',\n",
       "                ['V0-89984370',\n",
       "                 'V1-89984604',\n",
       "                 'V2-89984739',\n",
       "                 'V3-89985940',\n",
       "                 'V4-89986608',\n",
       "                 'V5-89986760',\n",
       "                 'V6-89987201'],\n",
       "                [89984370,\n",
       "                 89984604,\n",
       "                 89984739,\n",
       "                 89985940,\n",
       "                 89986608,\n",
       "                 89986760,\n",
       "                 89987201],\n",
       "                [0.00015,\n",
       "                 0.00015,\n",
       "                 0.00015,\n",
       "                 0.085784,\n",
       "                 0.107936,\n",
       "                 0.00015,\n",
       "                 0.00015],\n",
       "                (('2', 'I:A', '1:', '1:', '1:', '1:', '1:', '1:', '2:'),\n",
       "                 ('2', 'I:A', '1:', '1:', '1:', '1:', '1:', '1:', '2:'),\n",
       "                 ('2', 'I:B', '2:', '2:', '2:', '2:', '2:', '2:', '1:'),\n",
       "                 ('2', 'I:B', '1:', '1:', '1:', '1:', '1:', '1:', '1:'),\n",
       "                 ('2', 'II:D', '2:', '2|', '2|', '2|', '2|', '2|', '1:'),\n",
       "                 ('2', 'II:D', '1:', '1|', '1|', '1|', '1|', '1|', '2:'),\n",
       "                 ('2', 'II:C', '1:', '1|', '1|', '1|', '1|', '1|', '1:'),\n",
       "                 ('2', 'II:C', '1:', '1|', '1|', '1|', '1|', '1|', '2:'),\n",
       "                 ('2', 'II:B', '1:', '1|', '1|', '1|', '1|', '1|', '1:'),\n",
       "                 ('2', 'II:B', '1:', '1|', '1|', '1|', '1|', '1|', '2:'),\n",
       "                 ('2', 'II:A', '2:', '2|', '2|', '2|', '2|', '2|', '1:'),\n",
       "                 ('2', 'II:A', '1:', '1|', '1|', '1|', '1|', '1|', '2:')),\n",
       "                {'V0-89984370': 0.00015,\n",
       "                 'V1-89984604': 0.00015,\n",
       "                 'V2-89984739': 0.00015,\n",
       "                 'V3-89985940': 0.085784,\n",
       "                 'V4-89986608': 0.107936,\n",
       "                 'V5-89986760': 0.00015,\n",
       "                 'V6-89987201': 0.00015}]}),\n",
       "             ('ld',\n",
       "              [[[None],\n",
       "                [1.0, None],\n",
       "                [0.2380952380952381, 0.2380952380952381, None],\n",
       "                [0.2380952380952381, 0.2380952380952381, 1.0, None],\n",
       "                [0.2380952380952381, 0.2380952380952381, 1.0, 1.0, None],\n",
       "                [0.5555555555555556,\n",
       "                 0.5555555555555556,\n",
       "                 0.42857142857142866,\n",
       "                 0.42857142857142866,\n",
       "                 0.42857142857142866,\n",
       "                 None],\n",
       "                [0.06666666666666665,\n",
       "                 0.06666666666666665,\n",
       "                 0.14285714285714282,\n",
       "                 0.14285714285714282,\n",
       "                 0.14285714285714282,\n",
       "                 0.0,\n",
       "                 None]],\n",
       "               [[0, 1], [2, 3, 4], [3, 4]],\n",
       "               [['V0-89984370', 'V1-89984604'],\n",
       "                ['V2-89984739', 'V3-89985940', 'V4-89986608']]]),\n",
       "             ('coder',\n",
       "              {'input': [{'II:2': ['12', '12', '11', '11', '11', '12', '12'],\n",
       "                 'I:2': ['12', '12', '11', '11', '11', '12', '12'],\n",
       "                 'I:1': ['12', '12', '11', '11', '11', '11', '12'],\n",
       "                 'II:3': ['12', '12', '11', '11', '11', '12', '12'],\n",
       "                 'II:4': ['20', '12', '12', '12', '12', '12', '22'],\n",
       "                 'II:1': ['22', '22', '11', '11', '11', '12', '22'],\n",
       "                 'II:B': ['11', '11', '11', '11', '11', '11', '12'],\n",
       "                 'I:B': ['12', '12', '12', '12', '12', '12', '11'],\n",
       "                 'I:A': ['11', '11', '11', '11', '11', '11', '22'],\n",
       "                 'II:C': ['11', '11', '11', '11', '11', '11', '12'],\n",
       "                 'II:D': ['12', '12', '12', '12', '12', '12', '12'],\n",
       "                 'II:A': ['12', '12', '12', '12', '12', '12', '12']},\n",
       "                OrderedDict([('1',\n",
       "                              [['1',\n",
       "                                'I:1',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:'],\n",
       "                               ['1',\n",
       "                                'I:1',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '2:'],\n",
       "                               ['1',\n",
       "                                'I:2',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '2:',\n",
       "                                '2:'],\n",
       "                               ['1',\n",
       "                                'I:2',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:'],\n",
       "                               ['1',\n",
       "                                'II:3',\n",
       "                                '2:',\n",
       "                                '2|',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '2|',\n",
       "                                '2|'],\n",
       "                               ['1',\n",
       "                                'II:3',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1|'],\n",
       "                               ['1',\n",
       "                                'II:2',\n",
       "                                '2:',\n",
       "                                '2|',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '2|',\n",
       "                                '2|'],\n",
       "                               ['1',\n",
       "                                'II:2',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1|'],\n",
       "                               ['1',\n",
       "                                'II:1',\n",
       "                                '2:',\n",
       "                                '2|',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '2|',\n",
       "                                '2|'],\n",
       "                               ['1',\n",
       "                                'II:1',\n",
       "                                '2:',\n",
       "                                '2|',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '2|'],\n",
       "                               ['1',\n",
       "                                'II:4',\n",
       "                                '2:',\n",
       "                                '2|',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '2\\\\',\n",
       "                                '2|'],\n",
       "                               ['1',\n",
       "                                'II:4',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1\\\\',\n",
       "                                '2|']]),\n",
       "                             ('2',\n",
       "                              [['2',\n",
       "                                'I:A',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '2:'],\n",
       "                               ['2',\n",
       "                                'I:A',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '2:'],\n",
       "                               ['2',\n",
       "                                'I:B',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '1:'],\n",
       "                               ['2',\n",
       "                                'I:B',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:'],\n",
       "                               ['2',\n",
       "                                'II:D',\n",
       "                                '2:',\n",
       "                                '2|',\n",
       "                                '2|',\n",
       "                                '2|',\n",
       "                                '2|',\n",
       "                                '2|',\n",
       "                                '1:'],\n",
       "                               ['2',\n",
       "                                'II:D',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '2:'],\n",
       "                               ['2',\n",
       "                                'II:C',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1:'],\n",
       "                               ['2',\n",
       "                                'II:C',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '2:'],\n",
       "                               ['2',\n",
       "                                'II:B',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1:'],\n",
       "                               ['2',\n",
       "                                'II:B',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '2:'],\n",
       "                               ['2',\n",
       "                                'II:A',\n",
       "                                '2:',\n",
       "                                '2|',\n",
       "                                '2|',\n",
       "                                '2|',\n",
       "                                '2|',\n",
       "                                '2|',\n",
       "                                '1:'],\n",
       "                               ['2',\n",
       "                                'II:A',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '2:']])]),\n",
       "                {'V0-89984370': 0.00015,\n",
       "                 'V1-89984604': 0.00015,\n",
       "                 'V2-89984739': 0.00015,\n",
       "                 'V3-89985940': 0.085784,\n",
       "                 'V4-89986608': 0.107936,\n",
       "                 'V5-89986760': 0.00015,\n",
       "                 'V6-89987201': 0.00015},\n",
       "                {'1': ['V0-89984370',\n",
       "                  'V1-89984604',\n",
       "                  'V2-89984739',\n",
       "                  'V3-89985940',\n",
       "                  'V4-89986608',\n",
       "                  'V5-89986760',\n",
       "                  'V6-89987201'],\n",
       "                 '2': ['V0-89984370',\n",
       "                  'V1-89984604',\n",
       "                  'V2-89984739',\n",
       "                  'V3-89985940',\n",
       "                  'V4-89986608',\n",
       "                  'V5-89986760',\n",
       "                  'V6-89987201']},\n",
       "                [['V0-89984370', 'V1-89984604'],\n",
       "                 ['V2-89984739', 'V3-89985940', 'V4-89986608']]],\n",
       "               'output': [(('1', 'I:1', '1,1', '1111,11', '2,2', '2222,12'),\n",
       "                 ('1', 'I:2', '2,3', '2222,22', '1,1', '1111,11'),\n",
       "                 ('1', 'II:3', '2,3', '2222,22', '1,1', '1111,11'),\n",
       "                 ('1', 'II:2', '2,3', '2222,22', '1,1', '1111,11'),\n",
       "                 ('1', 'II:1', '2,3', '2222,22', '2,2', '2222,12'),\n",
       "                 ('1', 'II:4', '2,3', '2222,22', '1,2', '1111,12'),\n",
       "                 ('2', 'I:A', '2', '111112', '2', '111112'),\n",
       "                 ('2', 'I:B', '3', '222221', '1', '111111'),\n",
       "                 ('2', 'II:D', '3', '222221', '2', '111112'),\n",
       "                 ('2', 'II:C', '1', '111111', '2', '111112'),\n",
       "                 ('2', 'II:B', '1', '111111', '2', '111112'),\n",
       "                 ('2', 'II:A', '3', '222221', '2', '111112')),\n",
       "                {'II:2': (['2', '3'], ['2222', '22']),\n",
       "                 'I:2': (['2', '3'], ['2222', '22']),\n",
       "                 'I:1': (['1', '1'], ['1111', '11']),\n",
       "                 'II:3': (['2', '3'], ['2222', '22']),\n",
       "                 'II:4': (['2', '3'], ['2222', '22']),\n",
       "                 'II:1': (['2', '3'], ['2222', '22']),\n",
       "                 'II:B': (['1'], ['111111']),\n",
       "                 'I:B': (['3'], ['222221']),\n",
       "                 'I:A': (['2'], ['111112']),\n",
       "                 'II:C': (['1'], ['111111']),\n",
       "                 'II:D': (['3'], ['222221']),\n",
       "                 'II:A': (['3'], ['222221'])},\n",
       "                2,\n",
       "                OrderedDict([('1',\n",
       "                              ((0.999999999999997, 3.0400071909574507e-15),\n",
       "                               (0.9995498650574084,\n",
       "                                0.000449932412250011,\n",
       "                                2.0253034157679256e-07))),\n",
       "                             ('2',\n",
       "                              ((0.999550067496625,\n",
       "                                0.0004499325033749946,\n",
       "                                1.3677980457054751e-18),))])]}),\n",
       "             ('format',\n",
       "              {'II:2': [('2', '2222'), ('3', '22')],\n",
       "               'I:2': [('2', '2222'), ('3', '22')],\n",
       "               'I:1': [('1', '1111'), ('1', '11')],\n",
       "               'II:3': [('2', '2222'), ('3', '22')],\n",
       "               'II:4': [('2', '2222'), ('3', '22')],\n",
       "               'II:1': [('2', '2222'), ('3', '22')],\n",
       "               'II:B': [('1', '111111'), ('0', '0')],\n",
       "               'I:B': [('3', '222221'), ('0', '0')],\n",
       "               'I:A': [('2', '111112'), ('0', '0')],\n",
       "               'II:C': [('1', '111111'), ('0', '0')],\n",
       "               'II:D': [('3', '222221'), ('0', '0')],\n",
       "               'II:A': [('3', '222221'), ('0', '0')]})])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['MC1R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('dregions',\n",
       "              [['1',\n",
       "                '176432306',\n",
       "                '176811970',\n",
       "                'PAPPA2',\n",
       "                '186.278964324',\n",
       "                '238.991541401',\n",
       "                '136.402021932']]),\n",
       "             ('dvariants',\n",
       "              [[['1', 176659933, 'PAPPA2', 0.00015],\n",
       "                ['1', 176660247, 'PAPPA2', 0.00015],\n",
       "                ['1', 176660371, 'PAPPA2', 0.00015],\n",
       "                ['1', 176664842, 'PAPPA2', 0.241943],\n",
       "                ['1', 176668823, 'PAPPA2', 0.00015],\n",
       "                ['1', 176671914, 'PAPPA2', 0.00015],\n",
       "                ['1', 176679384, 'PAPPA2', 0.00015],\n",
       "                ['1', 176734756, 'PAPPA2', 0.352899],\n",
       "                ['1', 176811754, 'PAPPA2', 0.00015],\n",
       "                ['1', 176811873, 'PAPPA2', 0.00015]]]),\n",
       "             ('dfamvaridx',\n",
       "              [{'1': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "                '2': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}]),\n",
       "             ('dgeno',\n",
       "              [{'II:2': ['12',\n",
       "                 '11',\n",
       "                 '12',\n",
       "                 '11',\n",
       "                 '12',\n",
       "                 '11',\n",
       "                 '11',\n",
       "                 '12',\n",
       "                 '11',\n",
       "                 '12'],\n",
       "                'I:2': ['11',\n",
       "                 '11',\n",
       "                 '11',\n",
       "                 '11',\n",
       "                 '11',\n",
       "                 '11',\n",
       "                 '11',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '11'],\n",
       "                'I:1': ['12',\n",
       "                 '12',\n",
       "                 '22',\n",
       "                 '12',\n",
       "                 '22',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '22',\n",
       "                 '12',\n",
       "                 '12'],\n",
       "                'II:3': ['12',\n",
       "                 '11',\n",
       "                 '12',\n",
       "                 '11',\n",
       "                 '12',\n",
       "                 '11',\n",
       "                 '11',\n",
       "                 '12',\n",
       "                 '11',\n",
       "                 '12'],\n",
       "                'II:4': ['11',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '22',\n",
       "                 '22',\n",
       "                 '11'],\n",
       "                'II:1': ['12',\n",
       "                 '11',\n",
       "                 '12',\n",
       "                 '11',\n",
       "                 '12',\n",
       "                 '11',\n",
       "                 '11',\n",
       "                 '12',\n",
       "                 '11',\n",
       "                 '12'],\n",
       "                'II:B': ['12',\n",
       "                 '11',\n",
       "                 '11',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '11',\n",
       "                 '11',\n",
       "                 '11'],\n",
       "                'I:B': ['12',\n",
       "                 '11',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '22',\n",
       "                 '22',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '12'],\n",
       "                'I:A': ['11',\n",
       "                 '12',\n",
       "                 '11',\n",
       "                 '11',\n",
       "                 '11',\n",
       "                 '11',\n",
       "                 '11',\n",
       "                 '11',\n",
       "                 '11',\n",
       "                 '11'],\n",
       "                'II:C': ['12',\n",
       "                 '11',\n",
       "                 '11',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '11',\n",
       "                 '11',\n",
       "                 '11'],\n",
       "                'II:D': ['11',\n",
       "                 '11',\n",
       "                 '12',\n",
       "                 '11',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '11',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '12'],\n",
       "                'II:A': ['11',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '11',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '11',\n",
       "                 '12',\n",
       "                 '12',\n",
       "                 '12']}]),\n",
       "             ('gss',\n",
       "              [{0: {'1': ('12', '11', '12', '12', '11', '12'),\n",
       "                 '2': ('11', '12', '11', '11', '12', '12')},\n",
       "                1: {'1': ('12', '11', '12', '12', '11', '12'),\n",
       "                 '2': ('11', '12', '11', '11', '12', '12')},\n",
       "                2: {'1': ('12', '11', '12', '12', '11', '12'),\n",
       "                 '2': ('11', '12', '11', '11', '12', '12')},\n",
       "                3: {'1': ('12', '11', '12', '12', '11', '12'),\n",
       "                 '2': ('11', '12', '11', '11', '12', '12')},\n",
       "                4: {'1': ('12', '11', '12', '12', '11', '12'),\n",
       "                 '2': ('11', '12', '11', '11', '12', '12')},\n",
       "                5: {'1': ('12', '11', '12', '12', '11', '12'),\n",
       "                 '2': ('11', '12', '11', '11', '12', '12')},\n",
       "                6: {'1': ('12', '11', '12', '12', '11', '12'),\n",
       "                 '2': ('11', '12', '11', '11', '12', '12')},\n",
       "                7: {'1': ('12', '11', '12', '12', '11', '12'),\n",
       "                 '2': ('11', '12', '11', '11', '12', '12')},\n",
       "                8: {'1': ('12', '11', '12', '12', '11', '12'),\n",
       "                 '2': ('11', '12', '11', '11', '12', '12')},\n",
       "                9: {'1': ('12', '11', '12', '12', '11', '12'),\n",
       "                 '2': ('11', '12', '11', '11', '12', '12')}}]),\n",
       "             ('hapimp',\n",
       "              {'1': ['1',\n",
       "                ['V0-176659933',\n",
       "                 'V1-176660247',\n",
       "                 'V2-176660371',\n",
       "                 'V3-176664842',\n",
       "                 'V4-176668823',\n",
       "                 'V5-176671914',\n",
       "                 'V6-176679384',\n",
       "                 'V7-176734756',\n",
       "                 'V8-176811754',\n",
       "                 'V9-176811873'],\n",
       "                [176659933,\n",
       "                 176660247,\n",
       "                 176660371,\n",
       "                 176664842,\n",
       "                 176668823,\n",
       "                 176671914,\n",
       "                 176679384,\n",
       "                 176734756,\n",
       "                 176811754,\n",
       "                 176811873],\n",
       "                [0.00015,\n",
       "                 0.00015,\n",
       "                 0.00015,\n",
       "                 0.241943,\n",
       "                 0.00015,\n",
       "                 0.00015,\n",
       "                 0.00015,\n",
       "                 0.352899,\n",
       "                 0.00015,\n",
       "                 0.00015],\n",
       "                (('1',\n",
       "                  'I:1',\n",
       "                  '1:',\n",
       "                  '2:',\n",
       "                  '2:',\n",
       "                  '2:',\n",
       "                  '2:',\n",
       "                  '2:',\n",
       "                  '2:',\n",
       "                  '2:',\n",
       "                  '2:',\n",
       "                  '1:'),\n",
       "                 ('1',\n",
       "                  'I:1',\n",
       "                  '2:',\n",
       "                  '1:',\n",
       "                  '2:',\n",
       "                  '1:',\n",
       "                  '2:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '2:',\n",
       "                  '1:',\n",
       "                  '2:'),\n",
       "                 ('1',\n",
       "                  'I:2',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '2:',\n",
       "                  '2:',\n",
       "                  '1:'),\n",
       "                 ('1',\n",
       "                  'I:2',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:'),\n",
       "                 ('1',\n",
       "                  'II:4',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '2|',\n",
       "                  '2|',\n",
       "                  '1|'),\n",
       "                 ('1',\n",
       "                  'II:4',\n",
       "                  '1:',\n",
       "                  '2|',\n",
       "                  '2:',\n",
       "                  '2|',\n",
       "                  '2:',\n",
       "                  '2|',\n",
       "                  '2|',\n",
       "                  '2|',\n",
       "                  '2|',\n",
       "                  '1|'),\n",
       "                 ('1',\n",
       "                  'II:3',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|'),\n",
       "                 ('1',\n",
       "                  'II:3',\n",
       "                  '2:',\n",
       "                  '1|',\n",
       "                  '2:',\n",
       "                  '1|',\n",
       "                  '2:',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '2|',\n",
       "                  '1|',\n",
       "                  '2|'),\n",
       "                 ('1',\n",
       "                  'II:2',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|'),\n",
       "                 ('1',\n",
       "                  'II:2',\n",
       "                  '2:',\n",
       "                  '1|',\n",
       "                  '2:',\n",
       "                  '1|',\n",
       "                  '2:',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '2|',\n",
       "                  '1|',\n",
       "                  '2|'),\n",
       "                 ('1',\n",
       "                  'II:1',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|'),\n",
       "                 ('1',\n",
       "                  'II:1',\n",
       "                  '2:',\n",
       "                  '1|',\n",
       "                  '2:',\n",
       "                  '1|',\n",
       "                  '2:',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '2|',\n",
       "                  '1|',\n",
       "                  '2|'))],\n",
       "               '2': ['2',\n",
       "                ['V0-176659933',\n",
       "                 'V1-176660247',\n",
       "                 'V2-176660371',\n",
       "                 'V3-176664842',\n",
       "                 'V4-176668823',\n",
       "                 'V5-176671914',\n",
       "                 'V6-176679384',\n",
       "                 'V7-176734756',\n",
       "                 'V8-176811754',\n",
       "                 'V9-176811873'],\n",
       "                [176659933,\n",
       "                 176660247,\n",
       "                 176660371,\n",
       "                 176664842,\n",
       "                 176668823,\n",
       "                 176671914,\n",
       "                 176679384,\n",
       "                 176734756,\n",
       "                 176811754,\n",
       "                 176811873],\n",
       "                [0.00015,\n",
       "                 0.00015,\n",
       "                 0.00015,\n",
       "                 0.241943,\n",
       "                 0.00015,\n",
       "                 0.00015,\n",
       "                 0.00015,\n",
       "                 0.352899,\n",
       "                 0.00015,\n",
       "                 0.00015],\n",
       "                (('2',\n",
       "                  'I:A',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:'),\n",
       "                 ('2',\n",
       "                  'I:A',\n",
       "                  '1:',\n",
       "                  '2:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:'),\n",
       "                 ('2',\n",
       "                  'I:B',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '2:',\n",
       "                  '1:',\n",
       "                  '2:',\n",
       "                  '2:',\n",
       "                  '1:',\n",
       "                  '2:',\n",
       "                  '2:',\n",
       "                  '2:'),\n",
       "                 ('2',\n",
       "                  'I:B',\n",
       "                  '2:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '2:',\n",
       "                  '2:',\n",
       "                  '2:',\n",
       "                  '2:',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1:'),\n",
       "                 ('2',\n",
       "                  'II:D',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '2|',\n",
       "                  '1|',\n",
       "                  '2:',\n",
       "                  '2:',\n",
       "                  '1|',\n",
       "                  '2|',\n",
       "                  '2|',\n",
       "                  '2|'),\n",
       "                 ('2',\n",
       "                  'II:D',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|'),\n",
       "                 ('2',\n",
       "                  'II:C',\n",
       "                  '2:',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '2|',\n",
       "                  '2:',\n",
       "                  '2:',\n",
       "                  '2|',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|'),\n",
       "                 ('2',\n",
       "                  'II:C',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|'),\n",
       "                 ('2',\n",
       "                  'II:B',\n",
       "                  '2:',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '2|',\n",
       "                  '2:',\n",
       "                  '2:',\n",
       "                  '2|',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|'),\n",
       "                 ('2',\n",
       "                  'II:B',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|'),\n",
       "                 ('2',\n",
       "                  'II:A',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '2|',\n",
       "                  '1|',\n",
       "                  '2:',\n",
       "                  '2:',\n",
       "                  '1|',\n",
       "                  '2|',\n",
       "                  '2|',\n",
       "                  '2|'),\n",
       "                 ('2',\n",
       "                  'II:A',\n",
       "                  '1:',\n",
       "                  '2|',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1:',\n",
       "                  '1:',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|',\n",
       "                  '1|')),\n",
       "                {'V0-176659933': 0.00015,\n",
       "                 'V1-176660247': 0.00015,\n",
       "                 'V2-176660371': 0.00015,\n",
       "                 'V3-176664842': 0.241943,\n",
       "                 'V4-176668823': 0.00015,\n",
       "                 'V5-176671914': 0.00015,\n",
       "                 'V6-176679384': 0.00015,\n",
       "                 'V7-176734756': 0.352899,\n",
       "                 'V8-176811754': 0.00015,\n",
       "                 'V9-176811873': 0.00015}]}),\n",
       "             ('ld',\n",
       "              [[[None],\n",
       "                [0.1111111111111111, None],\n",
       "                [0.022222222222222223, 0.022222222222222223, None],\n",
       "                [0.1111111111111111,\n",
       "                 0.1111111111111111,\n",
       "                 0.022222222222222223,\n",
       "                 None],\n",
       "                [0.3333333333333334,\n",
       "                 0.0,\n",
       "                 0.6000000000000001,\n",
       "                 0.3333333333333334,\n",
       "                 None],\n",
       "                [0.022222222222222223,\n",
       "                 0.022222222222222223,\n",
       "                 0.2177777777777778,\n",
       "                 0.5555555555555556,\n",
       "                 0.6000000000000001,\n",
       "                 None],\n",
       "                [0.1111111111111111,\n",
       "                 0.1111111111111111,\n",
       "                 0.022222222222222223,\n",
       "                 1.0,\n",
       "                 0.3333333333333334,\n",
       "                 0.5555555555555556,\n",
       "                 None],\n",
       "                [0.0,\n",
       "                 0.0,\n",
       "                 0.6000000000000001,\n",
       "                 0.0,\n",
       "                 0.25,\n",
       "                 0.06666666666666665,\n",
       "                 0.0,\n",
       "                 None],\n",
       "                [0.19999999999999998,\n",
       "                 0.022222222222222223,\n",
       "                 0.2177777777777778,\n",
       "                 0.022222222222222223,\n",
       "                 0.06666666666666665,\n",
       "                 0.2177777777777778,\n",
       "                 0.022222222222222223,\n",
       "                 0.6000000000000001,\n",
       "                 None],\n",
       "                [0.1111111111111111,\n",
       "                 0.1111111111111111,\n",
       "                 0.5555555555555556,\n",
       "                 0.1111111111111111,\n",
       "                 0.3333333333333334,\n",
       "                 0.022222222222222223,\n",
       "                 0.1111111111111111,\n",
       "                 0.3333333333333334,\n",
       "                 0.022222222222222223,\n",
       "                 None]],\n",
       "               [[3, 6]],\n",
       "               [['V3-176664842', 'V6-176679384']]]),\n",
       "             ('coder',\n",
       "              {'input': [{'II:2': ['12',\n",
       "                  '11',\n",
       "                  '12',\n",
       "                  '11',\n",
       "                  '12',\n",
       "                  '11',\n",
       "                  '11',\n",
       "                  '12',\n",
       "                  '11',\n",
       "                  '12'],\n",
       "                 'I:2': ['11',\n",
       "                  '11',\n",
       "                  '11',\n",
       "                  '11',\n",
       "                  '11',\n",
       "                  '11',\n",
       "                  '11',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '11'],\n",
       "                 'I:1': ['12',\n",
       "                  '12',\n",
       "                  '22',\n",
       "                  '12',\n",
       "                  '22',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '22',\n",
       "                  '12',\n",
       "                  '12'],\n",
       "                 'II:3': ['12',\n",
       "                  '11',\n",
       "                  '12',\n",
       "                  '11',\n",
       "                  '12',\n",
       "                  '11',\n",
       "                  '11',\n",
       "                  '12',\n",
       "                  '11',\n",
       "                  '12'],\n",
       "                 'II:4': ['11',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '22',\n",
       "                  '22',\n",
       "                  '11'],\n",
       "                 'II:1': ['12',\n",
       "                  '11',\n",
       "                  '12',\n",
       "                  '11',\n",
       "                  '12',\n",
       "                  '11',\n",
       "                  '11',\n",
       "                  '12',\n",
       "                  '11',\n",
       "                  '12'],\n",
       "                 'II:B': ['12',\n",
       "                  '11',\n",
       "                  '11',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '11',\n",
       "                  '11',\n",
       "                  '11'],\n",
       "                 'I:B': ['12',\n",
       "                  '11',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '22',\n",
       "                  '22',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '12'],\n",
       "                 'I:A': ['11',\n",
       "                  '12',\n",
       "                  '11',\n",
       "                  '11',\n",
       "                  '11',\n",
       "                  '11',\n",
       "                  '11',\n",
       "                  '11',\n",
       "                  '11',\n",
       "                  '11'],\n",
       "                 'II:C': ['12',\n",
       "                  '11',\n",
       "                  '11',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '11',\n",
       "                  '11',\n",
       "                  '11'],\n",
       "                 'II:D': ['11',\n",
       "                  '11',\n",
       "                  '12',\n",
       "                  '11',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '11',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '12'],\n",
       "                 'II:A': ['11',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '11',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '11',\n",
       "                  '12',\n",
       "                  '12',\n",
       "                  '12']},\n",
       "                OrderedDict([('1',\n",
       "                              [['1',\n",
       "                                'I:1',\n",
       "                                '1:',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '1:'],\n",
       "                               ['1',\n",
       "                                'I:1',\n",
       "                                '2:',\n",
       "                                '1:',\n",
       "                                '2:',\n",
       "                                '1:',\n",
       "                                '2:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '2:',\n",
       "                                '1:',\n",
       "                                '2:'],\n",
       "                               ['1',\n",
       "                                'I:2',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '1:'],\n",
       "                               ['1',\n",
       "                                'I:2',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:'],\n",
       "                               ['1',\n",
       "                                'II:4',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '2|',\n",
       "                                '2|',\n",
       "                                '1|'],\n",
       "                               ['1',\n",
       "                                'II:4',\n",
       "                                '1:',\n",
       "                                '2|',\n",
       "                                '2:',\n",
       "                                '2|',\n",
       "                                '2:',\n",
       "                                '2|',\n",
       "                                '2|',\n",
       "                                '2|',\n",
       "                                '2|',\n",
       "                                '1|'],\n",
       "                               ['1',\n",
       "                                'II:3',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|'],\n",
       "                               ['1',\n",
       "                                'II:3',\n",
       "                                '2:',\n",
       "                                '1|',\n",
       "                                '2:',\n",
       "                                '1|',\n",
       "                                '2:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '2|',\n",
       "                                '1|',\n",
       "                                '2|'],\n",
       "                               ['1',\n",
       "                                'II:2',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|'],\n",
       "                               ['1',\n",
       "                                'II:2',\n",
       "                                '2:',\n",
       "                                '1|',\n",
       "                                '2:',\n",
       "                                '1|',\n",
       "                                '2:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '2|',\n",
       "                                '1|',\n",
       "                                '2|'],\n",
       "                               ['1',\n",
       "                                'II:1',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|'],\n",
       "                               ['1',\n",
       "                                'II:1',\n",
       "                                '2:',\n",
       "                                '1|',\n",
       "                                '2:',\n",
       "                                '1|',\n",
       "                                '2:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '2|',\n",
       "                                '1|',\n",
       "                                '2|']]),\n",
       "                             ('2',\n",
       "                              [['2',\n",
       "                                'I:A',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:'],\n",
       "                               ['2',\n",
       "                                'I:A',\n",
       "                                '1:',\n",
       "                                '2:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:'],\n",
       "                               ['2',\n",
       "                                'I:B',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '2:',\n",
       "                                '1:',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '1:',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '2:'],\n",
       "                               ['2',\n",
       "                                'I:B',\n",
       "                                '2:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1:'],\n",
       "                               ['2',\n",
       "                                'II:D',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '2|',\n",
       "                                '1|',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '1|',\n",
       "                                '2|',\n",
       "                                '2|',\n",
       "                                '2|'],\n",
       "                               ['2',\n",
       "                                'II:D',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|'],\n",
       "                               ['2',\n",
       "                                'II:C',\n",
       "                                '2:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '2|',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '2|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|'],\n",
       "                               ['2',\n",
       "                                'II:C',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|'],\n",
       "                               ['2',\n",
       "                                'II:B',\n",
       "                                '2:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '2|',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '2|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|'],\n",
       "                               ['2',\n",
       "                                'II:B',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|'],\n",
       "                               ['2',\n",
       "                                'II:A',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '2|',\n",
       "                                '1|',\n",
       "                                '2:',\n",
       "                                '2:',\n",
       "                                '1|',\n",
       "                                '2|',\n",
       "                                '2|',\n",
       "                                '2|'],\n",
       "                               ['2',\n",
       "                                'II:A',\n",
       "                                '1:',\n",
       "                                '2|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1:',\n",
       "                                '1:',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|',\n",
       "                                '1|']])]),\n",
       "                {'V0-176659933': 0.00015,\n",
       "                 'V1-176660247': 0.00015,\n",
       "                 'V2-176660371': 0.00015,\n",
       "                 'V3-176664842': 0.241943,\n",
       "                 'V4-176668823': 0.00015,\n",
       "                 'V5-176671914': 0.00015,\n",
       "                 'V6-176679384': 0.00015,\n",
       "                 'V7-176734756': 0.352899,\n",
       "                 'V8-176811754': 0.00015,\n",
       "                 'V9-176811873': 0.00015},\n",
       "                {'1': ['V0-176659933',\n",
       "                  'V1-176660247',\n",
       "                  'V2-176660371',\n",
       "                  'V3-176664842',\n",
       "                  'V4-176668823',\n",
       "                  'V5-176671914',\n",
       "                  'V6-176679384',\n",
       "                  'V7-176734756',\n",
       "                  'V8-176811754',\n",
       "                  'V9-176811873'],\n",
       "                 '2': ['V0-176659933',\n",
       "                  'V1-176660247',\n",
       "                  'V2-176660371',\n",
       "                  'V3-176664842',\n",
       "                  'V4-176668823',\n",
       "                  'V5-176671914',\n",
       "                  'V6-176679384',\n",
       "                  'V7-176734756',\n",
       "                  'V8-176811754',\n",
       "                  'V9-176811873']},\n",
       "                [['V3-176664842', 'V6-176679384']]],\n",
       "               'output': [(('1', 'I:1', '4', '1222222221', '3', '2122212212'),\n",
       "                 ('1', 'I:2', '2', '1111111221', '1', '1111111111'),\n",
       "                 ('1', 'II:4', '2', '1111111221', '4', '1222222221'),\n",
       "                 ('1', 'II:3', '1', '1111111111', '3', '2122212212'),\n",
       "                 ('1', 'II:2', '1', '1111111111', '3', '2122212212'),\n",
       "                 ('1', 'II:1', '1', '1111111111', '3', '2122212212'),\n",
       "                 ('2', 'I:A', '1', '1111111111', '2', '1212112111'),\n",
       "                 ('2', 'I:B', '4', '1121221222', '3', '2112222111'),\n",
       "                 ('2', 'II:D', '4', '1121221222', '1', '1111111111'),\n",
       "                 ('2', 'II:C', '3', '2112222111', '1', '1111111111'),\n",
       "                 ('2', 'II:B', '3', '2112222111', '1', '1111111111'),\n",
       "                 ('2', 'II:A', '4', '1121221222', '2', '1212112111')),\n",
       "                {'II:2': (['1'], ['1111111111']),\n",
       "                 'I:2': (['2'], ['1111111221']),\n",
       "                 'I:1': (['4'], ['1222222221']),\n",
       "                 'II:3': (['1'], ['1111111111']),\n",
       "                 'II:4': (['2'], ['1111111221']),\n",
       "                 'II:1': (['1'], ['1111111111']),\n",
       "                 'II:B': (['3'], ['2112222111']),\n",
       "                 'I:B': (['4'], ['1121221222']),\n",
       "                 'I:A': (['1'], ['1111111111']),\n",
       "                 'II:C': (['3'], ['2112222111']),\n",
       "                 'II:D': (['4'], ['1121221222']),\n",
       "                 'II:A': (['4'], ['1121221222'])},\n",
       "                1,\n",
       "                OrderedDict([('1',\n",
       "                              ((0.9998363838109469,\n",
       "                                0.00016361618905309718,\n",
       "                                1.3225147758256331e-20,\n",
       "                                7.937469761930493e-24),)),\n",
       "                             ('2',\n",
       "                              ((0.9999999928166995,\n",
       "                                7.183300126057594e-09,\n",
       "                                3.233697565987606e-16,\n",
       "                                1.6580038589959425e-19),))])]}),\n",
       "             ('format',\n",
       "              {'II:2': [('1', '1111111111')],\n",
       "               'I:2': [('2', '1111111221')],\n",
       "               'I:1': [('4', '1222222221')],\n",
       "               'II:3': [('1', '1111111111')],\n",
       "               'II:4': [('2', '1111111221')],\n",
       "               'II:1': [('1', '1111111111')],\n",
       "               'II:B': [('3', '2112222111')],\n",
       "               'I:B': [('4', '1121221222')],\n",
       "               'I:A': [('1', '1111111111')],\n",
       "               'II:C': [('3', '2112222111')],\n",
       "               'II:D': [('4', '1121221222')],\n",
       "               'II:A': [('4', '1121221222')]})])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['PAPPA2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One region test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = regions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor.getRegion(region)\n",
    "maker.getRegion(region)\n",
    "writer.getRegion(region)  \n",
    "extractor.apply(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data {'II:2': ['12', '12', '11', '11', '11', '12', '12'], 'I:2': ['12', '12', '11', '11', '11', '12', '12'], 'I:1': ['12', '12', '11', '11', '11', '11', '12'], 'II:3': ['12', '12', '11', '11', '11', '12', '12'], 'II:4': ['20', '12', '12', '12', '12', '12', '22'], 'II:1': ['22', '22', '11', '11', '11', '12', '22'], 'II:B': ['11', '11', '11', '11', '11', '11', '12'], 'I:B': ['12', '12', '12', '12', '12', '12', '11'], 'I:A': ['11', '11', '11', '11', '11', '11', '22'], 'II:C': ['11', '11', '11', '11', '11', '11', '12'], 'II:D': ['12', '12', '12', '12', '12', '12', '12'], 'II:A': ['12', '12', '12', '12', '12', '12', '12']}\n",
      "Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V0-89984370 V1-89984604 V2-89984739 V3-89985940 V4-89986608 V5-89986760 \n",
      "   V6-89987201 \n",
      "\n",
      "V0-89984370: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V1-89984604: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V2-89984739: 0 1 \n",
      "total familyCount:1\n",
      "V3-89985940: 0 1 \n",
      "total familyCount:1\n",
      "V4-89986608: 0 1 \n",
      "total familyCount:1\n",
      "V5-89986760: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V6-89987201: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V0-89984370 V1-89984604 V2-89984739 V3-89985940 V4-89986608 V5-89986760 \n",
      "   V6-89987201 \n",
      "\n",
      "V0-89984370: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V1-89984604: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V2-89984739: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V3-89985940: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V4-89986608: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V5-89986760: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V6-89987201: 0 0.5 0.5 \n",
      "total familyCount:1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "alphabet is required for object creation from an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/1974374.1.plot.q/ipykernel_14355/1565046380.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/1974374.1.plot.q/ipykernel_14355/1421714082.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;31m# calculate LD clusters using founder haplotypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__ClusterByLD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhaplotypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvarnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'clusters:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;31m# recoding the genotype of the region\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/1974374.1.plot.q/ipykernel_14355/1421714082.py\u001b[0m in \u001b[0;36m__ClusterByLD\u001b[0;34m(self, data, haplotypes, varnames)\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mfounder_haplotypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}-{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mihap\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;31m# calculate LD blocks, use r2 measure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfounder_haplotypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrixLD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidCharacters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"12\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"r2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mld\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/seqpy3v0/lib/python3.9/site-packages/egglib/_interface.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, source, alphabet)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAlign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mContainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0malphabet\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'alphabet is required for object creation from an iterable'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mnew_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphabet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0mnew_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: alphabet is required for object creation from an iterable"
     ]
    }
   ],
   "source": [
    "maker.apply(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes = OrderedDict()\n",
    "mafs = {}   ##Per fam per variant\n",
    "varnames = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker._MarkerMaker__Haplotype(data, haplotypes, mafs, varnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(data.families.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = items[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running family 1007\n"
     ]
    }
   ],
   "source": [
    "#for item in data.families:\n",
    "print('running family',item)\n",
    "varnames[item], positions, vcf_mafs = data.getFamVariants(item, style = \"map\")\n",
    "if len(varnames[item]) == 0:\n",
    "    print('here')\n",
    "    for person in data.families[item]:\n",
    "        data[person] = maker.missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['V8-13273', 'V17-13417']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varnames[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_log_output=env.tmp_log + str(os.getpid())\n",
    "maker.haplotyper.Execute(data.chrom, varnames[item], sorted(positions), data.getFamSamples(item), maker.rsq, tmp_log_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['4_364', '4_364_1', '0', '0', '1', '00', '00'],\n",
       " ['4_364', '4_364_17', '0', '0', '2', '00', '00'],\n",
       " ['4_364', '4_364_14', '0', '0', '1', '00', '00'],\n",
       " ['4_364', '4_364_13', '0', '0', '1', '00', '00'],\n",
       " ['4_364', '4_364_21', '0', '0', '1', '00', '00'],\n",
       " ['4_364', '4_364_2', '0', '0', '2', '00', '00'],\n",
       " ['4_364', '4_364_7', '4_364_1', '4_364_2', '1', '00', '11'],\n",
       " ['4_364', '4_364_6', '4_364_1', '4_364_2', '2', '11', '11'],\n",
       " ['4_364', '4_364_5', '4_364_1', '4_364_2', '2', '11', '11'],\n",
       " ['4_364', '4_364_20', '4_364_21', '4_364_5', '2', '00', '00'],\n",
       " ['4_364', '4_364_22', '4_364_21', '4_364_5', '1', '00', '00'],\n",
       " ['4_364', '4_364_99', '4_364_1', '4_364_2', '2', '12', '12'],\n",
       " ['4_364', '4_364_9', '4_364_13', '4_364_99', '2', '11', '11'],\n",
       " ['4_364', '4_364_12', '4_364_13', '4_364_99', '2', '00', '00'],\n",
       " ['4_364', '4_364_11', '4_364_13', '4_364_99', '1', '00', '00'],\n",
       " ['4_364', '4_364_10', '4_364_13', '4_364_99', '1', '00', '00'],\n",
       " ['4_364', '4_364_8', '4_364_13', '4_364_99', '2', '00', '00'],\n",
       " ['4_364', '4_364_16', '4_364_14', '4_364_8', '1', '00', '00'],\n",
       " ['4_364', '4_364_18', '4_364_16', '4_364_17', '1', '00', '00'],\n",
       " ['4_364', '4_364_19', '4_364_16', '4_364_17', '2', '00', '00'],\n",
       " ['4_364', '4_364_15', '4_364_14', '4_364_8', '2', '00', '00'],\n",
       " ['4_364', '4_364_3', '4_364_1', '4_364_2', '1', '00', '00'],\n",
       " ['4_364', '4_364_4', '4_364_1', '4_364_2', '2', '00', '00'],\n",
       " ['4_364', '4_364_23', '0', '0', '1', '00', '00'],\n",
       " ['4_364', '4_364_DCH23.4', '4_364_23', '4_364_4', '1', '00', '00']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.getFamSamples(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.markers = [\"V{}-{}\".format(idx, item[1]) for idx, item in enumerate(data.variants)]\n",
    "for item in data.families:\n",
    "    varnames[item], positions, vcf_mafs = data.getFamVariants(item, style = \"map\")\n",
    "    if len(varnames[item]) == 0:\n",
    "        for person in data.families[item]:\n",
    "            data[person] = self.missings\n",
    "        continue\n",
    "    if env.debug:\n",
    "        with env.lock:\n",
    "            sys.stderr.write('\\n'.join(['\\t'.join(x) for x in data.getFamSamples(item)]) + '\\n\\n')\n",
    "    # haplotyping\n",
    "    with env.lock:\n",
    "        if not env.prephased:\n",
    "            #with stdoutRedirect(to = env.tmp_log + str(os.getpid()) + '.log'):\n",
    "            #    haplotypes[item] = self.haplotyper.Execute(data.chrom, varnames[item],\n",
    "            #                                           sorted(positions), data.getFamSamples(item))[0]\n",
    "            tmp_log_output=env.tmp_log + str(os.getpid())\n",
    "            #with stdoutRedirect(to = tmp_log_output + '.log'):\n",
    "            haplotypes[item] = self.haplotyper.Execute(data.chrom, varnames[item], sorted(positions),\n",
    "                                                           data.getFamSamples(item), self.rsq, tmp_log_output)[0]\n",
    "\n",
    "        else:\n",
    "            haplotypes[item] = self.__PedToHaplotype(data.getFamSamples(item))\n",
    "    if len(haplotypes[item]) == 0:\n",
    "        # C++ haplotyping implementation failed\n",
    "        with env.chperror_counter.get_lock():\n",
    "            env.chperror_counter.value += 1\n",
    "    # either use privided MAF or computer MAF\n",
    "    if all(vcf_mafs):\n",
    "        for idx, v in enumerate(varnames[item]):\n",
    "            if v not in mafs:\n",
    "                mafs[v] = vcf_mafs[idx]\n",
    "    else:\n",
    "        # count founder alleles\n",
    "        for hap in haplotypes[item]:\n",
    "            if not data.tfam.is_founder(hap[1]):\n",
    "                continue\n",
    "            for idxv, v in enumerate(varnames[item]):\n",
    "                if v not in mafs:\n",
    "                    # [#alt, #haplotypes]\n",
    "                    mafs[v] = [0, 0]\n",
    "                gt = hap[2 + idxv][1] if hap[2 + idxv][0].isupper() else hap[2 + idxv][0]\n",
    "                if not gt == \"?\":\n",
    "                    mafs[v][0] += self.gtconv[gt]\n",
    "                    mafs[v][1] += 1.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker._MarkerMaker__Haplotype(data, haplotypes, mafs, varnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 1 units ignored due to absence in VCF file\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "    if env.triallelic_counter.value:\n",
    "        env.log('{:,d} tri-allelic loci were ignored'.format(env.triallelic_counter.value))\n",
    "    if env.commonvar_counter.value:\n",
    "        env.log('{:,d} variants ignored due to having MAF > {} and other specified constraints'.\\\n",
    "                format(env.commonvar_counter.value, args.maf_cutoff))\n",
    "    if env.null_counter.value:\n",
    "        env.log('{:,d} units ignored due to absence in VCF file'.format(env.null_counter.value))\n",
    "    if env.trivial_counter.value:\n",
    "        env.log('{:,d} units ignored due to absence of variation in samples'.format(env.trivial_counter.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cat: './seqlinkage-example/tmprst/SEQLinkage_tmp_wk92qcpq/*.*': No such file or directory\n",
      "\u001b[1;40;32mMESSAGE: Archiving regional marker data to directory [./seqlinkage-example/cache]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "fatal_errors = 0\n",
    "try:\n",
    "    # Error msg from C++ extension\n",
    "    os.system(\"cat {}/*.* > {}\".format(env.tmp_dir, env.tmp_log))\n",
    "    fatal_errors = wordCount(env.tmp_log)['fatal']\n",
    "except KeyError:\n",
    "    pass\n",
    "if env.chperror_counter.value:\n",
    "    env.error(\"{:,d} regional markers failed to be generated due to haplotyping failures!\".\\\n",
    "              format(env.chperror_counter.value))\n",
    "if fatal_errors:\n",
    "    env.error(\"{:,d} or more regional markers failed to be generated due to runtime errors!\".\\\n",
    "              format(fatal_errors))\n",
    "env.log('Archiving regional marker data to directory [{}]'.format(env.cache_dir))\n",
    "cache.write(arcroot = 'CACHE', source_dir = env.tmp_cache)\n",
    "env.jobs = args.jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./seqlinkage-example/tmprst/SEQLinkage_tmp_wk92qcpq/CACHE'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.tmp_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tsq20211129.chr1.freq', 'tsq20211129.chr1.tped', 'tsq20211129.tfam']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(env.tmp_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 2 units will be converted to MERLIN format\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 1 units successfully converted to MERLIN format\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: Archiving MERLIN format to directory [./seqlinkage-example/cache]\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: 2 units will be converted to LINKAGE format\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: 1 units successfully converted to LINKAGE format\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: Archiving LINKAGE format to directory [./seqlinkage-example/cache]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: write to PLINK or mega2 format\n",
    "tpeds = [os.path.join(env.tmp_cache, item) for item in os.listdir(env.tmp_cache) if item.startswith(env.output) and item.endswith('.tped')]\n",
    "for fmt in args.format:\n",
    "    cache.setID(fmt)\n",
    "    if not args.vanilla and cache.check():\n",
    "        env.log('Loading {} data from archive ...'.format(fmt.upper()))\n",
    "        cache.load(target_dir = env.tmp_dir, names = [fmt.upper()])\n",
    "    else:\n",
    "        env.log('{:,d} units will be converted to {} format'.format(env.success_counter.value, fmt.upper()))\n",
    "        env.format_counter.value = 0\n",
    "        format(tpeds, os.path.join(env.tmp_cache, \"{}.tfam\".format(env.output)),\n",
    "               args.prevalence, args.wild_pen, args.muta_pen, fmt,\n",
    "               args.inherit_mode, args.theta_max, args.theta_inc)\n",
    "        env.log('{:,d} units successfully converted to {} format\\n'.\\\n",
    "                format(env.format_counter.value, fmt.upper()), flush = True)\n",
    "        if env.skipped_counter.value:\n",
    "            # FIXME: perhaps we need to rephrase this message?\n",
    "            env.log('{} region - family pairs skipped'.\\\n",
    "                    format(env.skipped_counter.value))\n",
    "        env.log('Archiving {} format to directory [{}]'.format(fmt.upper(), env.cache_dir))\n",
    "        cache.write(arcroot = fmt.upper(),\n",
    "                    source_dir = os.path.join(env.tmp_dir, fmt.upper()), mode = 'a')\n",
    "mkpath(env.outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing run_linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.run_linkage = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.setID('analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LINKAGE'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      /mnt/mfs/statgen/yin/Github/linkage/SEQpy3/SEQLinkage/Utils.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?cache.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/mfs/statgen/yin/Github/linkage/SEQpy3/cache/LINKAGE.cache'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.cache_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not args.vanilla and cache.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = args.format[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/genemap.hg38.txt'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.blueprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.theta_inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.theta_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.output_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: Running linkage analysis ...\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: Linkage analysis succesfully performed for 1 units\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: 2 \"pedcheck\" runtime errors occurred\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: Report for [tsq20211129] is generated in HTML format\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if args.run_linkage:\n",
    "    cache.setID('analysis')\n",
    "    if not args.vanilla and cache.check():\n",
    "        env.log('Loading linkage analysis result from archive ...'.format(fmt.upper()))\n",
    "        cache.load(target_dir = env.output, names = ['heatmap'])\n",
    "    else:\n",
    "        env.log('Running linkage analysis ...'.format(fmt.upper()))\n",
    "        run_linkage(args.blueprint, args.theta_inc, args.theta_max, args.output_limit)\n",
    "        env.log('Linkage analysis succesfully performed for {:,d} units\\n'.\\\n",
    "                format(env.run_counter.value, fmt.upper()), flush = True)\n",
    "        if env.makeped_counter.value:\n",
    "            env.log('{} \"makeped\" runtime errors occurred'.format(env.makeped_counter.value))\n",
    "        if env.pedcheck_counter.value:\n",
    "            env.log('{} \"pedcheck\" runtime errors occurred'.format(env.pedcheck_counter.value))\n",
    "        if env.unknown_counter.value:\n",
    "            env.log('{} \"unknown\" runtime errors occurred'.format(env.unknown_counter.value))\n",
    "        if env.mlink_counter.value:\n",
    "            env.log('{} \"mlink\" runtime errors occurred'.format(env.mlink_counter.value))\n",
    "        cache.write(arcroot = 'heatmap', source_dir = os.path.join(env.output, 'heatmap'), mode = 'a')\n",
    "    html(args.theta_inc, args.theta_max, args.output_limit)\n",
    "else:\n",
    "    env.log('Saving data to [{}]'.format(os.path.abspath(env.output)))\n",
    "    cache.load(target_dir = env.output, names = [fmt.upper() for fmt in args.format])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: Saving data to [/mnt/mfs/statgen/yin/Github/linkage/SEQpy2/testseqlink]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "    env.log('Saving data to [{}]'.format(os.path.abspath(env.output)))\n",
    "    cache.load(target_dir = env.output, names = [fmt.upper() for fmt in args.format])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 3,479 samples found in [/mnt/mfs/statgen/yin/Github/linkage/SEQpy3/data/first1000snp_full_samples.vcf.gz]\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: 7 samples found in FAM file but not in VCF file:\n",
      "28_9_101, 28_9_100, 28_9_186, 1036_2, 22_1_20, 1036_1, 22_1_10\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: 3,461 samples in VCF file will be ignored due to absence in FAM file\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: Loading marker map from [data/genemap.hg38.txt] ...\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: 3 families with a total of 18 samples will be scanned for 28,325 pre-defined units\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: 0 units processed {5.65%} ...\u001b[0m                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in Haplotype"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 0 units processed {19.76%} ...\u001b[0m                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V8-13273 V12-13302 V17-13417 V22-13687 \n",
      "\n",
      "V8-13273: 0 0.707106 0.292894 \n",
      "total familyCount:1\n",
      "V12-13302: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V17-13417: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V22-13687: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "in Haplotype"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 0 units processed {14.12%} ...\u001b[0m                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in Haplotype"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 0 units processed {11.29%} ...\u001b[0m                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "running familyrunning family running family1036  \n",
      "10361036\n",
      "running familyrunning familyrunning family\n",
      " 22_1 \n",
      "22_1 running family\n",
      " running family22_1Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V8-13273 V12-13302 V17-13417 V22-13687 \n",
      "\n",
      "V8-13273: 0 0.707106 0.292894 \n",
      "total familyCount:1\n",
      "V12-13302: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V17-13417: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V22-13687: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V4-17385 V8-17407 \n",
      "\n",
      "V4-17385: 0 1 \n",
      "total familyCount:1\n",
      "V8-17407: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V8-13273 V15-13380 V17-13417 \n",
      "\n",
      "V8-13273: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V15-13380: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V17-13417: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V4-17385 V5-17398 V8-17407 \n",
      "\n",
      "V4-17385: 0 0.739454 0.260546 \n",
      "total familyCount:1\n",
      "V5-17398: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V8-17407: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "\n",
      "28_9running family\n",
      " 28_9 28_9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 3 units processed {36.70%} ...\u001b[0m                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V8-13273 V15-13380 V17-13417 \n",
      "\n",
      "V8-13273: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V15-13380: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V17-13417: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V3-17379 V4-17385 V9-17408 \n",
      "\n",
      "V3-17379: 0 0.824706 0.175294 \n",
      "total familyCount:1\n",
      "V4-17385: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V9-17408: 0 0.824706 0.175294 \n",
      "total familyCount:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 3 units processed {73.39%} ...\u001b[0m                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in Haplotype\n",
      "running family 1036\n",
      "running family 22_1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 3 units processed {79.04%} ...\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running family "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 3 units processed {81.86%} ...\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28_9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 3 units processed {81.86%} ...\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 4 units processed {93.15%} ...\u001b[0m                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V3-14464 V8-14653 V18-14907 V19-14930 V20-14933 V40-16103 V50-16378 \n",
      "   V55-16487 V69-17147 V79-17358 V85-17385 V89-17407 V124-17697 V151-17928 \n",
      "   V152-17929 V159-20184 V160-20191 V162-20212 V165-20227 V169-20235 \n",
      "   V171-20250 V178-20316 V180-20485 V182-20522 V184-20547 V194-29368 \n",
      "\n",
      "V3-14464: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V8-14653: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V18-14907: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V19-14930: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V20-14933: 0 1 \n",
      "total familyCount:1\n",
      "V40-16103: 0 0.707106 0.292894 \n",
      "total familyCount:1\n",
      "V50-16378: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V55-16487: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V69-17147: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V79-17358: 0 1 \n",
      "total familyCount:1\n",
      "V85-17385: 0 1 \n",
      "total familyCount:1\n",
      "V89-17407: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V124-17697: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V151-17928: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V152-17929: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V159-20184: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V160-20191: 0 1 \n",
      "total familyCount:1\n",
      "V162-20212: 0 1 \n",
      "total familyCount:1\n",
      "V165-20227: 0 1 \n",
      "total familyCount:1\n",
      "V169-20235: 0 1 \n",
      "total familyCount:1\n",
      "V171-20250: 0 1 \n",
      "total familyCount:1\n",
      "V178-20316: 0 1 \n",
      "total familyCount:1\n",
      "V180-20485: 0 1 \n",
      "total familyCount:1\n",
      "V182-20522: 0 1 \n",
      "total familyCount:1\n",
      "V184-20547: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V194-29368: 0 1 \n",
      "total familyCount:1\n",
      "Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V3-14464 V8-14653 V10-14677 V18-14907 V19-14930 V40-16103 V50-16378 \n",
      "   V55-16487 V69-17147 V80-17365 V85-17385 V86-17398 V89-17407 V95-17479 \n",
      "   V108-17559 V112-17589 V124-17697 V129-17722 V131-17746 V142-17829 \n",
      "   V155-19190 V159-20184 V160-20191 V162-20212 V165-20227 V169-20235 \n",
      "   V171-20250 V178-20316 V184-20547 \n",
      "\n",
      "V3-14464: 0 0.739454 0.260546 \n",
      "total familyCount:1\n",
      "V8-14653: 0 0.707103 0.292897 \n",
      "total familyCount:1\n",
      "V10-14677: 0 0.739454 0.260546 \n",
      "total familyCount:1\n",
      "V18-14907: 0 0.707103 0.292897 \n",
      "total familyCount:1\n",
      "V19-14930: 0 0.728713 0.271287 \n",
      "total familyCount:1\n",
      "V40-16103: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V50-16378: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V55-16487: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V69-17147: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V80-17365: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V85-17385: 0 0.739454 0.260546 \n",
      "total familyCount:1\n",
      "V86-17398: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V89-17407: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V95-17479: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V108-17559: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V112-17589: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V124-17697: 0 0.739454 0.260546 \n",
      "total familyCount:1\n",
      "V129-17722: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V131-17746: 0 0.739454 0.260546 \n",
      "total familyCount:1\n",
      "V142-17829: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V155-19190: 0 0.739454 0.260546 \n",
      "total familyCount:1\n",
      "V159-20184: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V160-20191: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V162-20212: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V165-20227: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V169-20235: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V171-20250: 0 0.739454 0.260546 \n",
      "total familyCount:1\n",
      "V178-20316: 0 0.739454 0.260546 \n",
      "total familyCount:1\n",
      "V184-20547: 0 0.739454 0.260546 \n",
      "total familyCount:1\n",
      "Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V3-14464 V5-14470 V8-14653 V10-14677 V18-14907 V19-14930 V34-16068 \n",
      "   V40-16103 V50-16378 V84-17379 V85-17385 V90-17408 V105-17519 V107-17556 \n",
      "   V116-17614 V155-19190 V158-20166 V160-20191 V162-20212 V165-20227 \n",
      "   V166-20227 V168-20231 V169-20235 V171-20250 V174-20254 V178-20316 \n",
      "   V183-20545 V184-20547 V194-29368 \n",
      "\n",
      "V3-14464: 0 0.646634 0.353366 \n",
      "total familyCount:1\n",
      "V5-14470: 0 0.824706 0.175294 \n",
      "total familyCount:1\n",
      "V8-14653: 0 0.452863 0.547137 \n",
      "total familyCount:1\n",
      "V10-14677: 0 0.656432 0.343568 \n",
      "total familyCount:1\n",
      "V18-14907: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V19-14930: 0 0.628665 0.371335 \n",
      "total familyCount:1\n",
      "V34-16068: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V40-16103: 0 0.628667 0.371333 \n",
      "total familyCount:1\n",
      "V50-16378: 0 0.617751 0.382249 \n",
      "total familyCount:1\n",
      "V84-17379: 0 0.824706 0.175294 \n",
      "total familyCount:1\n",
      "V85-17385: 0 0.5 0.5 \n",
      "total familyCount:1\n",
      "V90-17408: 0 0.824706 0.175294 \n",
      "total familyCount:1\n",
      "V105-17519: 0 0.824706 0.175294 \n",
      "total familyCount:1\n",
      "V107-17556: 0 0.820231 0.179769 \n",
      "total familyCount:1\n",
      "V116-17614: 0 0.824706 0.175294 \n",
      "total familyCount:1\n",
      "V155-19190: 0 0.6 0.4 \n",
      "total familyCount:1\n",
      "V158-20166: 0 0.628665 0.371335 \n",
      "total familyCount:1\n",
      "V160-20191: 0 0.628665 0.371335 \n",
      "total familyCount:1\n",
      "V162-20212: 0 0.628665 0.371335 \n",
      "total familyCount:1\n",
      "V165-20227: 0 0.656432 0.343568 \n",
      "total familyCount:1\n",
      "V166-20227: 0 0.646634 0.353366 \n",
      "total familyCount:1\n",
      "V168-20231: 0 0.811255 0.188745 \n",
      "total familyCount:1\n",
      "V169-20235: 0 0.824706 0.175294 \n",
      "total familyCount:1\n",
      "V171-20250: 0 0.628665 0.371335 \n",
      "total familyCount:1\n",
      "V174-20254: 0 0.824706 0.175294 \n",
      "total familyCount:1\n",
      "V178-20316: 0 0.628665 0.371335 \n",
      "total familyCount:1\n",
      "V183-20545: 0 0.820231 0.179769 \n",
      "total familyCount:1\n",
      "V184-20547: 0 0.617751 0.382249 \n",
      "total familyCount:1\n",
      "V194-29368: 0 0.656432 0.343568 \n",
      "total familyCount:1\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 4 units processed {99.94%} ...\u001b[0m                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V8-13273 V12-13302 V17-13417 V22-13687 \n",
      "\n",
      "V8-13273: 0 0.707106 0.292894 \n",
      "total familyCount:1\n",
      "V12-13302: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V17-13417: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "V22-13687: 0 0.75 0.25 \n",
      "total familyCount:1\n",
      "Estimating allele frequencies... [using maximum likelihood]0\n",
      "   V8-13273 V15-13380 V17-13417 \n",
      "\n",
      "V8-13273: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V15-13380: 0 0.744757 0.255243 \n",
      "total familyCount:1\n",
      "V17-13417: 0 0.744757 0.255243 \n",
      "total familyCount:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 4 units (from 256 variants) processed; 24 Mendelian inconsistencies and 116 recombination events handled\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: 28,321 units ignored due to absence in VCF file\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: Archiving regional marker data to directory [/mnt/mfs/statgen/yin/Github/linkage/SEQpy3/cache]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merlin\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;40;32mMESSAGE: 4 units will be converted to MERLIN format\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: 4 units successfully converted to MERLIN format\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: Archiving MERLIN format to directory [/mnt/mfs/statgen/yin/Github/linkage/SEQpy3/cache]\u001b[0m\n",
      "\u001b[1;40;32mMESSAGE: Saving data to [/mnt/mfs/statgen/yin/Github/linkage/SEQpy3/LINKAGE]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if args.no_save:\n",
    "    cache = NoCache()\n",
    "else:\n",
    "    cache = Cache(env.cache_dir, env.output, vars(args))\n",
    "cache.setID('vcf')\n",
    "# STEP 1: write encoded data to TPED format\n",
    "if not args.vanilla and cache.check():\n",
    "    env.log('Loading regional marker data from archive ...')\n",
    "    cache.load(target_dir = env.tmp_dir, names = ['CACHE'])\n",
    "    env.success_counter.value = sum(map(fileLinesCount, glob.glob('{}/*.tped'.format(env.tmp_cache))))\n",
    "    env.batch = 10\n",
    "else:\n",
    "    # load VCF file header\n",
    "    checkVCFBundle(args.vcf)\n",
    "    cache.clear()\n",
    "    try:\n",
    "        vs = cstatgen.VCFstream(args.vcf)\n",
    "    except Exception as e:\n",
    "        env.error(\"{}\".format(e), exit = True)\n",
    "    samples_vcf = vs.GetSampleNames()\n",
    "    if len(samples_vcf) == 0:\n",
    "        env.error(\"Fail to extract samples from [{}]\".format(args.vcf), exit = True)\n",
    "    env.log('{:,d} samples found in [{}]'.format(len(samples_vcf), args.vcf))\n",
    "    samples_not_vcf = checkSamples(samples_vcf, getColumn(args.tfam, 2))[1]\n",
    "    # load sample info\n",
    "    data = RData(samples_vcf, TFAMParser(args.tfam))\n",
    "    if len(data.families) == 0:\n",
    "        env.error('No valid family to process. ' \\\n",
    "                  'Families have to be at least trio with at least one member in VCF file.', exit = True)\n",
    "    if len(data.samples) == 0:\n",
    "        env.error('No valid sample to process. ' \\\n",
    "                  'Samples have to be in families, and present in both TFAM and VCF files.', exit = True)\n",
    "    rewriteFamfile(os.path.join(env.tmp_cache, '{}.tfam'.format(env.output)),\n",
    "                   data.tfam.samples, list(data.samples.keys()) + samples_not_vcf)\n",
    "    if args.single_markers:\n",
    "        regions = [(x[0], x[1], x[1], \"{}:{}\".format(x[0], x[1]), '.', '.', '.')\n",
    "                   for x in vs.GetGenomeCoordinates()]\n",
    "        args.blueprint = None\n",
    "    else:\n",
    "        # load blueprint\n",
    "        try:\n",
    "            env.log('Loading marker map from [{}] ...'.format(args.blueprint))\n",
    "            with open(args.blueprint, 'r') as f:\n",
    "                regions = [x.strip().split() for x in f.readlines()]\n",
    "        except IOError:\n",
    "            env.error(\"Cannot load regional marker blueprint [{}]. \".format(args.blueprint), exit = True)\n",
    "    env.log('{:,d} families with a total of {:,d} samples will be scanned for {:,d} pre-defined units'.\\\n",
    "            format(len(data.families), len(data.samples), len(regions)))\n",
    "    env.jobs = max(min(args.jobs, len(regions)), 1)\n",
    "    regions.extend([None] * env.jobs)\n",
    "    queue = Queue()\n",
    "    try:\n",
    "        faulthandler.enable(file=open(env.tmp_log + '.SEGV', 'w'))\n",
    "        for i in regions:\n",
    "            queue.put(i)\n",
    "        jobs = [EncoderWorker(\n",
    "            queue, len(regions), deepcopy(data),\n",
    "            RegionExtractor(args.vcf, chr_prefix = args.chr_prefix, allele_freq_info = args.freq),\n",
    "            MarkerMaker(args.bin, maf_cutoff = args.maf_cutoff),\n",
    "            LinkageWriter(len(samples_not_vcf))\n",
    "            ) for i in range(env.jobs)]\n",
    "        for j in jobs:\n",
    "            j.start()\n",
    "        for j in jobs:\n",
    "            j.join()\n",
    "        faulthandler.disable()\n",
    "    except KeyboardInterrupt:\n",
    "        # FIXME: need to properly close all jobs\n",
    "        raise ValueError(\"Use 'killall {}' to properly terminate all processes!\".format(env.prog))\n",
    "    else:\n",
    "        env.log('{:,d} units (from {:,d} variants) processed; '\\\n",
    "            '{:,d} Mendelian inconsistencies and {:,d} recombination events handled\\n'.\\\n",
    "            format(env.success_counter.value,\n",
    "                   env.variants_counter.value,\n",
    "                   env.mendelerror_counter.value,\n",
    "                   env.recomb_counter.value), flush = True)\n",
    "        if env.triallelic_counter.value:\n",
    "            env.log('{:,d} tri-allelic loci were ignored'.format(env.triallelic_counter.value))\n",
    "        if env.commonvar_counter.value:\n",
    "            env.log('{:,d} variants ignored due to having MAF > {}'.\\\n",
    "                    format(env.commonvar_counter.value, args.maf_cutoff))\n",
    "        if env.null_counter.value:\n",
    "            env.log('{:,d} units ignored due to absence in VCF file'.format(env.null_counter.value))\n",
    "        if env.trivial_counter.value:\n",
    "            env.log('{:,d} units ignored due to absence of variation in samples'.format(env.trivial_counter.value))\n",
    "        fatal_errors = 0\n",
    "        try:\n",
    "            # Error msg from C++ extension\n",
    "            os.system(\"cat {}/*.* > {}\".format(env.tmp_dir, env.tmp_log))\n",
    "            fatal_errors = wordCount(env.tmp_log)['fatal']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        if env.chperror_counter.value:\n",
    "            env.error(\"{:,d} regional markers failed to be generated due to haplotyping failures!\".\\\n",
    "                      format(env.chperror_counter.value))\n",
    "        if fatal_errors:\n",
    "            env.error(\"{:,d} or more regional markers failed to be generated due to runtime errors!\".\\\n",
    "                      format(fatal_errors))\n",
    "        env.log('Archiving regional marker data to directory [{}]'.format(env.cache_dir))\n",
    "        cache.write(arcroot = 'CACHE', source_dir = env.tmp_cache)\n",
    "env.jobs = args.jobs\n",
    "# STEP 2: write to PLINK or mega2 format\n",
    "tpeds = [os.path.join(env.tmp_cache, item) for item in os.listdir(env.tmp_cache) if item.startswith(env.output) and item.endswith('.tped')]\n",
    "for fmt in args.format:\n",
    "    print(fmt.lower())\n",
    "    cache.setID(fmt.lower())\n",
    "    if not args.vanilla and cache.check():\n",
    "        env.log('Loading {} data from archive ...'.format(fmt.upper()))\n",
    "        cache.load(target_dir = env.tmp_dir, names = [fmt.upper()])\n",
    "    else:\n",
    "        env.log('{:,d} units will be converted to {} format'.format(env.success_counter.value, fmt.upper()))\n",
    "        env.format_counter.value = 0\n",
    "        format(tpeds, os.path.join(env.tmp_cache, \"{}.tfam\".format(env.output)),\n",
    "               args.prevalence, args.wild_pen, args.muta_pen, fmt,\n",
    "               args.inherit_mode, args.theta_max, args.theta_inc)\n",
    "        env.log('{:,d} units successfully converted to {} format\\n'.\\\n",
    "                format(env.format_counter.value, fmt.upper()), flush = True)\n",
    "        if env.skipped_counter.value:\n",
    "            # FIXME: perhaps we need to rephrase this message?\n",
    "            env.log('{} region - family pairs skipped'.\\\n",
    "                    format(env.skipped_counter.value))\n",
    "        env.log('Archiving {} format to directory [{}]'.format(fmt.upper(), env.cache_dir))\n",
    "        cache.write(arcroot = fmt.upper(),\n",
    "                    source_dir = os.path.join(env.tmp_dir, fmt.upper()), mode = 'a')\n",
    "mkpath(env.outdir)\n",
    "if args.run_linkage:\n",
    "    cache.setID('analysis')\n",
    "    if not args.vanilla and cache.check():\n",
    "        env.log('Loading linkage analysis result from archive ...'.format(fmt.upper()))\n",
    "        cache.load(target_dir = env.output, names = ['heatmap'])\n",
    "    else:\n",
    "        env.log('Running linkage analysis ...'.format(fmt.upper()))\n",
    "        run_linkage(args.blueprint, args.theta_inc, args.theta_max, args.output_limit)\n",
    "        env.log('Linkage analysis succesfully performed for {:,d} units\\n'.\\\n",
    "                format(env.run_counter.value, fmt.upper()), flush = True)\n",
    "        if env.makeped_counter.value:\n",
    "            env.log('{} \"makeped\" runtime errors occurred'.format(env.makeped_counter.value))\n",
    "        if env.pedcheck_counter.value:\n",
    "            env.log('{} \"pedcheck\" runtime errors occurred'.format(env.pedcheck_counter.value))\n",
    "        if env.unknown_counter.value:\n",
    "            env.log('{} \"unknown\" runtime errors occurred'.format(env.unknown_counter.value))\n",
    "        if env.mlink_counter.value:\n",
    "            env.log('{} \"mlink\" runtime errors occurred'.format(env.mlink_counter.value))\n",
    "        cache.write(arcroot = 'heatmap', source_dir = os.path.join(env.output, 'heatmap'), mode = 'a')\n",
    "    html(args.theta_inc, args.theta_max, args.output_limit)\n",
    "else:\n",
    "    env.log('Saving data to [{}]'.format(os.path.abspath(env.output)))\n",
    "    cache.load(target_dir = env.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.run_linkage = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = ['1', '69090', '70008', 'OR4F5', '4.866641545668504e-06', '6.181823219621424e-06', '3.6135725636621673e-06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.getRegion(region)\n",
    "maker.getRegion(region)\n",
    "writer.getRegion(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes = OrderedDict()\n",
    "mafs = {}   ##Per fam per variant\n",
    "uniq_vars = []\n",
    "exclude_vars = []\n",
    "varnames = {}\n",
    "recombPos = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.apply(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker._MarkerMaker__Haplotype(data, haplotypes, mafs, varnames,recombPos,uniq_vars,exclude_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes['668']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker._MarkerMaker__ClusterByLD(data, haplotypes, varnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker._MarkerMaker__CodeHaplotypes(data, haplotypes, mafs, varnames, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = []\n",
    "if clusters is not None:\n",
    "    clusters_idx = [[[varnames[item].index(x) for x in y] for y in clusters] for item in haplotypes]\n",
    "else:\n",
    "    clusters_idx = [[[]] for item in haplotypes]\n",
    "maker.coder.Execute(haplotypes.values(), [[mafs[v] for v in varnames[item]] for item in haplotypes], clusters_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.coder.Print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[mafs[item][v] for v in varnames[item]] for item in haplotypes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames['668']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mafs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test clusterbyld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.apply(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes = OrderedDict()\n",
    "mafs = {}\n",
    "varnames = {}\n",
    "maker._MarkerMaker__Haplotype(data, haplotypes, mafs, varnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = [\"V{}-{}\".format(idx, item[1]) for idx, item in enumerate(data.variants)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = list(data.families.keys())[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item ='1036'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames[item], positions, vcf_mafs = data.getFamVariants(item, style = \"map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.haplotyper.Execute(data.chrom, varnames[item], sorted(positions), data.getFamSamples(item))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recombPos={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames[item], positions, vcf_mafs = data.getFamVariants(item, style = \"map\")\n",
    "recombPos[item]={}\n",
    "var_for_haplotype=[]\n",
    "positions_for_haplotype=[]\n",
    "output_sample=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.gnomAD_estimate.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_for_haplotype=varnames[item]\n",
    "positions_for_haplotype=positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "famid =item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_names = []\n",
    "S_no_parents = filter(lambda x: True if data.tfam.is_founder(x) else False, data.tfam.families[famid])\n",
    "graph = data.tfam.graph[famid].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(S_no_parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tfam.families[famid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(S_no_parents):\n",
    "    n = S_no_parents.pop()\n",
    "    sorted_names.append(n)\n",
    "    if n not in graph:\n",
    "        continue\n",
    "    offsprings = graph.pop(n)\n",
    "    for m in offsprings:\n",
    "        father, mother = data.tfam.get_parents(m)\n",
    "        if father not in graph and mother not in graph:\n",
    "            S_no_parents.append(m)\n",
    "if graph:\n",
    "    raise ValueError(\"There is a loop in the pedigree: {}\\n\".format(' '.join(graph.keys())))\n",
    "else:\n",
    "    return sorted_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tfam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect sample+genotypes\n",
    "for person in data.tfam.sort_family(item):\n",
    "    output_sample.append([])\n",
    "    last_ele=len(output_sample)-1\n",
    "    output_sample[last_ele] = data.tfam.samples[person][:-1]\n",
    "    if person in data.samples:\n",
    "        for marker in var_for_haplotype:\n",
    "            idx=int(marker.split('-')[0][1:])\n",
    "            output_sample[last_ele].append(data.genotype_all[person][idx])\n",
    "    else:\n",
    "        output_sample[last_ele].extend([\"00\"] * len(var_for_haplotype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.tfam.sort_family(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(output_sample[0][5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.tmp_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_log_output=env.tmp_log + str(os.getpid())\n",
    "haplotypes[item] = maker.haplotyper.Execute(data.chrom, var_for_haplotype, positions_for_haplotype, output_sample, maker.rsq, tmp_log_output)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_for_haplotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions_for_haplotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_for_haplotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions_for_haplotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.rsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for item in haplotypes:\n",
    "for hap_idx,haploid in enumerate(haplotypes[item]):\n",
    "    for vidx,var in enumerate(haploid[2:]):\n",
    "        if not var.endswith(':') and not var.endswith('|') and vidx!=0:\n",
    "            postvar_name=varnames[item][vidx]\n",
    "            prevar_name=varnames[item][vidx-1]\n",
    "            recomb_pair = (prevar_name,postvar_name)\n",
    "            print('run this')\n",
    "            try:\n",
    "                recombPos[item][recomb_pair].append(hap_idx)\n",
    "            except:\n",
    "                recombPos[item][recomb_pair]=[hap_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haploid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recombPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.rsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mafs = {}\n",
    "# either use privided MAF or computer MAF\n",
    "if all(vcf_mafs):\n",
    "    print('run this')\n",
    "    for idx, v in enumerate(varnames[item]):\n",
    "        if v not in mafs:\n",
    "            mafs[v] = vcf_mafs[idx]\n",
    "else:\n",
    "    # count founder alleles\n",
    "    for hap in haplotypes[item]:\n",
    "        if not data.tfam.is_founder(hap[1]):\n",
    "            continue\n",
    "        for idxv, v in enumerate(varnames[item]):\n",
    "            if v not in mafs:\n",
    "                # [#alt, #haplotypes]\n",
    "                mafs[v] = [0, 0]\n",
    "            gt = hap[2 + idxv][1] if hap[2 + idxv][0].isupper() else hap[2 + idxv][0]\n",
    "            if not gt == \"?\":\n",
    "                mafs[v][0] += self.gtconv[gt]\n",
    "                mafs[v][1] += 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_mafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mafs['V0-176659933'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.maf_cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_vars = []\n",
    "for v in mafs.keys():\n",
    "    if mafs[v] > maker.maf_cutoff:\n",
    "        exclude_vars.append(v)\n",
    "for i in haplotypes.keys():\n",
    "    haplotypes[i] = listit(haplotypes[i])\n",
    "    for j in range(len(haplotypes[i])):\n",
    "        haplotypes[i][j] = haplotypes[i][j][:2] + \\\n",
    "          [x for idx, x in enumerate(haplotypes[i][j][2:]) if varnames[i][idx] not in exclude_vars]\n",
    "    varnames[i] = [x for x in varnames[i] if x not in exclude_vars]\n",
    "    # handle trivial data\n",
    "    if len(varnames[i]) == 0:\n",
    "        for person in data.families[i]:\n",
    "            data[person] = self.missings\n",
    "        del varnames[i]\n",
    "        del haplotypes[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_exclude_vars=exclude_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_exclude_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recombPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_vars = []\n",
    "i = '1'\n",
    "for tmp_var in varnames[i]:\n",
    "    if tmp_var not in uniq_vars:\n",
    "             uniq_vars.append(tmp_var)\n",
    "varnames[i] = [x for x in varnames[i] if x not in tmp_exclude_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.genotype_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(varnames):\n",
    "    if not any ([len(varnames[x]) - 1 for x in varnames]):\n",
    "        # all families have only one variant\n",
    "        maker._MarkerMaker__AssignSNVHaplotypes(data, haplotypes, mafs, varnames)\n",
    "    else:\n",
    "        print('run this')\n",
    "        # calculate LD clusters using founder haplotypes\n",
    "        clusters = maker._MarkerMaker__ClusterByLD(data, haplotypes, varnames)\n",
    "        # recoding the genotype of the region\n",
    "        maker._MarkerMaker__CodeHaplotypes(data, haplotypes, mafs, varnames, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def __ClusterByLD(self, data, haplotypes, varnames):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('1',\n",
       "              [['1', 'I:1', '1:', '1:', '1:', '1:', '1:', '1:', '1:'],\n",
       "               ['1', 'I:1', '2:', '2:', '1:', '1:', '1:', '1:', '2:'],\n",
       "               ['1', 'I:2', '2:', '2:', '1:', '1:', '1:', '2:', '2:'],\n",
       "               ['1', 'I:2', '1:', '1:', '1:', '1:', '1:', '1:', '1:'],\n",
       "               ['1', 'II:3', '2:', '2|', '1:', '1:', '1:', '2|', '2|'],\n",
       "               ['1', 'II:3', '1:', '1|', '1:', '1:', '1:', '1|', '1|'],\n",
       "               ['1', 'II:2', '2:', '2|', '1:', '1:', '1:', '2|', '2|'],\n",
       "               ['1', 'II:2', '1:', '1|', '1:', '1:', '1:', '1|', '1|'],\n",
       "               ['1', 'II:1', '2:', '2|', '1:', '1:', '1:', '2|', '2|'],\n",
       "               ['1', 'II:1', '2:', '2|', '1:', '1:', '1:', '1|', '2|'],\n",
       "               ['1', 'II:4', '2:', '2|', '1:', '1:', '1:', '2\\\\', '2|'],\n",
       "               ['1', 'II:4', '1:', '1|', '1:', '1:', '1:', '1\\\\', '2|']]),\n",
       "             ('2',\n",
       "              [['2', 'I:A', '1:', '1:', '1:', '1:', '1:', '1:', '2:'],\n",
       "               ['2', 'I:A', '1:', '1:', '1:', '1:', '1:', '1:', '2:'],\n",
       "               ['2', 'I:B', '2:', '2:', '2:', '2:', '2:', '2:', '1:'],\n",
       "               ['2', 'I:B', '1:', '1:', '1:', '1:', '1:', '1:', '1:'],\n",
       "               ['2', 'II:D', '2:', '2|', '2|', '2|', '2|', '2|', '1:'],\n",
       "               ['2', 'II:D', '1:', '1|', '1|', '1|', '1|', '1|', '2:'],\n",
       "               ['2', 'II:C', '1:', '1|', '1|', '1|', '1|', '1|', '1:'],\n",
       "               ['2', 'II:C', '1:', '1|', '1|', '1|', '1|', '1|', '2:'],\n",
       "               ['2', 'II:B', '1:', '1|', '1|', '1|', '1|', '1|', '1:'],\n",
       "               ['2', 'II:B', '1:', '1|', '1|', '1|', '1|', '1|', '2:'],\n",
       "               ['2', 'II:A', '2:', '2|', '2|', '2|', '2|', '2|', '1:'],\n",
       "               ['2', 'II:A', '1:', '1|', '1|', '1|', '1|', '1|', '2:']])])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haplotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get founder haplotypes\n",
    "gtt = []\n",
    "founder_haplotypes = []\n",
    "markers = sorted(set(itertools.chain(*varnames.values())), key = lambda x: int(x.split(\"-\")[0][1:]))\n",
    "for item in haplotypes:\n",
    "    for ihap, hap in enumerate(haplotypes[item]):\n",
    "        if not data.tfam.is_founder(hap[1]):\n",
    "            continue\n",
    "        gt = [hap[2 + varnames[item].index(v)] if v in varnames[item] else '?' for v in markers]\n",
    "        founder_haplotypes.append((\"{}-{}\".format(hap[1], ihap % 2), \"\".join([x[1] if x[0].isupper() else x[0] for x in gt])))\n",
    "        gtt.append([\"{}-{}\".format(hap[1], ihap % 2), [x[1] if x[0].isupper() else x[0] for x in gt]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I:1-0', '1111111'),\n",
       " ('I:1-1', '2211112'),\n",
       " ('I:2-0', '2211122'),\n",
       " ('I:2-1', '1111111'),\n",
       " ('I:A-0', '1111112'),\n",
       " ('I:A-1', '1111112'),\n",
       " ('I:B-0', '2222221'),\n",
       " ('I:B-1', '1111111')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "founder_haplotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0],\n",
       " [[None],\n",
       "  [1.0, None],\n",
       "  [0.2380952380952381, 0.2380952380952381, None],\n",
       "  [0.2380952380952381, 0.2380952380952381, 1.0, None],\n",
       "  [0.2380952380952381, 0.2380952380952381, 1.0, 1.0, None],\n",
       "  [0.5555555555555556,\n",
       "   0.5555555555555556,\n",
       "   0.42857142857142866,\n",
       "   0.42857142857142866,\n",
       "   0.42857142857142866,\n",
       "   None],\n",
       "  [0.06666666666666665,\n",
       "   0.06666666666666665,\n",
       "   0.14285714285714282,\n",
       "   0.14285714285714282,\n",
       "   0.14285714285714282,\n",
       "   0.0,\n",
       "   None]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "egglib.stats.matrix_LD(Align.create(founder_haplotypes,egglib.Alphabet(cat='string',expl=['1','2'],miss='?')),('rsq'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I:1-0', ['1', '1', '1', '1', '1', '1', '1']],\n",
       " ['I:1-1', ['2', '2', '1', '1', '1', '1', '2']],\n",
       " ['I:2-0', ['2', '2', '1', '1', '1', '2', '2']],\n",
       " ['I:2-1', ['1', '1', '1', '1', '1', '1', '1']],\n",
       " ['I:A-0', ['1', '1', '1', '1', '1', '1', '2']],\n",
       " ['I:A-1', ['1', '1', '1', '1', '1', '1', '2']],\n",
       " ['I:B-0', ['2', '2', '2', '2', '2', '2', '1']],\n",
       " ['I:B-1', ['1', '1', '1', '1', '1', '1', '1']]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import egglib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1:', '1:', '1:', '1:', '1:', '1:', '1:']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldi,ld = egglib.stats.matrix_LD(Align.create(gtt,egglib.Alphabet(cat='char',expl=['1','2'],miss='?')),('rsq'))\n",
    "blocks = []\n",
    "for j in range(len(ldi)):\n",
    "    block = [j]\n",
    "    for k in range(j+1,len(ldi)):\n",
    "        if ld[k][j] > maker.r2:\n",
    "            block.append(k)\n",
    "    if len(block) > 1:\n",
    "        blocks.append(block)\n",
    "clusters = [[markers[idx] for idx in item] for item in list(connected_components(blocks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['V0-89984370', 'V1-89984604'], ['V2-89984739', 'V3-89985940', 'V4-89986608']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': ['V0-89984370',\n",
       "  'V1-89984604',\n",
       "  'V2-89984739',\n",
       "  'V3-89985940',\n",
       "  'V4-89986608',\n",
       "  'V5-89986760',\n",
       "  'V6-89987201'],\n",
       " '2': ['V0-89984370',\n",
       "  'V1-89984604',\n",
       "  'V2-89984739',\n",
       "  'V3-89985940',\n",
       "  'V4-89986608',\n",
       "  'V5-89986760',\n",
       "  'V6-89987201']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate LD blocks, use r2 measure\n",
    "ld = Align.create(founder_haplotypes).matrixLD(validCharacters=\"12\")[\"r2\"]\n",
    "blocks = []\n",
    "for j in ld:\n",
    "    block = [j]\n",
    "    for k in ld[j]:\n",
    "        if ld[j][k] > maker.r2:\n",
    "            block.append(k)\n",
    "    if len(block) > 1:\n",
    "        blocks.append(block)\n",
    "# get LD clusters\n",
    "clusters = [[markers[idx] for idx in item] for item in list(connected_components(blocks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(connected_components([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(connected_components(blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def __CodeHaplotypes(self, data, haplotypes, mafs, varnames, clusters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply CHP coding\n",
    "if clusters is not None:\n",
    "    clusters_idx = [[[varnames[item].index(x) for x in y] for y in clusters] for item in haplotypes]\n",
    "else:\n",
    "    clusters_idx = [[[]] for item in haplotypes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.coder.Execute(haplotypes.values(), [[mafs[v] for v in varnames[item]] for item in haplotypes], clusters_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.superMarkerCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line: [fid, sid, hap1, hap2]\n",
    "for line in maker.coder.GetHaplotypes():\n",
    "    print(line)\n",
    "    if not line[1] in data:\n",
    "        # this sample is not in VCF file. Every variant site should be missing\n",
    "        # they have to be skipped for now\n",
    "        continue\n",
    "    data[line[1]] = (line[2].split(','), line[3].split(','))\n",
    "    if len(data[line[1]][0]) > data.superMarkerCount:\n",
    "        data.superMarkerCount = len(data[line[1]][0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.superMarkerCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get MAF\n",
    "for item in haplotypes:\n",
    "    data.maf[item] = maker.coder.GetAlleleFrequencies(item)\n",
    "    data.maf[item] = tuple(tuple(np.array(v) / np.sum(v)) if np.sum(v) else v\n",
    "                      for v in data.maf[item])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.maf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker._MarkerMaker__FormatHaplotypes(data,recombPos,varnames,uniq_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data['I:1'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # apply CHP coding\n",
    "    for item in data.famvaridx:\n",
    "        if item not in haplotypes and data[data.families[item][0]] != ('0','0'):\n",
    "            # when only wild-type haplotypes are present in a family, still code them instead of ignoring the family\n",
    "            if self.freq_by_fam:\n",
    "                pop=data.freq_by_fam[item]\n",
    "                try:\n",
    "                    varnames[item]=data.total_varnames[pop]\n",
    "                    mafs[item]=data.total_mafs[pop]\n",
    "                except:\n",
    "                    continue\n",
    "            else:\n",
    "                varnames[item]=data.total_varnames['pop']\n",
    "                mafs[item]=data.total_mafs\n",
    "            haplotypes[item]=[]\n",
    "            for person in data.families[item]:\n",
    "                tmp_person=[item, person]\n",
    "                if '00' in data[person]:\n",
    "                    tmp_person+=['?:']*len(varnames[item])\n",
    "                else:\n",
    "                    tmp_person+=['1:']*len(varnames[item])\n",
    "                haplotypes[item].append(tmp_person)\n",
    "                haplotypes[item].append(tmp_person)\n",
    "        elif item in haplotypes:\n",
    "            nonvar_hap_flag=False\n",
    "            #determine if wild-type haplotype is present in a family\n",
    "            for hap in haplotypes[item]:\n",
    "                tmp_genes=[]\n",
    "                for tmpa in hap[2:]:\n",
    "                    if 'A' in tmpa or 'B' in tmpa:\n",
    "                        tmp_genes.append(tmpa[1])\n",
    "                    else:\n",
    "                        tmp_genes.append(tmpa[0])\n",
    "                if set(tmp_genes)==set(['1']):\n",
    "                    #non variant haplotype\n",
    "                    nonvar_hap_flag=True\n",
    "                    break\n",
    "            if not nonvar_hap_flag:\n",
    "                #if family don't have wild-type haplotype, add a fake one to ensure correct coding\n",
    "                var_num=len(varnames[item])\n",
    "                fake_person=[item, 'FAKEPERSON']+['1:']*var_num\n",
    "                haplotypes[item].append(fake_person)\n",
    "            for hidx,hap in enumerate(haplotypes[item]):\n",
    "                if hap[1] in data.missing_persons:\n",
    "                    missing_person=[item,hap[1]]+['?:']*len(varnames[item])\n",
    "                    haplotypes[item][hidx]=missing_person\n",
    "\n",
    "    if not clusters is None:\n",
    "        clusters_idx = [[[varnames[item].index(x) for x in y] for y in clusters] for item in haplotypes]\n",
    "    else:\n",
    "        clusters_idx = [[[]] for item in haplotypes]\n",
    "    if env.debug:\n",
    "        for item in haplotypes:\n",
    "            with env.lock:\n",
    "                print(varnames[item],file=sys.stderr)\n",
    "                print(\"hap{0}\\t{1}\\n\".format(item,haplotypes[item]),file=sys.stderr)\n",
    "    self.coder.Execute(haplotypes.values(), [[mafs[item][v] for v in varnames[item]] for item in haplotypes], clusters_idx)\n",
    "    if env.debug:\n",
    "        with env.lock:\n",
    "            if clusters:\n",
    "                print(\"Family LD clusters: \", clusters_idx, \"\\n\", file = sys.stderr)\n",
    "            self.coder.Print()\n",
    "    # line: [fid, sid, hap1, hap2]\n",
    "    for line in self.coder.GetHaplotypes():\n",
    "        if not line[1] in data:\n",
    "            # this sample is not in VCF file. Every variant site should be missing\n",
    "            # they have to be skipped for now\n",
    "            continue\n",
    "        data[line[1]] = (line[2].split(','), line[4].split(','))\n",
    "        #sub-region count for each sample individual\n",
    "        superMarkerCount=len(data[line[1]][0])\n",
    "        if line[0] not in data.patterns:\n",
    "            data.patterns[line[0]]=[[] for x in range(superMarkerCount)]\n",
    "        for t_Marker in range(superMarkerCount):\n",
    "            t_pat1=line[3].split(',')[t_Marker]\n",
    "            t_pat2=line[5].split(',')[t_Marker]\n",
    "            if t_pat1 not in data.patterns[line[0]][t_Marker]:\n",
    "                data.patterns[line[0]][t_Marker].append(t_pat1)\n",
    "            if t_pat2 not in data.patterns[line[0]][t_Marker]:\n",
    "                data.patterns[line[0]][t_Marker].append(t_pat2)\n",
    "        if len(data[line[1]][0]) > data.superMarkerCount:\n",
    "            data.superMarkerCount = len(data[line[1]][0])\n",
    "    # get MAF\n",
    "    for item in data.famvaridx:\n",
    "        if item not in haplotypes:\n",
    "            for person in data.families[item]:\n",
    "                data[person]=(['0']*data.superMarkerCount,['0']*data.superMarkerCount)\n",
    "    for item in haplotypes:\n",
    "        data.maf[item] = self.coder.GetAlleleFrequencies(item)\n",
    "        if not len(data.maf[item][0]):\n",
    "            continue\n",
    "        data.varnames_by_fam[item]=varnames[item]\n",
    "        wt_maf=0\n",
    "        if self.freq_by_fam:\n",
    "            try:\n",
    "                wt_maf=data.wt_maf[data.freq_by_fam[item]]\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            wt_maf=data.wt_maf['pop']\n",
    "        tmp_data_maf=[]\n",
    "        for v in data.maf[item]:\n",
    "            if len(v)==1:\n",
    "                tmp_data_maf.append((v[0],1-v[0]))\n",
    "            else:\n",
    "                if np.sum(v)<1:\n",
    "                    tmp_ratio=sum(v[1:])/(1-wt_maf)\n",
    "                    tmp_list=[wt_maf]\n",
    "                    if tmp_ratio==0:\n",
    "                        tmp_list.append(1-wt_maf)\n",
    "                    else:\n",
    "                        for tmpv in v[1:]:\n",
    "                            tmp_list.append(tmpv/tmp_ratio)\n",
    "                    tmp_data_maf.append(tuple(tmp_list))\n",
    "                else:\n",
    "                    tmp_data_maf.append(v)\n",
    "        data.maf[item]=tuple(tmp_data_maf)\n",
    "    if env.debug:\n",
    "        with env.lock:\n",
    "            print(\"marker freqs = \", data.maf, \"\\n\", file = sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data.families:\n",
    "    varnames[item], positions, vcf_mafs = data.getFamVariants(item, style = \"map\")\n",
    "    if len(varnames[item]) == 0:\n",
    "        for person in data.families[item]:\n",
    "            data[person] = self.missings\n",
    "        continue\n",
    "    if env.debug:\n",
    "        with env.lock:\n",
    "            sys.stderr.write('\\n'.join(['\\t'.join(x) for x in data.getFamSamples(item)]) + '\\n\\n')\n",
    "    # haplotyping\n",
    "    with env.lock:\n",
    "        if not env.prephased:\n",
    "            with stdoutRedirect(to = env.tmp_log + str(os.getpid()) + '.log'):\n",
    "                haplotypes[item] = self.haplotyper.Execute(data.chrom, varnames[item],\n",
    "                                                       sorted(positions), data.getFamSamples(item))[0]\n",
    "        else:\n",
    "            haplotypes[item] = self.__PedToHaplotype(data.getFamSamples(item))\n",
    "    if len(haplotypes[item]) == 0:\n",
    "        # C++ haplotyping implementation failed\n",
    "        with env.chperror_counter.get_lock():\n",
    "            env.chperror_counter.value += 1\n",
    "    # either use privided MAF or computer MAF\n",
    "    if all(vcf_mafs):\n",
    "        for idx, v in enumerate(varnames[item]):\n",
    "            if v not in mafs:\n",
    "                mafs[v] = vcf_mafs[idx]\n",
    "    else:\n",
    "        # count founder alleles\n",
    "        for hap in haplotypes[item]:\n",
    "            if not data.tfam.is_founder(hap[1]):\n",
    "                continue\n",
    "            for idxv, v in enumerate(varnames[item]):\n",
    "                if v not in mafs:\n",
    "                    # [#alt, #haplotypes]\n",
    "                    mafs[v] = [0, 0]\n",
    "                gt = hap[2 + idxv][1] if hap[2 + idxv][0].isupper() else hap[2 + idxv][0]\n",
    "                if not gt == \"?\":\n",
    "                    mafs[v][0] += self.gtconv[gt]\n",
    "                    mafs[v][1] += 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maker.haplotyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = []\n",
    "for _ in range(10):\n",
    "    a = queue.get()\n",
    "    print(a)\n",
    "    tmp.getRegion(a)\n",
    "    tmp.apply(dd)\n",
    "    tmp1.apply(dd)\n",
    "    #tmp2.apply(dd)\n",
    "    if len(dd.variants) != 0:\n",
    "        aa.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = deepcopy(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _MarkerMaker__Haplotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes = OrderedDict()\n",
    "mafs = {}   ##Per fam per variant\n",
    "uniq_vars = []\n",
    "exclude_vars = []\n",
    "varnames = {}\n",
    "recombPos = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.markers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def __Haplotype(self, data, haplotypes, mafs, varnames,recombPos,uniq_vars,exclude_vars):\n",
    "'''genetic haplotyping. haplotypes stores per family data'''\n",
    "# FIXME: it is SWIG's (2.0.12) fault not to properly destroy the object \"Pedigree\" in \"Execute()\"\n",
    "# So there is a memory leak here which I tried to partially handle on C++\n",
    "#\n",
    "# Per family haplotyping\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.markers = [\"V{}-{}\".format(idx, item[1]) for idx, item in enumerate(data.variants)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_mafs = {}\n",
    "if tmp1.freq_by_fam:\n",
    "    ## if families are from different populations\n",
    "    ## estimate MAF by different population\n",
    "    fam_to_analyze={}\n",
    "    for fam,pop in data.freq_by_fam.iteritems():\n",
    "        if pop not in fam_to_analyze:\n",
    "            fam_to_analyze[pop]=[fam]\n",
    "        else:\n",
    "            fam_to_analyze[pop].append(fam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tmp1.count:\n",
    "    ## estimate MAF by counting founder alleles\n",
    "    if tmp1.freq_by_fam:\n",
    "        local_count_mafs={}\n",
    "        for pop in fam_to_analyze:\n",
    "            local_count_mafs[pop]=tmp1._MarkerMaker__computefounderfreq(data,fam_to_analyze[pop])\n",
    "    else:\n",
    "        local_count_mafs=tmp1._MarkerMaker__computefounderfreq(data,data.families.keys())\n",
    "        print('run here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_count_mafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.mle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tmp1.mle:\n",
    "    ## estimate MLE allele frequency using all fam\n",
    "    local_mle_mafs={}\n",
    "    if tmp1.freq_by_fam:\n",
    "        for pop in fam_to_analyze:\n",
    "            local_mle_mafs[pop]={}\n",
    "            markers_to_analyze=[]\n",
    "            pos_all=[]\n",
    "            markers_analyzed={}\n",
    "            if pop not in data.mle_mafs:\n",
    "                data.mle_mafs[pop]={}\n",
    "            else:\n",
    "                for tmpv in data.mle_mafs[pop]:\n",
    "                    markers_analyzed[tmpv.split('-')[-1]]=data.mle_mafs[pop][tmpv]\n",
    "            output_log=env.tmp_log+\"AF_{}_{}.log\".format(pop,tmp1.name)\n",
    "            popidx=tmp1.af_info.index(pop)\n",
    "            variants_in_fams=[]\n",
    "            for item in fam_to_analyze[pop]:\n",
    "                for tmpvar in data.getFamVariants(item):\n",
    "                    if tmpvar not in variants_in_fams:\n",
    "                        variants_in_fams.append(tmpvar)\n",
    "            variants_in_fams=sorted(variants_in_fams, key=lambda x: x[1])\n",
    "            for item in variants_in_fams:\n",
    "                idx=data.variants.index(item)\n",
    "                if item[-1][popidx]==0:\n",
    "                    if str(item[1]) in markers_analyzed.keys():\n",
    "                        #if variant has been analyzed\n",
    "                        vname=\"V{}-{}\".format(idx,item[1])\n",
    "                        local_mle_mafs[pop][vname]=markers_analyzed[str(item[1])]\n",
    "                    else:\n",
    "                        #variant not analyzed before\n",
    "                        markers_to_analyze.append(\"V{}-{}\".format(idx,item[1]))\n",
    "                        pos_all.append(item[1])\n",
    "            tmp_mle_mafs=tmp1._MarkerMaker__getMLEfreq(data, markers_to_analyze, pos_all, fam_to_analyze[pop], tmp1.rsq, output_log)\n",
    "            if len(tmp_mle_mafs) > 0:\n",
    "                for vname,vmaf in tmp_mle_mafs.iteritems():\n",
    "                    data.mle_mafs[pop][vname]=vmaf\n",
    "                    local_mle_mafs[pop][vname]=vmaf\n",
    "    else:\n",
    "        #Homogeneous families\n",
    "        markers_to_analyze=[]\n",
    "        pos_all=[]\n",
    "        markers_analyzed={}\n",
    "        for tmpv in data.mle_mafs:\n",
    "            markers_analyzed[tmpv.split('-')[-1]]=data.mle_mafs[tmpv]\n",
    "        variants_in_fams=[]\n",
    "        for item in data.families.keys():\n",
    "            var_per_fam=[tuple(tmpvar) for tmpvar in data.getFamVariants(item)]\n",
    "            variants_in_fams=list(set(var_per_fam+variants_in_fams))\n",
    "        variants_in_fams=[list(tmpvar) for tmpvar in sorted(variants_in_fams, key=lambda x: x[1])]\n",
    "        for item in variants_in_fams:\n",
    "            idx=data.variants.index(item)\n",
    "            if item[-1]==0 or tmp1.af_info is None:\n",
    "                if str(item[1]) in markers_analyzed.keys():\n",
    "                    #if variant has been analyzed\n",
    "                    vname=\"V{}-{}\".format(idx,item[1])\n",
    "                    local_mle_mafs[vname]=markers_analyzed[str(item[1])]\n",
    "                else:\n",
    "                    #variant not analyzed before\n",
    "                    markers_to_analyze.append(\"V{}-{}\".format(idx,item[1]))\n",
    "                    pos_all.append(item[1])\n",
    "        output_log=env.tmp_log+\"AF_{}.log\".format(tmp1.name)\n",
    "        tmp_mle_mafs=tmp1._MarkerMaker__getMLEfreq(data, markers_to_analyze, pos_all, data.families.keys(), tmp1.rsq, output_log)\n",
    "        if len(tmp_mle_mafs) > 0:\n",
    "            for vname, vmaf in tmp_mle_mafs.iteritems():\n",
    "                data.mle_mafs[vname]=vmaf\n",
    "                local_mle_mafs[vname]=vmaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_mle_mafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.families.keys()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.getFamVariants(data.families.keys()[1],style=\"map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.famvaridx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.famsampidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnomAD_pop=None\n",
    "for item in data.families:\n",
    "    varnames[item], positions, vcf_mafs = data.getFamVariants(item, style = \"map\")\n",
    "    recombPos[item]={}\n",
    "    var_for_haplotype=[]\n",
    "    positions_for_haplotype=[]\n",
    "    output_sample=[]\n",
    "    if env.debug:\n",
    "        with env.lock:\n",
    "            sys.stderr.write('\\n'+repr(varnames[item])+'\\n')\n",
    "            sys.stderr.write('\\n'.join(['\\t'.join(x) for x in data.getFamSamples(item)]) + '\\n\\n')\n",
    "    # either use privided MAF or compute MAF\n",
    "    if tmp1.freq_by_fam:\n",
    "        mafs[item]={}\n",
    "        tfreq_fam=data.freq_by_fam[item]\n",
    "        for pop in data.gnomAD_estimate.keys():\n",
    "            if pop in tfreq_fam:\n",
    "                gnomAD_pop=pop\n",
    "                break\n",
    "    elif gnomAD_pop is None and data.freq is not None:\n",
    "        for pop in data.gnomAD_estimate.keys():\n",
    "            if pop in data.freq:\n",
    "                gnomAD_pop=pop\n",
    "                break\n",
    "    for idx, v in enumerate(varnames[item]):\n",
    "        tmp_maf_var=0\n",
    "        if tmp1.af_info is None:\n",
    "        #no vcf freq column specified\n",
    "            if v not in tmp_mafs:\n",
    "                if tmp1.mle:\n",
    "                #use MLE freq for all variants\n",
    "                    tmp_mafs[v]=local_mle_mafs[v]\n",
    "                elif tmp1.count:\n",
    "                #estimate MAF based on founder counts if MLE not specified\n",
    "                    tmp_mafs[v]=local_count_mafs[v]\n",
    "                tmp_maf_var=tmp_mafs[v]\n",
    "        elif not tmp1.af_info is None:\n",
    "            #if vcf freq column is specified\n",
    "            #use vcf_mafs if possible\n",
    "            if vcf_mafs[idx]:\n",
    "                tmp_maf_var=vcf_mafs[idx]\n",
    "                if tmp1.freq_by_fam:\n",
    "                    mafs[item][v] = vcf_mafs[idx]\n",
    "                else:\n",
    "                    if v not in tmp_mafs:\n",
    "                        tmp_mafs[v] = vcf_mafs[idx]\n",
    "            else:\n",
    "                #if variants do not have valid vcf_mafs values if specified\n",
    "                if tmp1.freq_by_fam:\n",
    "                    if gnomAD_pop is not None:\n",
    "                        mafs[item][v]=data.gnomAD_estimate[gnomAD_pop]\n",
    "                    elif tmp1.mle:\n",
    "                            mafs[item][v]=local_mle_mafs[data.freq_by_fam[item]][v]\n",
    "                    elif tmp1.count:\n",
    "                            mafs[item][v]=local_count_mafs[data.freq_by_fam[item]][v]\n",
    "                    tmp_maf_var=mafs[item][v]\n",
    "                else:\n",
    "                    if v not in tmp_mafs:\n",
    "                        if gnomAD_pop is not None:\n",
    "                            tmp_mafs[v]=data.gnomAD_estimate[gnomAD_pop]\n",
    "                        elif tmp1.mle:\n",
    "                            tmp_mafs[v]=local_mle_mafs[v]\n",
    "                        elif tmp1.count:\n",
    "                            tmp_mafs[v]=local_count_mafs[v]\n",
    "                    tmp_maf_var=tmp_mafs[v]\n",
    "        if tmp1.rvhaplo:\n",
    "            if tmp_maf_var<=tmp1.maf_cutoff:\n",
    "                var_for_haplotype.append(v)\n",
    "                positions_for_haplotype.append(positions[idx])\n",
    "    if not tmp1.rvhaplo:\n",
    "        var_for_haplotype=varnames[item]\n",
    "        positions_for_haplotype=positions\n",
    "    #collect sample+genotypes\n",
    "    for person in data.tfam.sort_family(item):\n",
    "        output_sample.append([])\n",
    "        last_ele=len(output_sample)-1\n",
    "        output_sample[last_ele] = data.tfam.samples[person][:-1]\n",
    "        if person in data.samples:\n",
    "            for marker in var_for_haplotype:\n",
    "                idx=int(marker.split('-')[0][1:])\n",
    "                output_sample[last_ele].append(data.genotype_all[person][idx])\n",
    "        else:\n",
    "            output_sample[last_ele].extend([\"00\"] * len(var_for_haplotype))\n",
    "    # haplotyping\n",
    "    if len(var_for_haplotype)==0:\n",
    "        varnames.pop(item,None)\n",
    "        #for person in data.families[item]:\n",
    "        #    data[person] = tmp1.missings\n",
    "        continue\n",
    "    for person in output_sample:\n",
    "        if set(person[5:])==set(['00']):\n",
    "            data.missing_persons.append(person[1])\n",
    "    with env.lock:\n",
    "        if not env.prephased:\n",
    "            tmp_log_output=env.tmp_log + str(os.getpid())\n",
    "            with stdoutRedirect(to = tmp_log_output + '.log'):\n",
    "                haplotypes[item] = tmp1.haplotyper.Execute(data.chrom, var_for_haplotype, positions_for_haplotype, output_sample, tmp1.rsq, tmp_log_output)[0]\n",
    "        else:\n",
    "            haplotypes[item] = tmp1.__PedToHaplotype(data.getFamSamples(item))\n",
    "    if len(haplotypes[item]) == 0:\n",
    "        # C++ haplotyping implementation failed\n",
    "        with env.chperror_counter.get_lock():\n",
    "            env.chperror_counter.value += 1\n",
    "    varnames[item]=var_for_haplotype\n",
    "    \n",
    "for item in haplotypes:\n",
    "    for hap_idx,haploid in enumerate(haplotypes[item]):\n",
    "        for vidx,var in enumerate(haploid[2:]):\n",
    "            if not var.endswith(':') and not var.endswith('|') and vidx!=0:\n",
    "                postvar_name=varnames[item][vidx]\n",
    "                prevar_name=varnames[item][vidx-1]\n",
    "                recomb_pair = (prevar_name,postvar_name)\n",
    "                try:\n",
    "                    recombPos[item][recomb_pair].append(hap_idx)\n",
    "                except:\n",
    "                    recombPos[item][recomb_pair]=[hap_idx]\n",
    "#\n",
    "# Compute founder MAFs\n",
    "#\n",
    "if len(tmp_mafs) > 0:\n",
    "    if tmp1.freq_by_fam:\n",
    "        for pop in tmp_mafs:\n",
    "            for v in tmp_mafs[pop]:\n",
    "                if type(tmp_mafs[pop][v]) is list:\n",
    "                    tmp_mafs[pop][v] = tmp_mafs[pop][v][0]/tmp_mafs[pop][v][1] if tmp_mafs[pop][v][1] >0 else 0.0\n",
    "    else:\n",
    "        for v in tmp_mafs:\n",
    "            if type(tmp_mafs[v]) is list:\n",
    "                tmp_mafs[v] = tmp_mafs[v][0]/tmp_mafs[v][1] if tmp_mafs[v][1] > 0 else 0.0\n",
    "## Make mafs consistent in structure regardless of freq_by_fam\n",
    "if tmp1.freq_by_fam:\n",
    "    for item in haplotypes:\n",
    "        popname=data.freq_by_fam[item]\n",
    "        if popname not in tmp_mafs:\n",
    "            continue\n",
    "        if item not in mafs:\n",
    "            mafs[item]=tmp_mafs[popname]\n",
    "        else:\n",
    "            for v in tmp_mafs[popname]:\n",
    "                if v not in mafs[item]:\n",
    "                    mafs[item][v]=tmp_mafs[popname][v]\n",
    "else:\n",
    "    for item in haplotypes:\n",
    "        mafs[item]=tmp_mafs\n",
    "if env.debug:\n",
    "    with env.lock:\n",
    "        print(\"variant mafs = \", mafs, \"\\n\", file = sys.stderr)\n",
    "##\n",
    "#\n",
    "# Drop some variants if maf is greater than given threshold\n",
    "#\n",
    "if not tmp1.maf_cutoff is None or tmp1.single_markers:\n",
    "    if tmp1.freq_by_fam:\n",
    "        exclude_vars=[[] for x in range(len(data.freq))]\n",
    "    for i in haplotypes.keys():\n",
    "        if tmp1.freq_by_fam:\n",
    "            pop_idx=data.freq.index(data.freq_by_fam[i])\n",
    "            tmp_exclude_vars=exclude_vars[pop_idx]\n",
    "        else:\n",
    "            tmp_exclude_vars=exclude_vars\n",
    "        for v in mafs[i].keys():\n",
    "            if not tmp1.maf_cutoff is None:\n",
    "                if mafs[i][v] > tmp1.maf_cutoff and v not in tmp_exclude_vars or v.split('-')[-1] not in data.include_vars:\n",
    "                    tmp_exclude_vars.append(v)\n",
    "            if tmp1.single_markers:\n",
    "                if v.split('-')[-1] not in data.include_vars:\n",
    "                    tmp_exclude_vars.append(v)\n",
    "        haplotypes[i] = listit(haplotypes[i])\n",
    "        tmp_remain_vars=[x for x in varnames[i] if x not in tmp_exclude_vars]\n",
    "        recomb_remain_vars=[]\n",
    "        if len(tmp_remain_vars) == 0:\n",
    "            recombPos[i]={}\n",
    "        else:\n",
    "            if len(recombPos[i]) > 0:\n",
    "                #extend recombination signal to neighbouring RVs\n",
    "                #if the original variant is to be excluded\n",
    "                #Only allow a maximum of one recombination event between one pair of consecutive markers\n",
    "                for pair in recombPos[i].keys():\n",
    "                    if pair[1] not in tmp_exclude_vars:\n",
    "                        if tmp_remain_vars.index(pair[1])!=0 and pair[1] not in recomb_remain_vars:\n",
    "                            recomb_remain_vars.append(pair[1])\n",
    "                        else:\n",
    "                            del recombPos[i][pair]\n",
    "                    else:\n",
    "                        if varnames[i].index(pair[1]) > varnames[i].index(tmp_remain_vars[-1]):\n",
    "                            #last variant\n",
    "                            del recombPos[i][pair]\n",
    "                            continue\n",
    "                        for tmp_idx in range(varnames[i].index(pair[1])+1,len(varnames[i])):\n",
    "                            if varnames[i][tmp_idx] not in tmp_exclude_vars:\n",
    "                                if tmp_remain_vars.index(varnames[i][tmp_idx])==0:\n",
    "                                    #delete recombination pair if the recombination was marked to the first remaining variant\n",
    "                                    del recombPos[i][pair]\n",
    "                                    break\n",
    "                                for tmp_hap in recombPos[i][pair]:\n",
    "                                    tmp_var=haplotypes[i][tmp_hap][tmp_idx+2]\n",
    "                                    if tmp_var.endswith(':') or tmp_var.endswith('|'):\n",
    "                                        haplotypes[i][tmp_hap][tmp_idx+2]=tmp_var[:-1]+'/'\n",
    "                                if varnames[i][tmp_idx] not in recomb_remain_vars:\n",
    "                                    recomb_remain_vars.append(varnames[i][tmp_idx])\n",
    "                                else:\n",
    "                                    del recombPos[i][pair]\n",
    "                                break\n",
    "        for j in range(len(haplotypes[i])):\n",
    "            haplotypes[i][j] = haplotypes[i][j][:2] + \\\n",
    "              [x for idx, x in enumerate(haplotypes[i][j][2:]) if varnames[i][idx] not in tmp_exclude_vars]\n",
    "        for tmp_var in varnames[i]:\n",
    "            if tmp_var not in uniq_vars:\n",
    "                     uniq_vars.append(tmp_var)\n",
    "        varnames[i] = [x for x in varnames[i] if x not in tmp_exclude_vars]\n",
    "        # handle trivial data\n",
    "        if len(varnames[i]) == 0:\n",
    "            del varnames[i]\n",
    "            del haplotypes[i]\n",
    "        if len(recombPos[i].keys())>tmp1.recomb_max:\n",
    "            #treat as missing if recombination events occurred more than speicified times\n",
    "            recombPos[i]={}\n",
    "            for person in data.families[i]:\n",
    "                data[person] = tmp1.missings\n",
    "            del varnames[i]\n",
    "            del haplotypes[i]\n",
    "    # count how many variants are removed\n",
    "    with env.commonvar_counter.get_lock():\n",
    "        if tmp1.freq_by_fam:\n",
    "            tmp_ex_vars=[tmp_var for tmp_vars in exclude_vars for tmp_var in tmp_vars]\n",
    "            env.commonvar_counter.value += len(set(tmp_ex_vars))\n",
    "        else:\n",
    "            env.commonvar_counter.value += len(exclude_vars)\n",
    "    # get total observed variants\n",
    "    if tmp1.freq_by_fam:\n",
    "        for item in varnames:\n",
    "            pop=data.freq_by_fam[item]\n",
    "            if pop not in data.total_mafs:\n",
    "                data.total_mafs[pop]={}\n",
    "                data.total_varnames[pop]=[]\n",
    "            for v in varnames[item]:\n",
    "                if v not in data.total_mafs[pop]:\n",
    "                    data.total_varnames[pop].append(v)\n",
    "                    data.total_mafs[pop][v]=mafs[item][v]\n",
    "        for pop in data.total_varnames:\n",
    "            data.total_varnames[pop]=sorted(data.total_varnames[pop], key=lambda x: int(x.split(\"-\")[0][1:]))\n",
    "            data.wt_maf[pop]=1.0\n",
    "            for v,tmaf in data.total_mafs[pop].iteritems():\n",
    "                data.wt_maf[pop]*=(1-tmaf)\n",
    "    else:\n",
    "        data.total_varnames['pop']=[]\n",
    "        for item in varnames:\n",
    "            for v in varnames[item]:\n",
    "                if v not in data.total_mafs:\n",
    "                    data.total_varnames['pop'].append(v)\n",
    "                    data.total_mafs[v]=mafs[item][v]\n",
    "        data.wt_maf['pop']=1.0\n",
    "        for v,tmaf in data.total_mafs.iteritems():\n",
    "            data.wt_maf['pop']*=(1-tmaf)\n",
    "        data.total_varnames['pop']=sorted(data.total_varnames['pop'], key=lambda x: int(x.split(\"-\")[0][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.total_varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??cstatgen.HaplotypingEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.wt_maf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = []\n",
    "for _ in range(1000):\n",
    "    a = queue.get()\n",
    "    tmp.getRegion(a)\n",
    "    tmp.apply(dd)\n",
    "    if len(dd.variants) != 0:\n",
    "        aa.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.getRegion(aa[0])\n",
    "dd = deepcopy(data)\n",
    "tmp.apply(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.apply(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1._MarkerMaker__Haplotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.freq_by_fam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.rvhaplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.apply(dd)\n",
    "tmp2.apply(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.freq_by_fam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.prephased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.tmp_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplotypes = OrderedDict()\n",
    "mafs = {}   ##Per fam per variant\n",
    "uniq_vars = []\n",
    "exclude_vars = []\n",
    "varnames = {}\n",
    "recombPos = {}\n",
    "tmp1._MarkerMaker__Haplotype(dd, haplotypes, mafs, varnames,recombPos,uniq_vars,exclude_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.recomb_perfam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.apply(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp2.apply(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.include_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.getRegion(aa[0])\n",
    "tmp1.apply(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.debug =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.chrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??EncoderWorker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.total_counter.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in jobs:\n",
    "    j.start()\n",
    "for j in jobs:\n",
    "    j.join()\n",
    "faulthandler.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    try:\n",
    "        faulthandler.enable(file=open(env.tmp_log + '.SEGV', 'w'))\n",
    "        for i in regions:\n",
    "            if isinstance(queue, list):\n",
    "                queue.append(i)\n",
    "            else:\n",
    "                queue.put(i)\n",
    "        freq_by_fam_flag = False\n",
    "        if not args.freq_by_fam is None:\n",
    "            print('haha')\n",
    "            freq_by_fam_flag = True\n",
    "            with open(args.freq_by_fam) as freq_fh:\n",
    "                for freq_line in freq_fh:\n",
    "                    tmp_eles=freq_line.split()   #Fam and Population\n",
    "                    data.freq_by_fam[tmp_eles[0]]=tmp_eles[1]\n",
    "            data.freq=sorted(list(set(data.freq_by_fam.values())))\n",
    "        else:\n",
    "            data.freq=args.freq\n",
    "        jobs = [EncoderWorker(\n",
    "            queue, len(regions), deepcopy(data),\n",
    "            RegionExtractor(args.vcf, chr_prefix = args.chr_prefix, allele_freq_info = data.freq, include_vars_file=args.include_vars),\n",
    "            MarkerMaker(args.bin, maf_cutoff = args.maf_cutoff,single_markers=args.single_markers,recomb_max=args.recomb_max,af_info=data.freq,freq_by_fam=freq_by_fam_flag,rsq=args.rsq,mle=args.mle, rvhaplo=args.rvhaplo, recomb_perfam=not args.recomb_cross_fam),\n",
    "            LinkageWriter(len(samples_not_vcf))\n",
    "            ) for i in range(env.jobs)]\n",
    "        for j in jobs:\n",
    "            j.start()\n",
    "        for j in jobs:\n",
    "            j.join()\n",
    "        faulthandler.disable()\n",
    "    except KeyboardInterrupt:\n",
    "        # FIXME: need to properly close all jobs\n",
    "        raise ValueError(\"Use 'killall {}' to properly terminate all processes!\".format(env.prog))\n",
    "    else:\n",
    "        env.log('{:,d} units (from {:,d} variants) processed; '\\\n",
    "            '{:,d} Mendelian inconsistencies and {:,d} recombination events handled\\n'.\\\n",
    "            format(env.success_counter.value,\n",
    "                   env.variants_counter.value,\n",
    "                   env.mendelerror_counter.value,\n",
    "                   env.recomb_counter.value), flush = True)\n",
    "        if env.triallelic_counter.value:\n",
    "            env.log('{:,d} tri-allelic loci were ignored'.format(env.triallelic_counter.value))\n",
    "        if env.commonvar_counter.value:\n",
    "            env.log('{:,d} variants ignored due to having MAF > {} and other specified constraints'.\\\n",
    "                    format(env.commonvar_counter.value, args.maf_cutoff))\n",
    "        if env.null_counter.value:\n",
    "            env.log('{:,d} units ignored due to absence in VCF file'.format(env.null_counter.value))\n",
    "        if env.trivial_counter.value:\n",
    "            env.log('{:,d} units ignored due to absence of variation in samples'.format(env.trivial_counter.value))\n",
    "        fatal_errors = 0\n",
    "        try:\n",
    "            # Error msg from C++ extension\n",
    "            os.system(\"cat {}/*.* > {}\".format(env.tmp_dir, env.tmp_log))\n",
    "            fatal_errors = wordCount(env.tmp_log)['fatal']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        if env.chperror_counter.value:\n",
    "            env.error(\"{:,d} regional markers failed to be generated due to haplotyping failures!\".\\\n",
    "                      format(env.chperror_counter.value))\n",
    "        if fatal_errors:\n",
    "            env.error(\"{:,d} or more regional markers failed to be generated due to runtime errors!\".\\\n",
    "                      format(fatal_errors))\n",
    "        env.log('Archiving regional marker data to directory [{}]'.format(env.cache_dir))\n",
    "        cache.write(arcroot = 'CACHE', source_dir = env.tmp_cache)\n",
    "env.jobs = args.jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: write to PLINK or mega2 format\n",
    "tpeds = [os.path.join(env.tmp_cache, item) for item in os.listdir(env.tmp_cache) if item.startswith(env.output) and item.endswith('.tped')]\n",
    "for fmt in args.format:\n",
    "    cache.setID(fmt)\n",
    "    if not args.vanilla and cache.check():\n",
    "        env.log('Loading {} data from archive ...'.format(fmt.upper()))\n",
    "        cache.load(target_dir = env.tmp_dir, names = [fmt.upper()])\n",
    "    else:\n",
    "        env.log('{:,d} units will be converted to {} format'.format(env.success_counter.value, fmt.upper()))\n",
    "        env.format_counter.value = 0\n",
    "        format(tpeds, os.path.join(env.tmp_cache, \"{}.tfam\".format(env.output)),\n",
    "               args.prevalence, args.wild_pen, args.muta_pen, fmt,\n",
    "               args.inherit_mode, args.theta_max, args.theta_inc)\n",
    "        env.log('{:,d} units successfully converted to {} format\\n'.\\\n",
    "                format(env.format_counter.value, fmt.upper()), flush = True)\n",
    "        if env.skipped_counter.value:\n",
    "            # FIXME: perhaps we need to rephrase this message?\n",
    "            env.log('{} region - family pairs skipped'.\\\n",
    "                    format(env.skipped_counter.value))\n",
    "        env.log('Archiving {} format to directory [{}]'.format(fmt.upper(), env.cache_dir))\n",
    "        cache.write(arcroot = fmt.upper(),\n",
    "                    source_dir = os.path.join(env.tmp_dir, fmt.upper()), mode = 'a')\n",
    "mkpath(env.outdir)\n",
    "if args.run_linkage:\n",
    "    cache.setID('analysis')\n",
    "    if not args.vanilla and cache.check():\n",
    "        env.log('Loading linkage analysis result from archive ...'.format(fmt.upper()))\n",
    "        cache.load(target_dir = env.output, names = ['heatmap'])\n",
    "    else:\n",
    "        env.log('Running linkage analysis ...'.format(fmt.upper()))\n",
    "        run_linkage(args.blueprint, args.theta_inc, args.theta_max, args.output_limit)\n",
    "        env.log('Linkage analysis succesfully performed for {:,d} units\\n'.\\\n",
    "                format(env.run_counter.value, fmt.upper()), flush = True)\n",
    "        if env.makeped_counter.value:\n",
    "            env.log('{} \"makeped\" runtime errors occurred'.format(env.makeped_counter.value))\n",
    "        if env.pedcheck_counter.value:\n",
    "            env.log('{} \"pedcheck\" runtime errors occurred'.format(env.pedcheck_counter.value))\n",
    "        if env.unknown_counter.value:\n",
    "            env.log('{} \"unknown\" runtime errors occurred'.format(env.unknown_counter.value))\n",
    "        if env.mlink_counter.value:\n",
    "            env.log('{} \"mlink\" runtime errors occurred'.format(env.mlink_counter.value))\n",
    "        cache.write(arcroot = 'heatmap', source_dir = os.path.join(env.output, 'heatmap'), mode = 'a')\n",
    "    html(args.theta_inc, args.theta_max, args.output_limit)\n",
    "else:\n",
    "    env.log('Saving data to [{}]'.format(os.path.abspath(env.output)))\n",
    "    cache.load(target_dir = env.output, names = [fmt.upper() for fmt in args.format])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
